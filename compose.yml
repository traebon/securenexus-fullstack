# docker compose v2 (no 'version:' key)
networks:
  proxy:
    external: false
    # Public-facing services only (Traefik, web apps, landing pages)
  database:
    internal: true
    # Database services only - isolated from internet
  monitoring:
    internal: true
    # Monitoring stack only - isolated from internet
  backend:
    internal: true
    # Internal services (workers, processors) - isolated from internet

volumes:
  caddy-data:
  caddy-config:
  grafana-data:
  prometheus-data:
  alertmanager-data:
  loki-data:
  redis-data:
  pg-data:
  pg-ssl:
  keycloak-db-data:
  etcd-data:
  mysql-data:
  tailscale-data:
  uptime-kuma-data:
  homarr-data:  # Homarr v1.0 data (previously homarr-v1-data)
  portainer-data:  # Portainer container management data
  # stalwart-data: # Removed - switched to mailcow
  # Byrne Accounting volumes
  erpnext-db-data:
  erpnext-redis-cache-data:
  erpnext-redis-queue-data:
  erpnext-sites-data:
  erpnext-assets-data:
  # Dickson Supplies volumes
  dickson-db-data:
  dickson-redis-cache-data:
  dickson-redis-queue-data:
  dickson-sites-data:
  dickson-assets-data:
  # Nextcloud volumes
  nextcloud-data:
  nextcloud-db-data:
  # Notesnook sync server volumes
  notesnook-data:
  notesnook-db-data:
  notesnook-s3-data:
  # Kanidm test evaluation volumes
  kanidm-data:

secrets:
  authentik_secret_key:
    file: ./secrets/authentik_secret_key.txt
  postgres_password:
    file: ./secrets/postgres_password.txt
  redis_password:
    file: ./secrets/redis_password.txt
  mysql_password:
    file: ./secrets/mysql_password.txt
  coredns_api_key:
    file: ./secrets/coredns_api_key.txt
  smtp_username:
    file: ./secrets/smtp_username.txt
  smtp_password:
    file: ./secrets/smtp_password.txt
  grafana_oauth_secret:
    file: ./secrets/grafana_oauth_secret.txt
  keycloak_db_password:
    file: ./secrets/keycloak_db_password.txt
  keycloak_admin_password:
    file: ./secrets/keycloak_admin_password.txt
  tailscale_authkey:
    file: ./secrets/tailscale_authkey.txt
  crowdsec_bouncer_api_key:
    file: ./secrets/crowdsec_bouncer_api_key.txt
  souin_redis_password:
    file: ./secrets/souin_redis_password.txt
  homarr_encryption_key:
    file: ./secrets/homarr_encryption_key.txt
  homarr_oauth_secret:
    file: ./secrets/homarr_oauth_secret.txt
  grafana_admin_password:
    file: ./secrets/grafana_admin_password.txt
  # Byrne Accounting secrets
  erpnext_db_password:
    file: ./secrets/erpnext_db_password.txt
  erpnext_admin_password:
    file: ./secrets/erpnext_admin_password.txt
  erpnext_redis_cache_password:
    file: ./secrets/erpnext_redis_cache_password.txt
  erpnext_redis_queue_password:
    file: ./secrets/erpnext_redis_queue_password.txt
  # Dickson Supplies secrets
  dickson_db_password:
    file: ./secrets/dickson_db_password.txt
  dickson_admin_password:
    file: ./secrets/dickson_admin_password.txt
  dickson_redis_cache_password:
    file: ./secrets/dickson_redis_cache_password.txt
  dickson_redis_queue_password:
    file: ./secrets/dickson_redis_queue_password.txt
  # Nextcloud secrets
  nextcloud_db_password:
    file: ./secrets/nextcloud_db_password.txt
  nextcloud_admin_password:
    file: ./secrets/nextcloud_admin_password.txt
  # Notesnook secrets
  notesnook_db_username:
    file: ./secrets/notesnook_db_username.txt
  notesnook_db_password:
    file: ./secrets/notesnook_db_password.txt
  notesnook_connection_string:
    file: ./secrets/notesnook_connection_string.txt
  notesnook_s3_password:
    file: ./secrets/notesnook_s3_password.txt
  notesnook_jwt_secret:
    file: ./secrets/notesnook_jwt_secret.txt
  notesnook_instance_name:
    file: ./secrets/notesnook_instance_name.txt
  notesnook_api_secret:
    file: ./secrets/notesnook_api_secret.txt
  notesnook_disable_signups:
    file: ./secrets/notesnook_disable_signups.txt
  notesnook_s3_access_key:
    file: ./secrets/notesnook_s3_access_key.txt
  notesnook_mongodb_keyfile:
    file: ./secrets/notesnook_mongodb_keyfile.txt
  # Database TLS certificates
  postgres_server_cert:
    file: ./certs/database/postgres/server-cert.pem
  postgres_server_key:
    file: ./certs/database/postgres/server-key.pem
  mysql_server_cert:
    file: ./certs/database/mysql/server-cert.pem
  mysql_server_key:
    file: ./certs/database/mysql/server-key.pem
  mongodb_tls_cert:
    file: ./certs/database/mongodb/mongodb-combined.pem
  # Watchtower secrets
  watchtower_email_password:
    file: ./secrets/watchtower_email_password.txt
  watchtower_api_token:
    file: ./secrets/watchtower_api_token.txt

services:
  docker-proxy:
    image: ghcr.io/tecnativa/docker-socket-proxy:latest
    restart: unless-stopped
    networks: [proxy]
    environment:
      INFO: "1"
      CONTAINERS: "1"
      SERVICES: "1"
      TASKS: "1"
      NETWORKS: "1"
      PLUGINS: "0"
      POST: "0"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f haproxy > /dev/null"]
      interval: 30s
      timeout: 5s
      retries: 3
    profiles: ["core"]

  traefik:
    image: traefik:v3.6.0
    restart: unless-stopped
    command:
      - "--configFile=/etc/traefik/traefik.yml"
      - "--certificatesresolvers.le.acme.email=${EMAIL}"
    networks: [proxy, database, monitoring, backend]  # Multi-network access for routing
    dns:
      - 8.8.8.8
      - 1.1.1.1
    ports:
      - "80:80"
      - "443:443"
      # - "587:587/tcp"  # Removed - mailcow handles mail ports directly
    environment:
      # DNS-01 via CoreDNS webhook
      ACME_WEBHOOK_URL: http://acme_webhook:5000/update-txt-record
    volumes:
      - ./config/traefik.yml:/etc/traefik/traefik.yml:ro        # static
      - ./config/dynamic:/etc/traefik/dynamic:ro                # dynamic
      - ./acme:/acme
    labels:
      - traefik.enable=true
      # Dashboard (secured: admin-vpn + internal service)
      - traefik.http.routers.api.rule=Host(`traefik.${DOMAIN}`)
      - traefik.http.routers.api.entrypoints=websecure
      - traefik.http.routers.api.tls.certresolver=le
      - traefik.http.routers.api.middlewares=admin-vpn@file,secure-headers@file
      - traefik.http.routers.api.service=api@internal
    # Security hardening
    security_opt:
      - no-new-privileges:true
      - apparmor:docker-default
    cap_drop:
      - ALL
    cap_add:
      - NET_BIND_SERVICE
      - DAC_OVERRIDE
      - CHOWN
      - SETUID
      - SETGID
    tmpfs:
      - /tmp:noexec,nosuid,size=100m
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:8080/ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    profiles: ["core"]

  souin_redis:
    image: redis:7-alpine
    restart: unless-stopped
    networks: [proxy]
    command: ["sh","-c","exec redis-server --requirepass \"$$(cat /run/secrets/souin_redis_password)\""]
    secrets: [souin_redis_password]
    healthcheck:
      test: ["CMD-SHELL","redis-cli -a \"$$(cat /run/secrets/souin_redis_password)\" ping | grep PONG"]
      interval: 10s
      timeout: 5s
      retries: 5
    profiles: ["core"]

  tailscale:
    image: tailscale/tailscale:latest
    container_name: tailscale
    restart: unless-stopped
    network_mode: host
    volumes:
      - tailscale-data:/var/lib/tailscale
      - /dev/net/tun:/dev/net/tun
    cap_add:
      - NET_ADMIN
      - NET_RAW
    environment:
      - TS_AUTHKEY_FILE=/run/secrets/tailscale_authkey
      - TS_STATE_DIR=/var/lib/tailscale
      - TS_USERSPACE=false
      - TS_ACCEPT_DNS=true
    secrets: [tailscale_authkey]
    command: ["tailscaled"]
    profiles: ["core"]

  # CrowdSec + Traefik bouncer
  crowdsec:
    image: crowdsecurity/crowdsec:latest
    restart: unless-stopped
    networks: [proxy]
    environment:
      COLLECTIONS: "crowdsecurity/traefik crowdsecurity/http-cve crowdsecurity/linux"
      GID: "0"
      DISABLE_AGENT: "true"
    volumes:
      - ./crowdsec/data:/var/lib/crowdsec/data
      - ./crowdsec/config:/etc/crowdsec
      - /var/log:/var/log:ro
    healthcheck:
      test: ["CMD-SHELL", "cscli lapi status || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
    profiles: ["core"]

  crowdsec_bouncer:
    image: fbonalair/traefik-crowdsec-bouncer:latest
    restart: unless-stopped
    depends_on: [crowdsec]
    networks: [proxy]
    env_file:
      - ./crowdsec_bouncer.env
    healthcheck:
      disable: true
    profiles: ["core"]

  # Identity

  # PostgreSQL SSL Certificate Setup Init Container (Optimized)
  postgres-ssl-setup:
    image: alpine:3.19
    restart: "no"
    networks: [database]
    command: |
      sh -c "
        echo 'üîê Setting up PostgreSQL SSL certificates with proper ownership...'

        # Create SSL directory
        mkdir -p /ssl-volume

        # Copy certificates from Docker secrets with proper ownership
        if [ -f /run/secrets/postgres_server_cert ] && [ -f /run/secrets/postgres_server_key ]; then
          cp /run/secrets/postgres_server_cert /ssl-volume/server.crt
          cp /run/secrets/postgres_server_key /ssl-volume/server.key

          # Set PostgreSQL user ownership (UID 70 in postgres:16-alpine)
          chown 70:70 /ssl-volume/server.*

          # Set secure permissions
          chmod 644 /ssl-volume/server.crt
          chmod 600 /ssl-volume/server.key

          echo '‚úÖ PostgreSQL SSL certificates prepared successfully'
          ls -la /ssl-volume/
        else
          echo '‚ùå Certificate secrets not found, PostgreSQL will start without TLS'
          echo '   Expected: /run/secrets/postgres_server_cert and /run/secrets/postgres_server_key'
          exit 1
        fi
      "
    secrets:
      - postgres_server_cert
      - postgres_server_key
    volumes:
      - pg-ssl:/ssl-volume
    profiles: ["identity"]

  authentik_db:
    image: postgres:16-alpine
    restart: unless-stopped
    depends_on:
      postgres-ssl-setup:
        condition: service_completed_successfully
    networks: [database]  # Secure database network
    environment:
      POSTGRES_DB: authentik
      POSTGRES_USER: authentik
      POSTGRES_PASSWORD_FILE: /run/secrets/postgres_password
      # Enable TLS encryption
      POSTGRES_INITDB_ARGS: "--auth-host=scram-sha-256 --auth-local=scram-sha-256"
    command: |
      postgres
        -c ssl=on
        -c ssl_cert_file=/var/lib/postgresql/ssl/server.crt
        -c ssl_key_file=/var/lib/postgresql/ssl/server.key
        -c ssl_prefer_server_ciphers=on
        -c ssl_min_protocol_version=TLSv1.2
    secrets:
      - postgres_password
    volumes:
      - pg-data:/var/lib/postgresql/data
      - pg-ssl:/var/lib/postgresql/ssl
    # Security hardening
    security_opt:
      - no-new-privileges:true
      - apparmor:docker-default
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - DAC_OVERRIDE
      - FOWNER
      - SETUID
      - SETGID
    tmpfs:
      - /tmp:noexec,nosuid,size=100m
      - /var/tmp:noexec,nosuid,size=50m
    healthcheck:
      test: ["CMD-SHELL","pg_isready -U authentik -d authentik"]
      interval: 10s
      timeout: 5s
      retries: 5
    profiles: ["identity"]

  redis_cache:
    image: redis:7-alpine
    restart: unless-stopped
    command: ["sh","-c","exec redis-server --requirepass \"$$(cat /run/secrets/redis_password)\" --appendonly yes --appendfsync everysec"]
    secrets: [redis_password]
    volumes:
      - redis-data:/data
    networks: [database]  # Moved to secure database network
    # Security hardening
    security_opt:
      - no-new-privileges:true
      - apparmor:docker-default
    cap_drop:
      - ALL
    cap_add:
      - SETUID
      - SETGID
      - CHOWN
    user: '999:999'
    tmpfs:
      - /tmp:noexec,nosuid,size=50m
    healthcheck:
      test: ["CMD-SHELL","redis-cli -a \"$$(cat /run/secrets/redis_password)\" ping | grep PONG"]
      interval: 10s
      timeout: 5s
      retries: 5
    profiles: ["identity"]

  authentik_server:
    image: ghcr.io/goauthentik/server:2025.10.1
    restart: unless-stopped
    depends_on: [authentik_db]
    networks: [proxy, database, backend]  # Multi-network: public access + database + internal
    command: server
    environment:
      AUTHENTIK_POSTGRESQL__HOST: authentik_db
      AUTHENTIK_POSTGRESQL__USER: authentik
      AUTHENTIK_POSTGRESQL__NAME: authentik
      AUTHENTIK_POSTGRESQL__PASSWORD: file:///run/secrets/postgres_password
      AUTHENTIK_POSTGRESQL__SSLMODE: disable
      AUTHENTIK_SECRET_KEY: file:///run/secrets/authentik_secret_key
    secrets: [postgres_password, authentik_secret_key]
    volumes: []
    # Security hardening
    security_opt:
      - no-new-privileges:true
      - apparmor:docker-default
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - DAC_OVERRIDE
      - SETUID
      - SETGID
      - NET_BIND_SERVICE
    tmpfs:
      - /tmp:noexec,nosuid,size=100m
      - /var/tmp:noexec,nosuid,size=50m
    healthcheck:
      test: ["CMD-SHELL", "python -c \"import urllib.request; urllib.request.urlopen('http://localhost:9000/-/health/ready/')\""]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    labels:
      - traefik.enable=true
      - traefik.http.routers.authentik.rule=Host(`authentik.${DOMAIN}`) || Host(`sso.${DOMAIN}`) || Host(`auth.${DOMAIN}`) || Host(`auth.byrne-accounts.org`)
      - traefik.http.routers.authentik.entrypoints=websecure
      - traefik.http.routers.authentik.tls.certresolver=le
      - traefik.http.routers.authentik.middlewares=authentik-protection@file
      - traefik.http.services.authentik.loadbalancer.server.port=9000
    profiles: ["identity"]

  authentik_worker:
    image: ghcr.io/goauthentik/server:2025.10.1
    restart: unless-stopped
    depends_on: [authentik_db]
    networks: [database, backend]  # Internal networks only
    environment:
      AUTHENTIK_POSTGRESQL__HOST: authentik_db
      AUTHENTIK_POSTGRESQL__USER: authentik
      AUTHENTIK_POSTGRESQL__NAME: authentik
      AUTHENTIK_POSTGRESQL__PASSWORD: file:///run/secrets/postgres_password
      AUTHENTIK_POSTGRESQL__SSLMODE: disable
      AUTHENTIK_SECRET_KEY: file:///run/secrets/authentik_secret_key
    secrets: [postgres_password, authentik_secret_key]
    volumes: []
    command: worker
    healthcheck:
      test: ["CMD", "python", "-c", "import os; exit(0 if os.path.exists('/proc/1/comm') else 1)"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    profiles: ["identity"]

  # Keycloak SSO (alternative to Authentik)
  keycloak_db:
    image: postgres:16-alpine
    restart: unless-stopped
    networks: [proxy]
    environment:
      POSTGRES_DB: keycloak
      POSTGRES_USER: keycloak
      POSTGRES_PASSWORD_FILE: /run/secrets/keycloak_db_password
    secrets: [keycloak_db_password]
    volumes:
      - keycloak-db-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL","pg_isready -U keycloak -d keycloak"]
      interval: 10s
      timeout: 5s
      retries: 5
    profiles: ["identity"]

  keycloak:
    image: quay.io/keycloak/keycloak:26.0.7
    restart: unless-stopped
    depends_on: [keycloak_db]
    networks: [proxy]
    command: start
    environment:
      KC_DB: postgres
      KC_DB_URL: jdbc:postgresql://keycloak_db:5432/keycloak
      KC_DB_USERNAME: keycloak
      KC_DB_PASSWORD: "${KC_DB_PASSWORD}"
      KC_HOSTNAME: keycloak.${DOMAIN}
      KC_PROXY_HEADERS: xforwarded
      KC_HTTP_RELATIVE_PATH: /
      KC_HTTP_ENABLED: "true"
      KC_HEALTH_ENABLED: "true"
      KC_METRICS_ENABLED: "true"
      KEYCLOAK_ADMIN: admin
      KEYCLOAK_ADMIN_PASSWORD: "${KEYCLOAK_ADMIN_PASSWORD}"
    healthcheck:
      test: ["CMD-SHELL", "exec 3<>/dev/tcp/127.0.0.1/9000 && echo -e 'GET /health/ready HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: close\\r\\n\\r\\n' >&3 && timeout 1 cat <&3 | grep -q 'UP'"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    labels:
      - traefik.enable=true
      - traefik.http.routers.keycloak.rule=Host(`keycloak.${DOMAIN}`)
      - traefik.http.routers.keycloak.entrypoints=websecure
      - traefik.http.routers.keycloak.tls.certresolver=le
      # No middleware - let Keycloak handle its own security headers
      - traefik.http.services.keycloak.loadbalancer.server.port=8080
    profiles: ["identity"]

  # Landing & Portal
  landing:
    image: caddy:2-alpine
    restart: unless-stopped
    networks: [proxy]
    volumes:
      - ./landing:/usr/share/caddy:ro
    labels:
      - traefik.enable=true
      # HTTPS router
      - traefik.http.routers.landing.rule=Host(`${DOMAIN}`)
      - traefik.http.routers.landing.entrypoints=websecure
      - traefik.http.routers.landing.tls.certresolver=le
      - traefik.http.routers.landing.middlewares=secure-headers@file
      # HTTP router (redirect to HTTPS)
      - traefik.http.routers.landing-http.rule=Host(`${DOMAIN}`)
      - traefik.http.routers.landing-http.entrypoints=web
      - traefik.http.routers.landing-http.middlewares=redirect-to-https@file
      - traefik.http.services.landing.loadbalancer.server.port=80
    profiles: ["portal"]

  # ERPNext Setup Wizard Portal
  erp-setup-portal:
    build: ./erp-setup-portal
    restart: unless-stopped
    networks: [proxy]
    labels:
      - traefik.enable=true
      # HTTPS router
      - traefik.http.routers.erp-setup.rule=Host(`setup.byrne-accounts.org`)
      - traefik.http.routers.erp-setup.entrypoints=websecure
      - traefik.http.routers.erp-setup.tls.certresolver=le
      - traefik.http.routers.erp-setup.middlewares=secure-headers@file
      # HTTP router (redirect to HTTPS)
      - traefik.http.routers.erp-setup-http.rule=Host(`setup.byrne-accounts.org`)
      - traefik.http.routers.erp-setup-http.entrypoints=web
      - traefik.http.routers.erp-setup-http.middlewares=redirect-to-https@file
      - traefik.http.services.erp-setup.loadbalancer.server.port=80
    profiles: ["byrne"]

  # Homarr v1.0 (Primary Portal)
  homarr:
    image: ghcr.io/homarr-labs/homarr:latest
    container_name: homarr
    restart: unless-stopped
    networks: [proxy]
    environment:
      - BASE_URL=https://portal.${DOMAIN}
      # Secrets loaded from secure files (not hardcoded)
      - SECRET_ENCRYPTION_KEY_FILE=/run/secrets/homarr_encryption_key
      - AUTH_PROVIDERS=oidc,credentials
      - AUTH_OIDC_ISSUER=https://sso.${DOMAIN}/application/o/homarr/
      - AUTH_OIDC_CLIENT_ID=homarr
      - AUTH_OIDC_CLIENT_SECRET_FILE=/run/secrets/homarr_oauth_secret
      - AUTH_OIDC_CLIENT_NAME=Authentik
    secrets:
      - homarr_encryption_key
      - homarr_oauth_secret
    entrypoint: >
      sh -c '
        export SECRET_ENCRYPTION_KEY=$$(cat /run/secrets/homarr_encryption_key 2>/dev/null || echo "")
        export AUTH_OIDC_CLIENT_SECRET=$$(cat /run/secrets/homarr_oauth_secret 2>/dev/null || echo "")
        exec /app/entrypoint.sh "$$@"
      '
    volumes:
      - homarr-data:/appdata
      - /var/run/docker.sock:/var/run/docker.sock:ro
    labels:
      - traefik.enable=true
      - traefik.docker.network=securenexus-fullstack_proxy
      # HTTPS router
      - traefik.http.routers.portal.rule=Host(`portal.${DOMAIN}`)
      - traefik.http.routers.portal.entrypoints=websecure
      - traefik.http.routers.portal.tls.certresolver=le
      - traefik.http.routers.portal.middlewares=secure-headers@file
      - traefik.http.routers.portal.service=portal-svc
      # HTTP router (redirect to HTTPS)
      - traefik.http.routers.portal-http.rule=Host(`portal.${DOMAIN}`)
      - traefik.http.routers.portal-http.entrypoints=web
      - traefik.http.routers.portal-http.middlewares=redirect-to-https@file
      - traefik.http.routers.portal-http.service=portal-svc
      - traefik.http.services.portal-svc.loadbalancer.server.port=7575
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://127.0.0.1:7575 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    profiles: ["portal"]

  # App Catalog - Custom application deployment system
  app-catalog:
    build:
      context: ./apps-catalog
      dockerfile: Dockerfile
    container_name: app-catalog
    restart: unless-stopped
    networks: [proxy]
    environment:
      - DOMAIN=${DOMAIN}
      - EMAIL=${EMAIL}
      - COMPOSE_PROJECT_NAME=securenexus-fullstack
    volumes:
      - ./apps-catalog/catalog:/catalog:ro
      - ./apps-catalog/deployed:/deployed
      - ./compose.yml:/compose.yml:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    labels:
      - traefik.enable=true
      - traefik.docker.network=securenexus-fullstack_proxy
      # HTTPS router
      - traefik.http.routers.apps.rule=Host(`apps.${DOMAIN}`)
      - traefik.http.routers.apps.entrypoints=websecure
      - traefik.http.routers.apps.tls.certresolver=le
      - traefik.http.routers.apps.middlewares=sso@file,secure-headers@file
      # HTTP router (redirect to HTTPS)
      - traefik.http.routers.apps-http.rule=Host(`apps.${DOMAIN}`)
      - traefik.http.routers.apps-http.entrypoints=web
      - traefik.http.routers.apps-http.middlewares=redirect-to-https@file
      - traefik.http.services.apps.loadbalancer.server.port=5000
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://127.0.0.1:5000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    profiles: ["portal"]

  # Documentation Wiki (MkDocs Material)
  wiki:
    build:
      context: ./wiki
      dockerfile: Dockerfile
    container_name: wiki
    restart: unless-stopped
    networks: [proxy]
    environment:
      - TZ=${TZ:-America/New_York}
    labels:
      - traefik.enable=true
      - traefik.docker.network=securenexus-fullstack_proxy
      # HTTPS router
      - traefik.http.routers.wiki.rule=Host(`docs.${DOMAIN}`)
      - traefik.http.routers.wiki.entrypoints=websecure
      - traefik.http.routers.wiki.tls.certresolver=le
      - traefik.http.routers.wiki.middlewares=secure-headers@file
      - traefik.http.routers.wiki.service=wiki-svc
      # HTTP router (redirect to HTTPS)
      - traefik.http.routers.wiki-http.rule=Host(`docs.${DOMAIN}`)
      - traefik.http.routers.wiki-http.entrypoints=web
      - traefik.http.routers.wiki-http.middlewares=redirect-to-https@file
      - traefik.http.routers.wiki-http.service=wiki-svc
      - traefik.http.services.wiki-svc.loadbalancer.server.port=8000
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://127.0.0.1:8000 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    profiles: ["portal"]

  # Monitoring
  prometheus:
    image: prom/prometheus:v2.53.0
    restart: unless-stopped
    networks: [monitoring]  # Moved to secure monitoring network
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    volumes:
      - prometheus-data:/prometheus
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/blackbox.yml:/etc/prometheus/blackbox.yml:ro
      - ./monitoring/alert_rules.yml:/etc/prometheus/alert_rules.yml:ro
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          memory: 1G
    labels:
      - traefik.enable=true
      - traefik.http.routers.prom.rule=Host(`prometheus.${DOMAIN}`)
      - traefik.http.routers.prom.entrypoints=websecure
      - traefik.http.routers.prom.tls.certresolver=le
      - traefik.http.routers.prom.middlewares=admin-vpn@file,secure-headers@file
      - traefik.http.services.prom.loadbalancer.server.port=9090
    # Security hardening
    security_opt:
      - no-new-privileges:true
      - apparmor:docker-default
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - DAC_OVERRIDE
      - SETUID
      - SETGID
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=100m
    profiles: ["monitoring"]

  alertmanager:
    image: prom/alertmanager:v0.27.0
    restart: unless-stopped
    networks: [monitoring]  # Moved to secure monitoring network
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
    volumes:
      - alertmanager-data:/alertmanager
      - ./monitoring/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
    labels:
      - traefik.enable=true
      - traefik.http.routers.alertmanager.rule=Host(`alerts.${DOMAIN}`)
      - traefik.http.routers.alertmanager.entrypoints=websecure
      - traefik.http.routers.alertmanager.tls.certresolver=le
      - traefik.http.routers.alertmanager.middlewares=admin-vpn@file,secure-headers@file
      - traefik.http.services.alertmanager.loadbalancer.server.port=9093
    profiles: ["monitoring"]

  blackbox:
    image: prom/blackbox-exporter:v0.25.0
    restart: unless-stopped
    networks: [monitoring]  # Monitoring network for metrics
    command: ["--config.file=/etc/blackbox_exporter/config.yml"]
    volumes:
      - ./monitoring/blackbox.yml:/etc/blackbox_exporter/config.yml:ro
    profiles: ["monitoring"]

  loki:
    image: grafana/loki:2.9.6
    restart: unless-stopped
    networks: [monitoring]  # Moved to secure monitoring network
    command: -config.file=/etc/loki/loki.yml
    volumes:
      - loki-data:/loki
      - ./monitoring/loki.yml:/etc/loki/loki.yml:ro
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3100/ready || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    profiles: ["monitoring"]

  promtail:
    image: grafana/promtail:2.9.6
    restart: unless-stopped
    networks: [proxy]
    command: -config.file=/etc/promtail/config.yml
    volumes:
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - ./monitoring/promtail.yml:/etc/promtail/config.yml:ro
    profiles: ["monitoring"]

  grafana:
    image: grafana/grafana:11.1.0
    restart: unless-stopped
    networks: [proxy, monitoring]  # Public access + monitoring data
    user: "472"
    environment:
      GF_SERVER_ROOT_URL: https://grafana.${DOMAIN}
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD_FILE: /run/secrets/grafana_admin_password  # Loaded from Docker secret
      GF_AUTH_DISABLE_LOGIN_FORM: "false"
      GF_AUTH_ANONYMOUS_ENABLED: "false"
      # OAuth / OIDC (Authentik) - secret loaded from file
      GF_AUTH_GENERIC_OAUTH_ENABLED: "true"
      GF_AUTH_GENERIC_OAUTH_NAME: "Authentik"
      GF_AUTH_GENERIC_OAUTH_CLIENT_ID: "grafana-oauth"
      GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET: ""  # Loaded from Docker secret
      GF_AUTH_GENERIC_OAUTH_SCOPES: "openid profile email"
      GF_AUTH_GENERIC_OAUTH_AUTH_URL: https://sso.${DOMAIN}/application/o/authorize/
      GF_AUTH_GENERIC_OAUTH_TOKEN_URL: https://sso.${DOMAIN}/application/o/token/
      GF_AUTH_GENERIC_OAUTH_API_URL: https://sso.${DOMAIN}/application/o/userinfo/
      GF_AUTH_GENERIC_OAUTH_ALLOW_SIGN_UP: "true"
      GF_AUTH_GENERIC_OAUTH_AUTO_LOGIN: "false"
      GF_AUTH_GENERIC_OAUTH_ROLE_ATTRIBUTE_PATH: "contains(groups[*], 'Grafana Admins') && 'Admin' || contains(groups[*], 'authentik Admins') && 'Admin' || 'Viewer'"
      GF_AUTH_GENERIC_OAUTH_ROLE_ATTRIBUTE_STRICT: "false"
      GF_AUTH_SIGNOUT_REDIRECT_URL: https://sso.${DOMAIN}/application/o/grafana-oauth/end-session/
    secrets:
      - grafana_oauth_secret
      - grafana_admin_password
    entrypoint: >
      sh -c '
        export GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET=$$(cat /run/secrets/grafana_oauth_secret 2>/dev/null || echo "")
        exec /run.sh
      '
    volumes:
      - grafana-data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./monitoring/dashboards:/var/lib/grafana/dashboards:ro
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    labels:
      - traefik.enable=true
      - traefik.http.routers.grafana.rule=Host(`grafana.${DOMAIN}`)
      - traefik.http.routers.grafana.entrypoints=websecure
      - traefik.http.routers.grafana.tls.certresolver=le
      - traefik.http.routers.grafana.middlewares=admin-vpn@file,secure-headers@file
      - traefik.http.services.grafana.loadbalancer.server.port=3000
    # Security hardening
    security_opt:
      - no-new-privileges:true
      - apparmor:docker-default
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - DAC_OVERRIDE
      - SETUID
      - SETGID
    tmpfs:
      - /tmp:noexec,nosuid,size=100m
    profiles: ["monitoring"]

  uptime-kuma:
    image: louislam/uptime-kuma:1
    restart: unless-stopped
    networks: [proxy]
    volumes:
      - uptime-kuma-data:/app/data
      - /var/run/docker.sock:/var/run/docker.sock:ro
    labels:
      - traefik.enable=true
      - traefik.http.routers.uptime.rule=Host(`status.${DOMAIN}`)
      - traefik.http.routers.uptime.entrypoints=websecure
      - traefik.http.routers.uptime.tls.certresolver=le
      - traefik.http.routers.uptime.middlewares=secure-headers@file
      - traefik.http.services.uptime.loadbalancer.server.port=3001
      # Watchtower: Allow auto-updates (non-critical monitoring service)
      - com.centurylinklabs.watchtower.enable=true
      - com.centurylinklabs.watchtower.monitor-only=false
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3001 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    profiles: ["monitoring"]

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.49.2
    restart: unless-stopped
    networks: [proxy]
    cap_add:
      - SYS_ADMIN
      - SYS_PTRACE
    security_opt:
      - apparmor:unconfined
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
    labels:
      # Watchtower: Allow auto-updates (non-critical monitoring service)
      - com.centurylinklabs.watchtower.enable=true
      - com.centurylinklabs.watchtower.monitor-only=false
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          memory: 512M
    healthcheck:
      test: ["CMD", "wget", "-q", "--tries=1", "--spider", "http://localhost:8080/metrics"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles: ["monitoring"]

  node-exporter:
    image: prom/node-exporter:v1.8.1
    restart: unless-stopped
    networks: [monitoring]  # Monitoring network for metrics
    pid: host
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
    profiles: ["monitoring"]

  redis-exporter:
    image: redis-exporter-with-shell:local
    restart: unless-stopped
    networks: [monitoring, database]  # Access Redis DB + provide metrics
    secrets: [redis_password]
    profiles: ["monitoring"]

  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:v0.15.0
    restart: unless-stopped
    networks: [monitoring, database]  # Access PostgreSQL + provide metrics
    secrets: [postgres_password]
    volumes:
      - ./scripts/postgres-exporter-wrapper.sh:/usr/local/bin/postgres-exporter-wrapper.sh:ro
    entrypoint: ["/bin/sh"]
    command: ["/usr/local/bin/postgres-exporter-wrapper.sh"]
    profiles: ["monitoring"]

  # Certificate Management

  cert-manager:
    build:
      context: ./cert-manager
      dockerfile: Dockerfile
    image: securenexus/cert-manager:latest
    container_name: cert-manager
    restart: unless-stopped
    networks: [proxy, monitoring]  # Access to Traefik + monitoring metrics
    user: "0:0"  # Run as root to access ACME files
    volumes:
      - ./acme:/acme:ro  # Read Traefik ACME storage
      - ./cert-manager/config.json:/etc/cert-manager/config.json:ro
      - ./certs:/certs:ro  # Read database certificates for distribution
      - /var/run/docker.sock:/var/run/docker.sock:ro  # Docker API access for restarts
    environment:
      - CONFIG_PATH=/etc/cert-manager/config.json
    labels:
      - traefik.enable=true
      - traefik.docker.network=securenexus-fullstack_proxy
      # Metrics endpoint (VPN-only access)
      - traefik.http.routers.cert-manager-metrics.rule=Host(`certs.${DOMAIN}`) && PathPrefix(`/metrics`)
      - traefik.http.routers.cert-manager-metrics.entrypoints=websecure
      - traefik.http.routers.cert-manager-metrics.tls.certresolver=le
      - traefik.http.routers.cert-manager-metrics.middlewares=admin-vpn@file,secure-headers@file
      - traefik.http.routers.cert-manager-metrics.service=cert-manager-metrics
      - traefik.http.services.cert-manager-metrics.loadbalancer.server.port=8080
      # Health endpoint (internal)
      - traefik.http.routers.cert-manager-health.rule=Host(`certs.${DOMAIN}`) && PathPrefix(`/health`)
      - traefik.http.routers.cert-manager-health.entrypoints=websecure
      - traefik.http.routers.cert-manager-health.tls.certresolver=le
      - traefik.http.routers.cert-manager-health.middlewares=admin-vpn@file,secure-headers@file
      - traefik.http.routers.cert-manager-health.service=cert-manager-health
      - traefik.http.services.cert-manager-health.loadbalancer.server.port=8080
      # API endpoint (VPN-only access)
      - traefik.http.routers.cert-manager-api.rule=Host(`certs.${DOMAIN}`) && PathPrefix(`/certificates`)
      - traefik.http.routers.cert-manager-api.entrypoints=websecure
      - traefik.http.routers.cert-manager-api.tls.certresolver=le
      - traefik.http.routers.cert-manager-api.middlewares=admin-vpn@file,secure-headers@file
      - traefik.http.routers.cert-manager-api.service=cert-manager-api
      - traefik.http.services.cert-manager-api.loadbalancer.server.port=8080
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8080/metrics')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          memory: 128M
    profiles: ["monitoring"]

  # Controlled Auto-Updates with Testing Policy
  watchtower:
    image: containrrr/watchtower:1.7.0
    container_name: watchtower
    restart: unless-stopped
    networks: [monitoring]  # Monitoring network for metrics
    command:
      - --schedule=0 2 * * SUN        # Weekly updates on Sundays at 2 AM
      - --label-enable                # Only update labeled containers
      - --cleanup                     # Remove old images after update
      - --debug                       # Enable debug logging
    environment:
      # Metrics collection
      WATCHTOWER_HTTP_API_METRICS: true
      WATCHTOWER_HTTP_API_TOKEN: /run/secrets/watchtower_api_token
      # Conservative update policy - explicitly disable restart features
      WATCHTOWER_NO_RESTART: true
      WATCHTOWER_INCLUDE_STOPPED: false
      WATCHTOWER_INCLUDE_RESTARTING: false
      WATCHTOWER_REVIVE_STOPPED: false
    secrets:
      - watchtower_email_password
      - watchtower_api_token
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    labels:
      # Watchtower monitors itself (with caution)
      - com.centurylinklabs.watchtower.enable=false
      - traefik.enable=true
      - traefik.docker.network=securenexus-fullstack_monitoring
      # Metrics endpoint (VPN-only access)
      - traefik.http.routers.watchtower.rule=Host(`updates.${DOMAIN}`)
      - traefik.http.routers.watchtower.entrypoints=websecure
      - traefik.http.routers.watchtower.tls.certresolver=le
      - traefik.http.routers.watchtower.middlewares=admin-vpn@file,secure-headers@file
      - traefik.http.services.watchtower.loadbalancer.server.port=8080
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8080/v1/update"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '0.3'
          memory: 128M
        reservations:
          memory: 64M
    profiles: ["monitoring"]

  # Container Management

  # Portainer - Docker container management with SSO
  portainer:
    image: portainer/portainer-ee:latest
    container_name: portainer
    restart: unless-stopped
    networks: [proxy]
    command: -H unix:///var/run/docker.sock
    volumes:
      - portainer-data:/data
      - /var/run/docker.sock:/var/run/docker.sock
    labels:
      - traefik.enable=true
      - traefik.docker.network=securenexus-fullstack_proxy
      # HTTPS router
      - traefik.http.routers.frontend.rule=Host(`portainer.${DOMAIN}`)
      - traefik.http.routers.frontend.entrypoints=websecure
      - traefik.http.services.frontend.loadbalancer.server.port=9000
      - traefik.http.routers.frontend.service=frontend
      - traefik.http.routers.frontend.tls.certresolver=le
      - traefik.http.routers.frontend.middlewares=secure-headers@file
      # HTTP router (redirect to HTTPS)
      - traefik.http.routers.frontend-http.rule=Host(`portainer.${DOMAIN}`)
      - traefik.http.routers.frontend-http.entrypoints=web
      - traefik.http.routers.frontend-http.middlewares=redirect-to-https@file
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          memory: 256M
    profiles: ["portal"]

  # DNS: CoreDNS with etcd + MySQL backends + DNS Updater + ACME TXT Webhook

  # etcd - Key-value store for dynamic DNS records
  etcd:
    image: quay.io/coreos/etcd:v3.5.16
    restart: unless-stopped
    networks: [proxy]
    command:
      - /usr/local/bin/etcd
      - --name=etcd0
      - --data-dir=/etcd-data
      - --listen-client-urls=http://0.0.0.0:2379
      - --advertise-client-urls=http://etcd:2379
      - --listen-peer-urls=http://0.0.0.0:2380
      - --initial-advertise-peer-urls=http://etcd:2380
      - --initial-cluster=etcd0=http://etcd:2380
      - --initial-cluster-state=new
      - --initial-cluster-token=etcd-cluster
    volumes:
      - etcd-data:/etcd-data
    healthcheck:
      test: ["CMD", "etcdctl", "endpoint", "health"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles: ["dns"]

  # MySQL database for CoreDNS mysql plugin
  mysql-db:
    image: mysql:8.0
    restart: unless-stopped
    networks: [database]  # Secure database network
    environment:
      MYSQL_DATABASE: coredns
      MYSQL_USER: coredns
      MYSQL_ROOT_PASSWORD_FILE: /run/secrets/mysql_password
      MYSQL_PASSWORD_FILE: /run/secrets/mysql_password
    command: |
      mysqld
        --ssl-cert=/run/secrets/mysql_server_cert
        --ssl-key=/run/secrets/mysql_server_key
        --require-secure-transport=ON
        --ssl-cipher=ECDHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384
        --tls-version=TLSv1.2,TLSv1.3
    secrets:
      - mysql_password
      - mysql_server_cert
      - mysql_server_key
    volumes:
      - mysql-data:/var/lib/mysql
      - ./dns/mysql-init:/docker-entrypoint-initdb.d:ro
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    profiles: ["dns"]

  # CoreDNS - Modern DNS server with plugin architecture
  coredns:
    image: coredns/coredns:latest
    restart: unless-stopped
    depends_on: [etcd, mysql-db]
    networks: [proxy]
    command: -conf /etc/coredns/Corefile
    ports:
      - "0.0.0.0:53:53/tcp"     # Standard DNS port
      - "0.0.0.0:53:53/udp"     # Standard DNS port
      - "853:853/tcp"           # DNS-over-TLS
      - "8181:8080"            # Health check endpoint
      - "9153:9153"            # Prometheus metrics
    environment:
      - MYSQL_PASSWORD_FILE=/run/secrets/mysql_password
    secrets: [mysql_password]
    volumes:
      - ./dns/Corefile:/etc/coredns/Corefile:ro
      - ./dns/certs:/etc/coredns/certs:ro
      - ./dns/dnssec_keys:/etc/coredns/dnssec_keys
      - ./dns/zones:/etc/coredns/zones:ro
    labels:
      - traefik.enable=true
      - traefik.http.routers.coredns.rule=Host(`dns.${DOMAIN}`)
      - traefik.http.routers.coredns.entrypoints=websecure
      - traefik.http.routers.coredns.tls.certresolver=le
      - traefik.http.routers.coredns.middlewares=admin-vpn@file,secure-headers@file
      - traefik.http.services.coredns.loadbalancer.server.port=8080
      - "coredns.name=dns"
    healthcheck:
      test: ["CMD", "/coredns", "-version"]
      interval: 30s
      timeout: 5s
      retries: 3
    profiles: ["dns"]

  # DNS Updater - Automatically creates DNS records based on container labels
  dns-updater:
    image: alpine:3.20
    restart: unless-stopped
    depends_on: [etcd]
    networks: [proxy]
    environment:
      - DOMAIN=${DOMAIN}
      - ETCD_HOST=etcd
      - ETCD_PORT=2379
      - DNS_TTL=300
    volumes:
      - ./scripts/dns-updater.sh:/usr/local/bin/dns-updater:ro
    entrypoint: ["/bin/sh", "-c"]
    command: >
      "apk add --no-cache curl jq bash docker-cli && \
       while true; do /usr/local/bin/dns-updater || true; sleep 30; done"
    healthcheck:
      test: ["CMD", "pgrep", "-f", "dns-updater"]
      interval: 30s
      timeout: 5s
      retries: 3
    profiles: ["dns"]

  # etcd Browser - Web UI for viewing etcd data (optional)
  # Commented out for now - can be enabled if needed
  # etcd-browser:
  #   image: evildecay/etcdkeeper:latest
  #   restart: unless-stopped
  #   networks: [proxy]
  #   environment:
  #     HOST: 0.0.0.0
  #   command: ["etcdkeeper", "-h", "0.0.0.0", "-p", "8080"]
  #   labels:
  #     - traefik.enable=true
  #     - traefik.http.routers.etcd-ui.rule=Host(`etcd-ui.${DOMAIN}`)
  #     - traefik.http.routers.etcd-ui.entrypoints=websecure
  #     - traefik.http.routers.etcd-ui.tls.certresolver=le
  #     - traefik.http.routers.etcd-ui.middlewares=admin-vpn@file,secure-headers@file
  #     - traefik.http.services.etcd-ui.loadbalancer.server.port=8080
  #     - "coredns.name=etcd-ui"
  #   profiles: ["dns"]

  # ACME TXT webhook - Updates etcd for DNS-01 challenges
  acme_webhook:
    image: python:3.12-alpine
    restart: unless-stopped
    networks: [proxy]
    depends_on: [etcd]
    working_dir: /app
    volumes:
      - ./acme-webhook-etcd:/app:ro
    environment:
      ETCD_HOST: etcd
      ETCD_PORT: 2379
      DNS_TTL: "120"
    entrypoint: ["sh", "-c"]
    command: >
      "pip install --no-cache-dir flask etcd3 && \
       python3 /app/app.py"
    healthcheck:
      test: ["CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:5000/health')"]
      interval: 30s
      timeout: 5s
      retries: 3
    profiles: ["dns"]

  wellknown:
    image: caddy:2-alpine
    restart: unless-stopped
    networks: [proxy]
    volumes:
      - ./wellknown/Caddyfile:/etc/caddy/Caddyfile:ro
    labels:
      - traefik.enable=true
      - traefik.http.routers.wk.rule=PathPrefix(`/.well-known/`) && !PathPrefix(`/.well-known/acme-challenge/`)
      - traefik.http.routers.wk.entrypoints=websecure
      - traefik.http.routers.wk.tls=true
      - traefik.http.routers.wk.middlewares=secure-headers@file
      - traefik.http.services.wk.loadbalancer.server.port=80

  # Mail - mailcow
  # NOTE: mailcow requires its own docker-compose.yml in a subdirectory
  # See mail/mailcow/ for mailcow installation
  # Ports exposed: 25, 143, 465, 587, 993 (handled by mailcow's compose)

  brand-static:
    image: nginx:alpine
    container_name: brand-static
    restart: unless-stopped
    volumes:
      - ./branding:/usr/share/nginx/html:ro
    labels:
      - traefik.enable=true
      - traefik.http.routers.brand-static.rule=Host(`brand.${DOMAIN}`)
      - traefik.http.routers.brand-static.entrypoints=websecure
      - traefik.http.routers.brand-static.tls=true
      - traefik.http.middlewares.brand-headers.headers.customResponseHeaders.Cache-Control=max-age=86400,public
      - traefik.http.routers.brand-static.middlewares=brand-headers@docker
    networks:
      - proxy

  # Byrne Accounting Services
  # Marketing website
  byrne-website:
    build:
      context: ./byrne-website
      dockerfile: Dockerfile
    container_name: byrne-website
    restart: unless-stopped
    networks: [proxy]
    labels:
      - traefik.enable=true
      - traefik.docker.network=securenexus-fullstack_proxy

      # Main website - HTTPS router
      - traefik.http.routers.byrne.rule=Host(`byrne-accounts.org`) || Host(`www.byrne-accounts.org`)
      - traefik.http.routers.byrne.entrypoints=websecure
      - traefik.http.routers.byrne.tls.certresolver=le
      - traefik.http.routers.byrne.middlewares=secure-headers@file
      - traefik.http.routers.byrne.service=byrne

      # Main website - HTTP router (redirect to HTTPS)
      - traefik.http.routers.byrne-http.rule=Host(`byrne-accounts.org`) || Host(`www.byrne-accounts.org`)
      - traefik.http.routers.byrne-http.entrypoints=web
      - traefik.http.routers.byrne-http.middlewares=redirect-to-https@file

      # Client Portal - HTTPS router
      - traefik.http.routers.byrne-portal.rule=Host(`portal.byrne-accounts.org`)
      - traefik.http.routers.byrne-portal.entrypoints=websecure
      - traefik.http.routers.byrne-portal.tls.certresolver=le
      - traefik.http.routers.byrne-portal.middlewares=secure-headers@file
      - traefik.http.routers.byrne-portal.service=byrne

      # Client Portal - HTTP router (redirect to HTTPS)
      - traefik.http.routers.byrne-portal-http.rule=Host(`portal.byrne-accounts.org`)
      - traefik.http.routers.byrne-portal-http.entrypoints=web
      - traefik.http.routers.byrne-portal-http.middlewares=redirect-to-https@file

      # Service definition
      - traefik.http.services.byrne.loadbalancer.server.port=80
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://127.0.0.1/health || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
    profiles: ["byrne"]

  # ERPNext database
  erpnext-db:
    image: mariadb:10.6
    container_name: erpnext-db
    restart: unless-stopped
    networks: [proxy]
    command: --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci --skip-character-set-client-handshake --skip-innodb-read-only-compressed
    environment:
      MYSQL_ROOT_PASSWORD_FILE: /run/secrets/erpnext_db_password
      MYSQL_DATABASE: erpnext
      MYSQL_USER: erpnext
      MYSQL_PASSWORD_FILE: /run/secrets/erpnext_db_password
    secrets: [erpnext_db_password]
    volumes:
      - erpnext-db-data:/var/lib/mysql
      - ./frappe_docker/mariadb.cnf:/etc/mysql/conf.d/custom.cnf:ro
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    healthcheck:
      test: ["CMD", "healthcheck.sh", "--connect", "--innodb_initialized"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 60s
    profiles: ["byrne"]

  # ERPNext Redis cache
  erpnext-redis-cache:
    image: redis:7-alpine
    container_name: erpnext-redis-cache
    restart: unless-stopped
    networks: [proxy]
    command: ["sh", "-c", "exec redis-server --requirepass \"$$(cat /run/secrets/erpnext_redis_cache_password)\" --maxmemory 512mb --maxmemory-policy allkeys-lru"]
    secrets: [erpnext_redis_cache_password]
    volumes:
      - erpnext-redis-cache-data:/data
    healthcheck:
      test: ["CMD-SHELL", "redis-cli -a \"$$(cat /run/secrets/erpnext_redis_cache_password)\" ping | grep PONG"]
      interval: 10s
      timeout: 5s
      retries: 5
    profiles: ["byrne"]

  # ERPNext Redis queue
  erpnext-redis-queue:
    image: redis:7-alpine
    container_name: erpnext-redis-queue
    restart: unless-stopped
    networks: [proxy]
    command: ["sh", "-c", "exec redis-server --requirepass \"$$(cat /run/secrets/erpnext_redis_queue_password)\" --maxmemory 256mb --maxmemory-policy noeviction --save 60 1000"]
    secrets: [erpnext_redis_queue_password]
    volumes:
      - erpnext-redis-queue-data:/data
    healthcheck:
      test: ["CMD-SHELL", "redis-cli -a \"$$(cat /run/secrets/erpnext_redis_queue_password)\" ping | grep PONG"]
      interval: 10s
      timeout: 5s
      retries: 5
    profiles: ["byrne"]

  # ERPNext configurator (creates site on first run)
  erpnext-configurator:
    image: erpnext-posawesome:latest
    container_name: erpnext-configurator
    restart: "no"
    depends_on:
      erpnext-db:
        condition: service_healthy
      erpnext-redis-cache:
        condition: service_healthy
      erpnext-redis-queue:
        condition: service_healthy
    networks: [proxy]
    secrets:
      - erpnext_db_password
      - erpnext_admin_password
    environment:
      DB_ROOT_PASSWORD_FILE: /run/secrets/erpnext_db_password
      ADMIN_PASSWORD_FILE: /run/secrets/erpnext_admin_password
    command:
      - bash
      - -c
      - |
        if [ ! -f '/home/frappe/frappe-bench/sites/erp.byrne-accounts.org/site_config.json' ]; then
          echo 'Initializing new ERPNext site...';
          DB_ROOT_PASSWORD=$(cat /run/secrets/erpnext_db_password)
          ADMIN_PASSWORD=$(cat /run/secrets/erpnext_admin_password)
          bench new-site erp.byrne-accounts.org \
            --mariadb-root-password "$$DB_ROOT_PASSWORD" \
            --admin-password "$$ADMIN_PASSWORD" \
            --db-host erpnext-db \
            --db-type mariadb \
            --install-app erpnext \
            --no-mariadb-socket;
          bench --site erp.byrne-accounts.org set-config developer_mode 0;
          bench --site erp.byrne-accounts.org set-config maintenance_mode 0;
          echo 'Site initialization complete';
        else
          echo 'Site already exists, skipping initialization';
        fi
    volumes:
      - erpnext-sites-data:/home/frappe/frappe-bench/sites
      - erpnext-assets-data:/home/frappe/frappe-bench/sites/assets
      - ./branding:/branding:ro
      - ./erp/branding:/custom-branding:ro
    profiles: ["byrne"]

  # ERPNext backend (Frappe/ERPNext application server)
  erpnext-backend:
    image: erpnext-posawesome:latest
    container_name: erpnext-backend
    restart: unless-stopped
    depends_on:
      erpnext-db:
        condition: service_healthy
      erpnext-redis-cache:
        condition: service_healthy
      erpnext-redis-queue:
        condition: service_healthy
      erpnext-configurator:
        condition: service_completed_successfully
    networks: [proxy]
    environment:
      SITE_NAME: erp.byrne-accounts.org
    command: ["bash", "-c", "cd /home/frappe/frappe-bench && bench serve --port 8000"]
    volumes:
      - erpnext-sites-data:/home/frappe/frappe-bench/sites
      - erpnext-assets-data:/home/frappe/frappe-bench/sites/assets
      - ./branding:/branding:ro
      - ./erp/branding:/custom-branding:ro
    labels:
      - traefik.enable=true
      - traefik.docker.network=securenexus-fullstack_proxy
      # ERP access - temporarily public for initial setup (add SSO after Authentik outpost configured)
      - traefik.http.routers.erp.rule=Host(`erp.byrne-accounts.org`)
      - traefik.http.routers.erp.entrypoints=websecure
      - traefik.http.routers.erp.tls.certresolver=le
      - traefik.http.routers.erp.middlewares=secure-headers@file
      - traefik.http.routers.erp.service=erp
      - traefik.http.services.erp.loadbalancer.server.port=8000
      # HTTP redirect to HTTPS
      - traefik.http.routers.erp-http.rule=Host(`erp.byrne-accounts.org`)
      - traefik.http.routers.erp-http.entrypoints=web
      - traefik.http.routers.erp-http.middlewares=redirect-to-https@file
      # POS access subdomain - temporarily public for initial setup
      - traefik.http.routers.pos.rule=Host(`pos.byrne-accounts.org`)
      - traefik.http.routers.pos.entrypoints=websecure
      - traefik.http.routers.pos.tls.certresolver=le
      - traefik.http.routers.pos.middlewares=secure-headers@file
      - traefik.http.routers.pos.service=erp
      - traefik.http.routers.pos-http.rule=Host(`pos.byrne-accounts.org`)
      - traefik.http.routers.pos-http.entrypoints=web
      - traefik.http.routers.pos-http.middlewares=redirect-to-https@file
    healthcheck:
      test: ["CMD-SHELL", "curl -f -H 'Host: erp.byrne-accounts.org' http://localhost:8000/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    profiles: ["byrne"]

  # ERPNext SocketIO (real-time updates)
  erpnext-socketio:
    image: erpnext-posawesome:latest
    container_name: erpnext-socketio
    restart: unless-stopped
    depends_on:
      - erpnext-backend
    networks: [proxy]
    environment:
      SITE_NAME: erp.byrne-accounts.org
      REDIS_CACHE: redis://erpnext-redis-cache:6379
      REDIS_QUEUE: redis://erpnext-redis-queue:6379
      REDIS_SOCKETIO: redis://erpnext-redis-cache:6379
    command: ["bash", "-c", "cd /home/frappe/frappe-bench && node apps/frappe/socketio.js"]
    volumes:
      - erpnext-sites-data:/home/frappe/frappe-bench/sites
    labels:
      - traefik.enable=true
      - traefik.docker.network=securenexus-fullstack_proxy
      - traefik.http.routers.erpnext-socketio.rule=Host(`erp.byrne-accounts.org`) && PathPrefix(`/socket.io`)
      - traefik.http.routers.erpnext-socketio.entrypoints=websecure
      - traefik.http.routers.erpnext-socketio.tls.certresolver=le
      - traefik.http.services.erpnext-socketio.loadbalancer.server.port=9000
    healthcheck:
      test: ["CMD-SHELL", "test -f /proc/1/comm && exit 0 || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s
    profiles: ["byrne"]

  # ERPNext worker (background jobs)
  erpnext-worker:
    image: erpnext-posawesome:latest
    container_name: erpnext-worker
    restart: unless-stopped
    depends_on:
      - erpnext-backend
    networks: [proxy]
    environment:
      SITE_NAME: erp.byrne-accounts.org
      REDIS_CACHE: redis://erpnext-redis-cache:6379
      REDIS_QUEUE: redis://erpnext-redis-queue:6379
    command: ["bash", "-c", "cd /home/frappe/frappe-bench && bench --site erp.byrne-accounts.org worker --queue default,short,long"]
    volumes:
      - erpnext-sites-data:/home/frappe/frappe-bench/sites
      - erpnext-assets-data:/home/frappe/frappe-bench/sites/assets
    healthcheck:
      test: ["CMD-SHELL", "test -f /proc/1/comm && exit 0 || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    profiles: ["byrne"]

  # ERPNext scheduler (scheduled tasks)
  erpnext-scheduler:
    image: erpnext-posawesome:latest
    container_name: erpnext-scheduler
    restart: unless-stopped
    depends_on:
      - erpnext-backend
    networks: [proxy]
    environment:
      SITE_NAME: erp.byrne-accounts.org
    command: ["bash", "-c", "cd /home/frappe/frappe-bench && bench --site erp.byrne-accounts.org schedule"]
    volumes:
      - erpnext-sites-data:/home/frappe/frappe-bench/sites
      - erpnext-assets-data:/home/frappe/frappe-bench/sites/assets
    healthcheck:
      test: ["CMD-SHELL", "test -f /proc/1/comm && exit 0 || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    profiles: ["byrne"]


  # ====================================================================================
  # DICKSON SUPPLIES - CLIENT ERP INSTANCE
  # ====================================================================================
  
  # Dickson MariaDB database
  dickson-db:
    image: mariadb:10.6
    container_name: dickson-db
    restart: unless-stopped
    networks: [proxy]
    environment:
      MARIADB_ROOT_PASSWORD_FILE: /run/secrets/dickson_db_password
      MARIADB_CHARACTER_SET_SERVER: utf8mb4
      MARIADB_COLLATION_SERVER: utf8mb4_unicode_ci
    secrets: [dickson_db_password]
    volumes:
      - dickson-db-data:/var/lib/mysql
    healthcheck:
      test: ["CMD", "healthcheck.sh", "--connect", "--innodb_initialized"]
      interval: 10s
      timeout: 5s
      retries: 10
    profiles: ["dickson"]

  # Dickson Redis cache
  dickson-redis-cache:
    image: redis:7-alpine
    container_name: dickson-redis-cache
    restart: unless-stopped
    networks: [proxy]
    command: ["sh", "-c", "exec redis-server --requirepass \"$$(cat /run/secrets/dickson_redis_cache_password)\" --maxmemory 512mb --maxmemory-policy allkeys-lru"]
    secrets: [dickson_redis_cache_password]
    volumes:
      - dickson-redis-cache-data:/data
    healthcheck:
      test: ["CMD-SHELL", "redis-cli -a \"$$(cat /run/secrets/dickson_redis_cache_password)\" ping | grep PONG"]
      interval: 10s
      timeout: 5s
      retries: 5
    profiles: ["dickson"]

  # Dickson Redis queue
  dickson-redis-queue:
    image: redis:7-alpine
    container_name: dickson-redis-queue
    restart: unless-stopped
    networks: [proxy]
    command: ["sh", "-c", "exec redis-server --requirepass \"$$(cat /run/secrets/dickson_redis_queue_password)\" --maxmemory 256mb --maxmemory-policy noeviction --save 60 1000"]
    secrets: [dickson_redis_queue_password]
    volumes:
      - dickson-redis-queue-data:/data
    healthcheck:
      test: ["CMD-SHELL", "redis-cli -a \"$$(cat /run/secrets/dickson_redis_queue_password)\" ping | grep PONG"]
      interval: 10s
      timeout: 5s
      retries: 5
    profiles: ["dickson"]

  # Dickson configurator (creates site on first run)
  dickson-configurator:
    image: erpnext-posawesome:latest
    container_name: dickson-configurator
    restart: "no"
    depends_on:
      dickson-db:
        condition: service_healthy
      dickson-redis-cache:
        condition: service_healthy
      dickson-redis-queue:
        condition: service_healthy
    networks: [proxy]
    secrets: [dickson_db_password, dickson_admin_password]
    command:
      - bash
      - -c
      - |
        DB_ROOT_PASSWORD=$$(cat /run/secrets/dickson_db_password)
        ADMIN_PASSWORD=$$(cat /run/secrets/dickson_admin_password)
        if [ ! -f '/home/frappe/frappe-bench/sites/erp.dickson-supplies.com/site_config.json' ]; then
          echo 'Initializing new Dickson ERPNext site...';
          bench new-site erp.dickson-supplies.com \
            --mariadb-root-password "$$DB_ROOT_PASSWORD" \
            --admin-password "$$ADMIN_PASSWORD" \
            --db-host dickson-db \
            --db-type mariadb \
            --install-app erpnext \
            --no-mariadb-socket;
          bench --site erp.dickson-supplies.com set-config developer_mode 0;
          bench --site erp.dickson-supplies.com set-config maintenance_mode 0;
          echo 'Dickson site initialization complete';
        else
          echo 'Dickson site already exists, skipping initialization';
        fi
    volumes:
      - dickson-sites-data:/home/frappe/frappe-bench/sites
      - dickson-assets-data:/home/frappe/frappe-bench/sites/assets
      - ./erp/dickson-branding:/custom-branding:ro
    profiles: ["dickson"]

  # Dickson backend (Frappe/ERPNext application server)
  dickson-backend:
    image: erpnext-posawesome:latest
    container_name: dickson-backend
    restart: unless-stopped
    depends_on:
      dickson-db:
        condition: service_healthy
      dickson-redis-cache:
        condition: service_healthy
      dickson-redis-queue:
        condition: service_healthy
      dickson-configurator:
        condition: service_completed_successfully
    networks: [proxy]
    environment:
      SITE_NAME: erp.dickson-supplies.com
    command: ["bash", "-c", "cd /home/frappe/frappe-bench && bench serve --port 8000"]
    volumes:
      - dickson-sites-data:/home/frappe/frappe-bench/sites
      - dickson-assets-data:/home/frappe/frappe-bench/sites/assets
      - ./erp/dickson-branding:/custom-branding:ro
    labels:
      - traefik.enable=true
      - traefik.docker.network=securenexus-fullstack_proxy
      - traefik.http.services.dickson.loadbalancer.server.port=8000
      # ERP domain
      - traefik.http.routers.dickson-erp.rule=Host(`erp.dickson-supplies.com`)
      - traefik.http.routers.dickson-erp.entrypoints=websecure
      - traefik.http.routers.dickson-erp.tls.certresolver=le
      - traefik.http.routers.dickson-erp.middlewares=secure-headers@file
      - traefik.http.routers.dickson-erp.service=dickson
      - traefik.http.routers.dickson-erp-http.rule=Host(`erp.dickson-supplies.com`)
      - traefik.http.routers.dickson-erp-http.entrypoints=web
      - traefik.http.routers.dickson-erp-http.middlewares=redirect-to-https@file
      # POS domain
      - traefik.http.routers.dickson-pos.rule=Host(`pos.dickson-supplies.com`)
      - traefik.http.routers.dickson-pos.entrypoints=websecure
      - traefik.http.routers.dickson-pos.tls.certresolver=le
      - traefik.http.routers.dickson-pos.middlewares=secure-headers@file
      - traefik.http.routers.dickson-pos.service=dickson
      - traefik.http.routers.dickson-pos-http.rule=Host(`pos.dickson-supplies.com`)
      - traefik.http.routers.dickson-pos-http.entrypoints=web
      - traefik.http.routers.dickson-pos-http.middlewares=redirect-to-https@file
    healthcheck:
      test: ["CMD-SHELL", "curl -f -H 'Host: erp.dickson-supplies.com' http://localhost:8000/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    profiles: ["dickson"]

  # Dickson SocketIO (real-time updates)
  dickson-socketio:
    image: erpnext-posawesome:latest
    container_name: dickson-socketio
    restart: unless-stopped
    depends_on:
      - dickson-backend
    networks: [proxy]
    environment:
      SITE_NAME: erp.dickson-supplies.com
      REDIS_CACHE: redis://dickson-redis-cache:6379
      REDIS_QUEUE: redis://dickson-redis-queue:6379
      REDIS_SOCKETIO: redis://dickson-redis-cache:6379
    command: ["bash", "-c", "cd /home/frappe/frappe-bench && node apps/frappe/socketio.js"]
    volumes:
      - dickson-sites-data:/home/frappe/frappe-bench/sites
    labels:
      - traefik.enable=true
      - traefik.docker.network=securenexus-fullstack_proxy
      - traefik.http.routers.dickson-socketio.rule=Host(`erp.dickson-supplies.com`) && PathPrefix(`/socket.io`)
      - traefik.http.routers.dickson-socketio.entrypoints=websecure
      - traefik.http.routers.dickson-socketio.tls.certresolver=le
      - traefik.http.services.dickson-socketio.loadbalancer.server.port=9000
    healthcheck:
      test: ["CMD-SHELL", "test -f /proc/1/comm && exit 0 || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s
    profiles: ["dickson"]

  # Dickson worker (background jobs)
  dickson-worker:
    image: erpnext-posawesome:latest
    container_name: dickson-worker
    restart: unless-stopped
    depends_on:
      - dickson-backend
    networks: [proxy]
    environment:
      SITE_NAME: erp.dickson-supplies.com
      REDIS_CACHE: redis://dickson-redis-cache:6379
      REDIS_QUEUE: redis://dickson-redis-queue:6379
    command: ["bash", "-c", "cd /home/frappe/frappe-bench && bench --site erp.dickson-supplies.com worker --queue default,short,long"]
    volumes:
      - dickson-sites-data:/home/frappe/frappe-bench/sites
      - dickson-assets-data:/home/frappe/frappe-bench/sites/assets
    healthcheck:
      test: ["CMD-SHELL", "test -f /proc/1/comm && exit 0 || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    profiles: ["dickson"]

  # Dickson scheduler (scheduled tasks)
  dickson-scheduler:
    image: erpnext-posawesome:latest
    container_name: dickson-scheduler
    restart: unless-stopped
    depends_on:
      - dickson-backend
    networks: [proxy]
    environment:
      SITE_NAME: erp.dickson-supplies.com
    command: ["bash", "-c", "cd /home/frappe/frappe-bench && bench --site erp.dickson-supplies.com schedule"]
    volumes:
      - dickson-sites-data:/home/frappe/frappe-bench/sites
      - dickson-assets-data:/home/frappe/frappe-bench/sites/assets
    healthcheck:
      test: ["CMD-SHELL", "test -f /proc/1/comm && exit 0 || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    profiles: ["dickson"]


  # ===== CLOUD STORAGE & SYNC SERVICES =====

  # Nextcloud Database
  nextcloud-db:
    image: postgres:15-alpine
    restart: unless-stopped
    networks: [proxy]
    environment:
      POSTGRES_USER: nextcloud
      POSTGRES_DB: nextcloud
      POSTGRES_PASSWORD_FILE: /run/secrets/nextcloud_db_password
      PGDATA: /var/lib/postgresql/data
    volumes:
      - nextcloud-db-data:/var/lib/postgresql/data
    secrets: [nextcloud_db_password]
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U nextcloud -d nextcloud"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles: ["cloud"]

  # Nextcloud Application
  nextcloud:
    image: nextcloud:29-apache
    restart: unless-stopped
    depends_on:
      - nextcloud-db
    networks: [proxy]
    environment:
      POSTGRES_HOST: nextcloud-db
      POSTGRES_DB: nextcloud
      POSTGRES_USER: nextcloud
      POSTGRES_PASSWORD_FILE: /run/secrets/nextcloud_db_password
      NEXTCLOUD_ADMIN_USER: admin
      NEXTCLOUD_ADMIN_PASSWORD_FILE: /run/secrets/nextcloud_admin_password
      NEXTCLOUD_TRUSTED_DOMAINS: "nextcloud.${DOMAIN}"
      OVERWRITEPROTOCOL: https
      OVERWRITEHOST: "nextcloud.${DOMAIN}"
      # REDIS_HOST: redis_cache  # Disabled - causes session issues
      # REDIS_HOST_PASSWORD_FILE: /run/secrets/redis_password
    volumes:
      - nextcloud-data:/var/www/html
    secrets: [nextcloud_db_password, nextcloud_admin_password, redis_password]
    labels:
      - traefik.enable=true
      # Main Nextcloud interface
      - traefik.http.routers.nextcloud.rule=Host(`nextcloud.${DOMAIN}`)
      - traefik.http.routers.nextcloud.entrypoints=websecure
      - traefik.http.routers.nextcloud.tls.certresolver=le
      - traefik.http.routers.nextcloud.middlewares=secure-headers@file,nextcloud-headers@file
      - traefik.http.services.nextcloud.loadbalancer.server.port=80
      # Nextcloud headers middleware for WebDAV and CardDAV
      - traefik.http.middlewares.nextcloud-headers.headers.customrequestheaders.X-Forwarded-Proto=https
      - traefik.http.middlewares.nextcloud-headers.headers.customrequestheaders.X-Forwarded-For=$$remote_addr
      - traefik.http.middlewares.nextcloud-headers.headers.stsSeconds=155520011
      - traefik.http.middlewares.nextcloud-headers.headers.stsIncludeSubdomains=true
      - traefik.http.middlewares.nextcloud-headers.headers.stsPreload=true
      - traefik.http.middlewares.nextcloud-headers.headers.forceSTSHeader=true
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:80/status.php || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    profiles: ["cloud"]

  # ===== NOTESNOOK SYNC SERVER INFRASTRUCTURE =====

  # ===== NOTESNOOK SELF-HOSTED SYNC SERVER =====

  # Environment validation container
  notesnook-validate:
    image: alpine:latest
    entrypoint: /bin/sh
    secrets:
      - notesnook_instance_name
      - notesnook_api_secret
      - notesnook_disable_signups
    command:
      - -c
      - |
        echo "Validating Notesnook configuration..."
        if [ ! -s /run/secrets/notesnook_instance_name ]; then
          echo "Error: Instance name not configured"
          exit 1
        fi
        if [ ! -s /run/secrets/notesnook_api_secret ]; then
          echo "Error: API secret not configured"
          exit 1
        fi
        echo "All required environment variables are set."
    restart: "no"
    profiles: ["cloud"]

  # MongoDB database with replica set
  notesnook-db:
    image: mongo:7.0.12
    hostname: notesnook-db
    restart: unless-stopped
    depends_on:
      notesnook-validate:
        condition: service_completed_successfully
    networks: [database]
    environment: {}
    volumes:
      - notesnook-db-data:/data/db
      - notesnook-db-data:/data/configdb
    command: --replSet rs0 --bind_ip_all --noauth
    healthcheck:
      test: echo 'db.runCommand("ping").ok' | mongosh mongodb://localhost:27017 --quiet
      interval: 40s
      timeout: 30s
      retries: 3
      start_period: 60s
    profiles: ["cloud"]

  # MongoDB replica set initialization
  notesnook-initiate-rs:
    image: mongo:7.0.12
    depends_on:
      - notesnook-db
    networks: [database]
    entrypoint: /bin/sh
    command:
      - -c
      - |
        mongosh mongodb://notesnook-db:27017 <<EOF
          rs.initiate();
          rs.status();
        EOF
    restart: "no"
    profiles: ["cloud"]

  # MinIO S3 storage for attachments
  notesnook-s3:
    image: minio/minio:RELEASE.2024-07-29T22-14-52Z
    restart: unless-stopped
    depends_on:
      notesnook-validate:
        condition: service_completed_successfully
    networks: [backend]
    environment:
      MINIO_ROOT_USER_FILE: /run/secrets/notesnook_s3_access_key
      MINIO_ROOT_PASSWORD_FILE: /run/secrets/notesnook_s3_password
      MINIO_BROWSER: "on"
    secrets: [notesnook_s3_access_key, notesnook_s3_password]
    volumes:
      - notesnook-s3-data:/data/s3
    command: server /data/s3 --console-address :9090
    healthcheck:
      test: timeout 5s bash -c ':> /dev/tcp/127.0.0.1/9000' || exit 1
      interval: 40s
      timeout: 30s
      retries: 3
      start_period: 60s
    profiles: ["cloud"]

  # S3 bucket setup
  notesnook-setup-s3:
    image: minio/mc:RELEASE.2024-07-26T13-08-44Z
    depends_on:
      - notesnook-s3
    networks: [backend]
    secrets: [notesnook_s3_access_key, notesnook_s3_password]
    entrypoint: /bin/bash
    command:
      - -c
      - |
        ACCESS_KEY=$(cat /run/secrets/notesnook_s3_access_key)
        SECRET_KEY=$(cat /run/secrets/notesnook_s3_password)
        until mc alias set minio http://notesnook-s3:9000/ $ACCESS_KEY $SECRET_KEY; do
          sleep 1;
        done;
        mc mb minio/attachments -p
    restart: "no"
    profiles: ["cloud"]

  # Identity/Auth server
  notesnook-identity:
    image: notesnook-identity:source
    restart: unless-stopped
    depends_on:
      - notesnook-db
      - notesnook-initiate-rs
    networks: [proxy, backend, database]
    environment:
      # Official Repository Format
      AUTH_SERVER_PUBLIC_URL: https://identity.${DOMAIN}
      NOTESNOOK_APP_PUBLIC_URL: https://notes.${DOMAIN}
      MONOGRAPH_PUBLIC_URL: https://mono.${DOMAIN}
      ATTACHMENTS_SERVER_PUBLIC_URL: https://files.${DOMAIN}
      # Server Discovery (Internal)
      NOTESNOOK_SERVER_PORT: 5264
      NOTESNOOK_SERVER_HOST: notesnook-server
      IDENTITY_SERVER_PORT: 8264
      IDENTITY_SERVER_HOST: notesnook-identity
      IDENTITY_SERVER_URL: https://identity.${DOMAIN}
      SSE_SERVER_PORT: 7264
      SSE_SERVER_HOST: notesnook-sse
      # Configuration
      SELF_HOSTED: 1
      # Database connection
      MONGODB_CONNECTION_STRING: mongodb://notesnook-db:27017/identity?replSet=rs0
      MONGODB_DATABASE_NAME: identity
      # MongoDB Configuration for IdentityServer4.MongoDB
      MongoDBConfiguration__ConnectionString: mongodb://notesnook-db:27017/identity?replSet=rs0
      MongoDBConfiguration__Database: identity
      # Instance configuration
      INSTANCE_NAME: My SecureNexus Notesnook
      NOTESNOOK_API_SECRET: b9H5JY2RQEJDs0RItiMl7YDzInAjZXMCUjfxGimkOdg=
      DISABLE_SIGNUPS: false
    volumes:
      - ./config/notesnook/appsettings.json:/app/appsettings.json:ro
    labels:
      - traefik.enable=true
      - traefik.http.routers.notesnook-auth.rule=Host(`auth.${DOMAIN}`)
      - traefik.http.routers.notesnook-auth.entrypoints=websecure
      - traefik.http.routers.notesnook-auth.tls.certresolver=le
      - traefik.http.routers.notesnook-auth.middlewares=secure-headers@file,crowdsec-fa@file
      - traefik.http.services.notesnook-auth.loadbalancer.server.port=8264
    healthcheck:
      test: ["CMD", "wget", "--tries=1", "-nv", "-q", "http://localhost:8264/health", "-O-"]
      interval: 40s
      timeout: 30s
      retries: 3
      start_period: 60s
    profiles: ["cloud"]

  # Main Notesnook sync server
  notesnook-server:
    image: notesnook-server:source
    restart: unless-stopped
    depends_on:
      - notesnook-s3
      - notesnook-setup-s3
      - notesnook-identity
    networks: [proxy, backend, database]
    secrets:
      - notesnook_s3_access_key
      - notesnook_s3_password
      - notesnook_connection_string
    environment:
      # Official Repository Format
      AUTH_SERVER_PUBLIC_URL: https://identity.${DOMAIN}
      NOTESNOOK_APP_PUBLIC_URL: https://notes.${DOMAIN}
      MONOGRAPH_PUBLIC_URL: https://mono.${DOMAIN}
      ATTACHMENTS_SERVER_PUBLIC_URL: https://files.${DOMAIN}
      # Server Discovery (Internal)
      NOTESNOOK_SERVER_PORT: 5264
      NOTESNOOK_SERVER_HOST: notesnook-server
      IDENTITY_SERVER_PORT: 8264
      IDENTITY_SERVER_HOST: notesnook-identity
      IDENTITY_SERVER_URL: https://identity.${DOMAIN}
      SSE_SERVER_PORT: 7264
      SSE_SERVER_HOST: notesnook-sse
      SELF_HOSTED: 1
      # Database connection
      MONGODB_CONNECTION_STRING: mongodb://notesnook-db:27017/notesnook?replSet=rs0
      MONGODB_CONNECTION_STRING_FILE: /run/secrets/notesnook_connection_string
      MONGODB_DATABASE_NAME: notesnook
      # S3 configuration
      S3_INTERNAL_SERVICE_URL: "http://notesnook-s3:9000/"
      S3_INTERNAL_BUCKET_NAME: "attachments"
      S3_ACCESS_KEY_ID_FILE: /run/secrets/notesnook_s3_access_key
      S3_ACCESS_KEY_FILE: /run/secrets/notesnook_s3_password
      S3_SERVICE_URL: "https://files.${DOMAIN}/"
      S3_REGION: "us-east-1"
      S3_BUCKET_NAME: "attachments"
    labels:
      - traefik.enable=true
      # Main API endpoint with WebSocket support
      - traefik.http.routers.notesnook-notes.rule=Host(`notes.${DOMAIN}`)
      - traefik.http.routers.notesnook-notes.entrypoints=websecure
      - traefik.http.routers.notesnook-notes.tls.certresolver=le
      - traefik.http.routers.notesnook-notes.middlewares=secure-headers@file,crowdsec-fa@file
      - traefik.http.services.notesnook-notes.loadbalancer.server.port=5264
    healthcheck:
      test: ["CMD", "wget", "--tries=1", "-nv", "-q", "http://localhost:5264/health", "-O-"]
      interval: 40s
      timeout: 30s
      retries: 3
      start_period: 60s
    profiles: ["cloud"]

  # Server-Sent Events server
  notesnook-sse:
    image: streetwriters/sse:latest
    restart: unless-stopped
    depends_on:
      - notesnook-identity
      - notesnook-server
    networks: [proxy, backend, database]
    secrets: [notesnook_connection_string]
    environment:
      # Official Repository Format
      AUTH_SERVER_PUBLIC_URL: https://identity.${DOMAIN}
      NOTESNOOK_APP_PUBLIC_URL: https://notes.${DOMAIN}
      MONOGRAPH_PUBLIC_URL: https://mono.${DOMAIN}
      ATTACHMENTS_SERVER_PUBLIC_URL: https://files.${DOMAIN}
      # Server Discovery (Internal)
      NOTESNOOK_SERVER_PORT: 5264
      NOTESNOOK_SERVER_HOST: notesnook-server
      IDENTITY_SERVER_PORT: 8264
      IDENTITY_SERVER_HOST: notesnook-identity
      IDENTITY_SERVER_URL: https://identity.${DOMAIN}
      SSE_SERVER_PORT: 7264
      SSE_SERVER_HOST: notesnook-sse
      SELF_HOSTED: 1
      # Database connection
      MONGODB_CONNECTION_STRING_FILE: /run/secrets/notesnook_connection_string
    labels:
      - traefik.enable=true
      # SSE endpoint with WebSocket support
      - traefik.http.routers.notesnook-events.rule=Host(`events.${DOMAIN}`)
      - traefik.http.routers.notesnook-events.entrypoints=websecure
      - traefik.http.routers.notesnook-events.tls.certresolver=le
      - traefik.http.routers.notesnook-events.middlewares=secure-headers@file,crowdsec-fa@file
      - traefik.http.services.notesnook-events.loadbalancer.server.port=7264
    healthcheck:
      test: ["CMD", "wget", "--tries=1", "-nv", "-q", "http://localhost:7264/health", "-O-"]
      interval: 40s
      timeout: 30s
      retries: 3
      start_period: 60s
    profiles: ["cloud"]

  # MonoGraph server for public notes
  notesnook-monograph:
    image: streetwriters/monograph:latest
    restart: unless-stopped
    depends_on:
      - notesnook-server
    networks: [proxy, backend]
    environment:
      # Official Repository Format
      AUTH_SERVER_PUBLIC_URL: https://identity.${DOMAIN}
      NOTESNOOK_APP_PUBLIC_URL: https://notes.${DOMAIN}
      MONOGRAPH_PUBLIC_URL: https://mono.${DOMAIN}
      ATTACHMENTS_SERVER_PUBLIC_URL: https://files.${DOMAIN}
      # Server Discovery (Internal)
      NOTESNOOK_SERVER_PORT: 5264
      NOTESNOOK_SERVER_HOST: notesnook-server
      IDENTITY_SERVER_PORT: 8264
      IDENTITY_SERVER_HOST: notesnook-identity
      IDENTITY_SERVER_URL: https://identity.${DOMAIN}
      SSE_SERVER_PORT: 7264
      SSE_SERVER_HOST: notesnook-sse
      SELF_HOSTED: 1
      # MonoGraph configuration
      API_HOST: http://notesnook-server:5264/
      PUBLIC_URL: https://mono.${DOMAIN}/
    labels:
      - traefik.enable=true
      # MonoGraph endpoint with caching
      - traefik.http.routers.notesnook-mono.rule=Host(`mono.${DOMAIN}`)
      - traefik.http.routers.notesnook-mono.entrypoints=websecure
      - traefik.http.routers.notesnook-mono.tls.certresolver=le
      - traefik.http.routers.notesnook-mono.middlewares=secure-headers@file,crowdsec-fa@file
      - traefik.http.services.notesnook-mono.loadbalancer.server.port=3000
    healthcheck:
      test: ["CMD", "wget", "--tries=1", "-nv", "-q", "http://localhost:3000/api/health", "-O-"]
      interval: 40s
      timeout: 30s
      retries: 3
      start_period: 60s
    profiles: ["cloud"]

  # ===== KANIDM IDENTITY MANAGEMENT (TEST EVALUATION) =====

  # Kanidm - Modern Rust-based Identity Management Server
  # Test deployment alongside Authentik for evaluation
  kanidm:
    image: kanidm/server:1.1.0-rc.15
    restart: unless-stopped
    command: ["/sbin/kanidmd", "server", "-c", "/data/server.toml"]
    networks: [proxy]
    volumes:
      - kanidm-data:/data
      - ./kanidm/config/server.toml:/data/server.toml:ro
      - ./kanidm/certs/fullchain.pem:/data/fullchain.pem:ro
      - ./kanidm/certs/privkey.pem:/data/privkey.pem:ro
    environment:
      RUST_LOG: info
    labels:
      # Web UI
      - traefik.enable=true
      - traefik.http.routers.kanidm.rule=Host(`kanidm.${DOMAIN}`)
      - traefik.http.routers.kanidm.entrypoints=websecure
      - traefik.http.routers.kanidm.tls.certresolver=le
      - traefik.http.routers.kanidm.middlewares=secure-headers@file
      - traefik.http.services.kanidm.loadbalancer.server.port=8443
      - traefik.http.services.kanidm.loadbalancer.server.scheme=https
      - traefik.http.services.kanidm.loadbalancer.serverstransport=kanidm-transport@file
    healthcheck:
      test: ["CMD-SHELL", "curl -k -f https://localhost:8443/status || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    profiles: ["test"]

  # Caddy reverse proxy (replaces Traefik)
  caddy:
    image: caddy-enhanced:latest
    restart: unless-stopped
    networks: [proxy]
    ports:
      - "80:80"    # HTTP (auto-redirects to HTTPS)
      - "443:443"  # HTTPS
    environment:
      - DOMAIN=${DOMAIN}
      - EMAIL=${EMAIL}
    volumes:
      - ./config/caddy/Caddyfile:/etc/caddy/Caddyfile:ro
      - ./config/caddy/snippets:/etc/caddy/snippets:ro
      - caddy-data:/data
      - caddy-config:/config
    healthcheck:
      test: ["CMD", "caddy", "validate", "--config", "/etc/caddy/Caddyfile"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    profiles: ["core"]

# Database TLS certificates added to main secrets section above

