# docker compose v2 (no 'version:' key)
networks:
  proxy:
    external: false
    # Public-facing services only (Traefik, web apps, landing pages)
  database:
    internal: true
    # Database services only - isolated from internet
  monitoring:
    internal: true
    # Monitoring stack only - isolated from internet
  backend:
    internal: true
    # Internal services (workers, processors) - isolated from internet

volumes:
  caddy-data:
  caddy-config:
  grafana-data:
  prometheus-data:
  alertmanager-data:
  loki-data:
  redis-data:
  pg-data:
  pg-ssl:
  keycloak-db-data:
  etcd-data:
  mysql-data:
  tailscale-data:
  uptime-kuma-data:
  dashy-data:  # Dashy dashboard data
  portainer-data:  # Portainer container management data
  # stalwart-data: # Removed - switched to mailcow
  # Byrne Accounting volumes
  erpnext-db-data:
  erpnext-redis-cache-data:
  erpnext-redis-queue-data:
  erpnext-sites-data:
  erpnext-assets-data:
  # Dickson Supplies volumes
  dickson-db-data:
  dickson-redis-cache-data:
  dickson-redis-queue-data:
  dickson-sites-data:
  dickson-assets-data:
  # Nextcloud volumes
  nextcloud-data:
  nextcloud-db-data:
  # Notesnook sync server volumes
  notesnook-data:
  notesnook-db-data:
  notesnook-s3-data:
  # Kanidm test evaluation volumes
  kanidm-data:
  # Evil Rabbit Art volumes
  evilrabbit-mysql-data:
  evilrabbit-wp-data:
  # Additional Cloud Services volumes (databases shared)
  vikunja-data:
  calibre-data:
  hedgedoc-data:
  forgejo-data:
  bitwarden-data:
  firefly-data:

secrets:
  authentik_secret_key:
    file: ./secrets/authentik_secret_key.txt
  postgres_password:
    file: ./secrets/postgres_password.txt
  redis_password:
    file: ./secrets/redis_password.txt
  mysql_password:
    file: ./secrets/mysql_password.txt
  coredns_api_key:
    file: ./secrets/coredns_api_key.txt
  smtp_username:
    file: ./secrets/smtp_username.txt
  smtp_password:
    file: ./secrets/smtp_password.txt
  grafana_oauth_secret:
    file: ./secrets/grafana_oauth_secret.txt
  keycloak_db_password:
    file: ./secrets/keycloak_db_password.txt
  keycloak_admin_password:
    file: ./secrets/keycloak_admin_password.txt
  tailscale_authkey:
    file: ./secrets/tailscale_authkey.txt
  crowdsec_bouncer_api_key:
    file: ./secrets/crowdsec_bouncer_api_key
  souin_redis_password:
    file: ./secrets/souin_redis_password.txt
  calibre_password:
    file: ./secrets/calibre_password.txt
  dashy_oauth_secret:
    file: ./secrets/dashy_oauth_secret.txt
  grafana_admin_password:
    file: ./secrets/grafana_admin_password.txt
  # Byrne Accounting secrets
  erpnext_db_password:
    file: ./secrets/erpnext_db_password.txt
  erpnext_admin_password:
    file: ./secrets/erpnext_admin_password.txt
  erpnext_redis_cache_password:
    file: ./secrets/erpnext_redis_cache_password.txt
  erpnext_redis_queue_password:
    file: ./secrets/erpnext_redis_queue_password.txt
  # Dickson Supplies secrets
  dickson_db_password:
    file: ./secrets/dickson_db_password.txt
  dickson_admin_password:
    file: ./secrets/dickson_admin_password.txt
  dickson_redis_cache_password:
    file: ./secrets/dickson_redis_cache_password.txt
  dickson_redis_queue_password:
    file: ./secrets/dickson_redis_queue_password.txt
  # Nextcloud secrets
  nextcloud_db_password:
    file: ./secrets/nextcloud_db_password.txt
  nextcloud_admin_password:
    file: ./secrets/nextcloud_admin_password.txt
  # Notesnook secrets
  notesnook_db_username:
    file: ./secrets/notesnook_db_username.txt
  notesnook_db_password:
    file: ./secrets/notesnook_db_password.txt
  notesnook_connection_string:
    file: ./secrets/notesnook_connection_string.txt
  notesnook_s3_password:
    file: ./secrets/notesnook_s3_password.txt
  notesnook_jwt_secret:
    file: ./secrets/notesnook_jwt_secret.txt
  notesnook_instance_name:
    file: ./secrets/notesnook_instance_name.txt
  notesnook_api_secret:
    file: ./secrets/notesnook_api_secret.txt
  notesnook_disable_signups:
    file: ./secrets/notesnook_disable_signups.txt
  notesnook_s3_access_key:
    file: ./secrets/notesnook_s3_access_key.txt
  notesnook_mongodb_keyfile:
    file: ./secrets/notesnook_mongodb_keyfile.txt
  notesnook_oauth_secret:
    file: ./secrets/notesnook_oauth_secret.txt
  # Database TLS certificates
  postgres_server_cert:
    file: ./certs/database/postgres/server-cert.pem
  postgres_server_key:
    file: ./certs/database/postgres/server-key.pem
  mysql_server_cert:
    file: ./certs/database/mysql/server-cert.pem
  mysql_server_key:
    file: ./certs/database/mysql/server-key.pem
  mongodb_tls_cert:
    file: ./certs/database/mongodb/mongodb-combined.pem
  # Watchtower secrets
  watchtower_email_password:
    file: ./secrets/watchtower_email_password.txt
  watchtower_api_token:
    file: ./secrets/watchtower_api_token.txt
  # Evil Rabbit Art secrets
  evilrabbit_db_root_password:
    file: ./secrets/evilrabbit_db_root_password.txt
  evilrabbit_db_password:
    file: ./secrets/evilrabbit_db_password.txt
  evilrabbit_admin_password:
    file: ./secrets/evilrabbit_admin_password.txt
  # Additional Cloud Services secrets
  vikunja_db_password:
    file: ./secrets/vikunja_db_password.txt
  vikunja_jwt_secret:
    file: ./secrets/vikunja_jwt_secret.txt
  hedgedoc_db_password:
    file: ./secrets/hedgedoc_db_password.txt
  hedgedoc_session_secret:
    file: ./secrets/hedgedoc_session_secret.txt
  forgejo_db_password:
    file: ./secrets/forgejo_db_password.txt
  forgejo_secret_key:
    file: ./secrets/forgejo_secret_key.txt
  bitwarden_admin_token:
    file: ./secrets/bitwarden_admin_token.txt
  firefly_db_password:
    file: ./secrets/firefly_db_password.txt
  firefly_app_key:
    file: ./secrets/firefly_app_key.txt

services:
  docker-proxy:
    image: ghcr.io/tecnativa/docker-socket-proxy:0.1.2
    restart: unless-stopped
    networks: [proxy, database]
    environment:
      INFO: "1"
      CONTAINERS: "1"
      SERVICES: "1"
      TASKS: "1"
      NETWORKS: "1"
      PLUGINS: "0"
      POST: "0"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f haproxy > /dev/null"]
      interval: 30s
      timeout: 5s
      retries: 3
    profiles: ["core"]


  souin_redis:
    image: redis:7-alpine
    restart: unless-stopped
    networks: [proxy, database]
    command: ["sh","-c","exec redis-server --requirepass \"$$(cat /run/secrets/souin_redis_password)\""]
    secrets: [souin_redis_password]
    healthcheck:
      test: ["CMD-SHELL","redis-cli -a \"$$(cat /run/secrets/souin_redis_password)\" ping | grep PONG"]
      interval: 10s
      timeout: 5s
      retries: 5
    profiles: ["core"]

  tailscale:
    image: tailscale/tailscale:v1.76.6
    container_name: tailscale
    restart: unless-stopped
    network_mode: host
    volumes:
      - tailscale-data:/var/lib/tailscale
      - /dev/net/tun:/dev/net/tun
    cap_add:
      - NET_ADMIN
      - NET_RAW
    environment:
      - TS_AUTHKEY_FILE=/run/secrets/tailscale_authkey
      - TS_STATE_DIR=/var/lib/tailscale
      - TS_USERSPACE=false
      - TS_ACCEPT_DNS=true
    secrets: [tailscale_authkey]
    command: ["tailscaled"]
    profiles: ["core"]

  # CrowdSec + caddy bouncer
  crowdsec:
    image: crowdsecurity/crowdsec:v1.7.3
    container_name: crowdsec-1
    restart: unless-stopped
    networks:
      proxy:
        aliases:
          - crowdsec
    environment:
      COLLECTIONS: "crowdsecurity/base-http-scenarios crowdsecurity/http-cve crowdsecurity/linux crowdsecurity/sshd crowdsecurity/caddy"
      PARSERS: "crowdsecurity/docker-logs crowdsecurity/cri-logs"
      GID: "0"
      DISABLE_AGENT: "false"
      LAPI_LISTEN: "0.0.0.0:8080"
    volumes:
      - ./crowdsec/data:/var/lib/crowdsec/data
      - ./crowdsec/config:/etc/crowdsec
      - /var/log:/var/log:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    dns:
      - 1.1.1.1
      - 8.8.8.8
    healthcheck:
      test: ["CMD-SHELL", "cscli lapi status || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
    profiles: ["core"]



  # Identity

  # PostgreSQL SSL Certificate Setup Init Container (Optimized)
  postgres-ssl-setup:
    image: alpine:3.19
    restart: "no"
    networks: [database]
    command: |
      sh -c "
        echo 'üîê Setting up PostgreSQL SSL certificates with proper ownership...'

        # Create SSL directory
        mkdir -p /ssl-volume

        # Copy certificates from Docker secrets with proper ownership
        if [ -f /run/secrets/postgres_server_cert ] && [ -f /run/secrets/postgres_server_key ]; then
          cp /run/secrets/postgres_server_cert /ssl-volume/server.crt
          cp /run/secrets/postgres_server_key /ssl-volume/server.key

          # Set PostgreSQL user ownership (UID 70 in postgres:16-alpine)
          chown 70:70 /ssl-volume/server.*

          # Set secure permissions
          chmod 644 /ssl-volume/server.crt
          chmod 600 /ssl-volume/server.key

          echo '‚úÖ PostgreSQL SSL certificates prepared successfully'
          ls -la /ssl-volume/
        else
          echo '‚ùå Certificate secrets not found, PostgreSQL will start without TLS'
          echo '   Expected: /run/secrets/postgres_server_cert and /run/secrets/postgres_server_key'
          exit 1
        fi
      "
    secrets:
      - postgres_server_cert
      - postgres_server_key
    volumes:
      - pg-ssl:/ssl-volume
    profiles: ["identity"]

  authentik_db:
    image: postgres:16-alpine
    restart: unless-stopped
    depends_on:
      postgres-ssl-setup:
        condition: service_completed_successfully
    networks: [database]  # Secure database network
    environment:
      POSTGRES_DB: authentik
      POSTGRES_USER: authentik
      POSTGRES_PASSWORD_FILE: /run/secrets/postgres_password
      # Enable TLS encryption
      POSTGRES_INITDB_ARGS: "--auth-host=scram-sha-256 --auth-local=scram-sha-256"
    command: |
      postgres
        -c ssl=on
        -c ssl_cert_file=/var/lib/postgresql/ssl/server.crt
        -c ssl_key_file=/var/lib/postgresql/ssl/server.key
        -c ssl_prefer_server_ciphers=on
        -c ssl_min_protocol_version=TLSv1.2
    secrets:
      - postgres_password
    volumes:
      - pg-data:/var/lib/postgresql/data
      - pg-ssl:/var/lib/postgresql/ssl
    # Security hardening
    security_opt:
      - no-new-privileges:true
      - apparmor:docker-default
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - DAC_OVERRIDE
      - FOWNER
      - SETUID
      - SETGID
    tmpfs:
      - /tmp:noexec,nosuid,size=100m
      - /var/tmp:noexec,nosuid,size=50m
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.2'
          memory: 256M
    healthcheck:
      test: ["CMD-SHELL","pg_isready -U authentik -d authentik"]
      interval: 10s
      timeout: 5s
      retries: 5
    profiles: ["identity"]

  redis_cache:
    image: redis:7-alpine
    restart: unless-stopped
    command: ["sh","-c","exec redis-server --requirepass \"$$(cat /run/secrets/redis_password)\" --appendonly yes --appendfsync everysec"]
    secrets: [redis_password]
    volumes:
      - redis-data:/data
    networks: [database]  # Moved to secure database network
    # Security hardening
    security_opt:
      - no-new-privileges:true
      - apparmor:docker-default
    cap_drop:
      - ALL
    cap_add:
      - SETUID
      - SETGID
      - CHOWN
    user: '999:999'
    tmpfs:
      - /tmp:noexec,nosuid,size=50m
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 64M
    healthcheck:
      test: ["CMD-SHELL","redis-cli -a \"$$(cat /run/secrets/redis_password)\" ping | grep PONG"]
      interval: 10s
      timeout: 5s
      retries: 5
    profiles: ["identity"]

  authentik_server:
    image: ghcr.io/goauthentik/server:2025.10.1
    restart: unless-stopped
    depends_on: [authentik_db]
    networks: [proxy, database, backend]  # Multi-network: public access + database + internal
    command: server
    environment:
      AUTHENTIK_POSTGRESQL__HOST: authentik_db
      AUTHENTIK_POSTGRESQL__USER: authentik
      AUTHENTIK_POSTGRESQL__NAME: authentik
      AUTHENTIK_POSTGRESQL__PASSWORD: file:///run/secrets/postgres_password
      AUTHENTIK_POSTGRESQL__SSLMODE: disable
      AUTHENTIK_SECRET_KEY: file:///run/secrets/authentik_secret_key
    secrets: [postgres_password, authentik_secret_key]
    volumes: []
    # Security hardening
    security_opt:
      - no-new-privileges:true
      - apparmor:docker-default
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - DAC_OVERRIDE
      - SETUID
      - SETGID
      - NET_BIND_SERVICE
    tmpfs:
      - /tmp:noexec,nosuid,size=100m
      - /var/tmp:noexec,nosuid,size=50m
    healthcheck:
      test: ["CMD-SHELL", "python -c \"import urllib.request; urllib.request.urlopen('http://localhost:9000/-/health/ready/')\""]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 512M
    profiles: ["identity"]

  authentik_worker:
    image: ghcr.io/goauthentik/server:2025.10.1
    restart: unless-stopped
    depends_on: [authentik_db]
    networks: [database, backend]  # Internal networks only
    environment:
      AUTHENTIK_POSTGRESQL__HOST: authentik_db
      AUTHENTIK_POSTGRESQL__USER: authentik
      AUTHENTIK_POSTGRESQL__NAME: authentik
      AUTHENTIK_POSTGRESQL__PASSWORD: file:///run/secrets/postgres_password
      AUTHENTIK_POSTGRESQL__SSLMODE: disable
      AUTHENTIK_SECRET_KEY: file:///run/secrets/authentik_secret_key
    secrets: [postgres_password, authentik_secret_key]
    volumes: []
    command: worker
    healthcheck:
      test: ["CMD", "python", "-c", "import os; exit(0 if os.path.exists('/proc/1/comm') else 1)"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 1G
        reservations:
          cpus: '0.1'
          memory: 512M
    profiles: ["identity"]

  # Keycloak SSO (alternative to Authentik)
  keycloak_db:
    image: postgres:16-alpine
    restart: unless-stopped
    networks: [proxy, database]
    environment:
      POSTGRES_DB: keycloak
      POSTGRES_USER: keycloak
      POSTGRES_PASSWORD_FILE: /run/secrets/keycloak_db_password
    secrets: [keycloak_db_password]
    volumes:
      - keycloak-db-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL","pg_isready -U keycloak -d keycloak"]
      interval: 10s
      timeout: 5s
      retries: 5
    profiles: ["identity"]

  keycloak:
    image: quay.io/keycloak/keycloak:26.0.7
    restart: unless-stopped
    depends_on: [keycloak_db]
    networks: [proxy, database]
    command: start
    environment:
      KC_DB: postgres
      KC_DB_URL: jdbc:postgresql://keycloak_db:5432/keycloak
      KC_DB_USERNAME: keycloak
      KC_DB_PASSWORD: file:///run/secrets/keycloak_db_password
      KC_HOSTNAME: keycloak.${DOMAIN}
      KC_PROXY_HEADERS: xforwarded
      KC_HTTP_RELATIVE_PATH: /
      KC_HTTP_ENABLED: "true"
      KC_HEALTH_ENABLED: "true"
      KC_METRICS_ENABLED: "true"
      KEYCLOAK_ADMIN: admin
      KEYCLOAK_ADMIN_PASSWORD: file:///run/secrets/keycloak_admin_password
    secrets: [keycloak_db_password, keycloak_admin_password]
    healthcheck:
      test: ["CMD-SHELL", "exec 3<>/dev/tcp/127.0.0.1/9000 && echo -e 'GET /health/ready HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: close\\r\\n\\r\\n' >&3 && timeout 1 cat <&3 | grep -q 'UP'"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

      # No middleware - let Keycloak handle its own security headers
    profiles: ["identity"]

  # Landing & Portal
  landing:
    image: caddy:2-alpine
    restart: unless-stopped
    networks: [proxy, database]
    volumes:
      - ./landing:/usr/share/caddy:ro
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:80/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
      # HTTPS router
      # HTTP router (redirect to HTTPS)
    profiles: ["portal"]

  # ERPNext Setup Wizard Portal
  erp-setup-portal:
    build: ./erp-setup-portal
    restart: unless-stopped
    networks: [proxy, database]

      # HTTPS router
      # HTTP router (redirect to HTTPS)
    profiles: ["byrne"]


  # Dashy Dashboard (Modern replacement for Homarr)
  dashy:
    image: lissy93/dashy:latest
    container_name: dashy
    restart: unless-stopped
    command: ["node", "server"]
    networks: [proxy, database]
    environment:
      - NODE_ENV=production
      - NODE_OPTIONS=--max_old_space_size=1536
      - UID=1000
      - GID=1000
      # Base URL for proper routing
      - BASE_URL=https://dashboard.${DOMAIN}
      # Authentik OIDC configuration via secrets
      - OIDC_CLIENT_SECRET_FILE=/run/secrets/dashy_oauth_secret
    secrets:
      - dashy_oauth_secret
    volumes:
      - ./config/dashy/conf.yml:/app/public/conf.yml:ro
      - dashy-data:/app/public
      - ./config/dashy/item-icons:/app/public/item-icons:ro
    healthcheck:
      test: ["CMD", "node", "-e", "const http = require('http'); http.get('http://localhost:8080', (res) => { process.exit(res.statusCode === 200 ? 0 : 1) }).on('error', () => { process.exit(1) })"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 1G
        reservations:
          cpus: '0.05'
          memory: 128M
    profiles: ["portal"]

  # App Catalog - Custom application deployment system
  app-catalog:
    build:
      context: ./apps-catalog
      dockerfile: Dockerfile
    container_name: app-catalog
    restart: unless-stopped
    networks: [proxy, database]
    environment:
      - DOMAIN=${DOMAIN}
      - EMAIL=${EMAIL}
      - COMPOSE_PROJECT_NAME=securenexus-fullstack
    volumes:
      - ./apps-catalog/catalog:/catalog:ro
      - ./apps-catalog/deployed:/deployed
      - ./compose.yml:/compose.yml:ro
      # Docker socket access removed for security - app-catalog is high risk

      # HTTPS router
      # HTTP router (redirect to HTTPS)
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://127.0.0.1:5000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    profiles: ["portal"]

  # Documentation Wiki (MkDocs Material)
  wiki:
    build:
      context: ./wiki
      dockerfile: Dockerfile
    container_name: wiki
    restart: unless-stopped
    networks: [proxy, database]
    environment:
      - TZ=${TZ:-America/New_York}

      # HTTPS router
      # HTTP router (redirect to HTTPS)
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://127.0.0.1:8000 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    profiles: ["portal"]

  # Monitoring
  prometheus:
    image: prom/prometheus:v2.53.0
    restart: unless-stopped
    networks: [monitoring]  # Moved to secure monitoring network
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    volumes:
      - prometheus-data:/prometheus
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/blackbox.yml:/etc/prometheus/blackbox.yml:ro
      - ./monitoring/alert_rules.yml:/etc/prometheus/alert_rules.yml:ro
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          memory: 1G

    # Security hardening
    security_opt:
      - no-new-privileges:true
      - apparmor:docker-default
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - DAC_OVERRIDE
      - SETUID
      - SETGID
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=100m
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:9090/-/healthy || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    profiles: ["monitoring"]

  alertmanager:
    image: prom/alertmanager:v0.27.0
    restart: unless-stopped
    networks: [monitoring]  # Moved to secure monitoring network
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
    volumes:
      - alertmanager-data:/alertmanager
      - ./monitoring/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:9093/-/healthy || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    profiles: ["monitoring"]

  blackbox:
    image: prom/blackbox-exporter:v0.25.0
    restart: unless-stopped
    networks: [monitoring]  # Monitoring network for metrics
    command: ["--config.file=/etc/blackbox_exporter/config.yml"]
    volumes:
      - ./monitoring/blackbox.yml:/etc/blackbox_exporter/config.yml:ro
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:9115/metrics || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    profiles: ["monitoring"]

  loki:
    image: grafana/loki:2.9.6
    restart: unless-stopped
    networks: [monitoring]  # Moved to secure monitoring network
    command: -config.file=/etc/loki/loki.yml
    volumes:
      - loki-data:/loki
      - ./monitoring/loki.yml:/etc/loki/loki.yml:ro
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3100/ready || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    profiles: ["monitoring"]

  promtail:
    image: grafana/promtail:2.9.6
    restart: unless-stopped
    networks: [proxy, database]
    command: -config.file=/etc/promtail/config.yml
    volumes:
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - ./monitoring/promtail.yml:/etc/promtail/config.yml:ro
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:9080/ready || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    profiles: ["monitoring"]

  grafana:
    image: grafana/grafana:11.1.0
    restart: unless-stopped
    networks: [proxy, monitoring]  # Public access + monitoring data
    user: "472"
    environment:
      GF_SERVER_ROOT_URL: https://grafana.${DOMAIN}
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD_FILE: /run/secrets/grafana_admin_password  # Loaded from Docker secret
      GF_AUTH_DISABLE_LOGIN_FORM: "false"
      GF_AUTH_ANONYMOUS_ENABLED: "false"
      # OAuth / OIDC (Authentik) - secret loaded from file
      GF_AUTH_GENERIC_OAUTH_ENABLED: "true"
      GF_AUTH_GENERIC_OAUTH_NAME: "Authentik"
      GF_AUTH_GENERIC_OAUTH_CLIENT_ID: "grafana-oauth"
      GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET: ""  # Loaded from Docker secret
      GF_AUTH_GENERIC_OAUTH_SCOPES: "openid profile email"
      GF_AUTH_GENERIC_OAUTH_AUTH_URL: https://sso.${DOMAIN}/application/o/authorize/
      GF_AUTH_GENERIC_OAUTH_TOKEN_URL: https://sso.${DOMAIN}/application/o/token/
      GF_AUTH_GENERIC_OAUTH_API_URL: https://sso.${DOMAIN}/application/o/userinfo/
      GF_AUTH_GENERIC_OAUTH_ALLOW_SIGN_UP: "true"
      GF_AUTH_GENERIC_OAUTH_AUTO_LOGIN: "false"
      GF_AUTH_GENERIC_OAUTH_ROLE_ATTRIBUTE_PATH: "contains(groups[*], 'Grafana Admins') && 'Admin' || contains(groups[*], 'authentik Admins') && 'Admin' || 'Viewer'"
      GF_AUTH_GENERIC_OAUTH_ROLE_ATTRIBUTE_STRICT: "false"
      GF_AUTH_SIGNOUT_REDIRECT_URL: https://sso.${DOMAIN}/application/o/grafana-oauth/end-session/
    secrets:
      - grafana_oauth_secret
      - grafana_admin_password
    entrypoint: >
      sh -c '
        export GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET=$$(cat /run/secrets/grafana_oauth_secret 2>/dev/null || echo "")
        exec /run.sh
      '
    volumes:
      - grafana-data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./monitoring/dashboards:/var/lib/grafana/dashboards:ro
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

    # Security hardening
    security_opt:
      - no-new-privileges:true
      - apparmor:docker-default
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - DAC_OVERRIDE
      - SETUID
      - SETGID
    tmpfs:
      - /tmp:noexec,nosuid,size=100m
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.05'
          memory: 128M
    profiles: ["monitoring"]

  uptime-kuma:
    image: louislam/uptime-kuma:1
    restart: unless-stopped
    networks: [proxy, database]
    volumes:
      - uptime-kuma-data:/app/data
      - /var/run/docker.sock:/var/run/docker.sock:ro
    labels:
      # Watchtower: Allow auto-updates (non-critical monitoring service)
      - com.centurylinklabs.watchtower.enable=true
      - com.centurylinklabs.watchtower.monitor-only=false
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3001 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M
    profiles: ["monitoring"]

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.49.2
    restart: unless-stopped
    networks: [proxy, database]
    # Removed privileged capabilities for security
    # Only keep essential read-only monitoring access
    read_only: true
    security_opt:
      - no-new-privileges:true
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro

      # Watchtower: Allow auto-updates (non-critical monitoring service)
      - com.centurylinklabs.watchtower.enable=true
      - com.centurylinklabs.watchtower.monitor-only=false
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          memory: 512M
    healthcheck:
      test: ["CMD", "wget", "-q", "--tries=1", "--spider", "http://localhost:8080/metrics"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles: ["monitoring"]

  node-exporter:
    image: prom/node-exporter:v1.8.1
    restart: unless-stopped
    networks: [monitoring]  # Monitoring network for metrics
    pid: host
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:9100/metrics || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    profiles: ["monitoring"]

  redis-exporter:
    image: redis-exporter-with-shell:local
    restart: unless-stopped
    networks: [monitoring, database]  # Access Redis DB + provide metrics
    secrets: [redis_password]
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:9121/metrics || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    profiles: ["monitoring"]

  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:v0.15.0
    restart: unless-stopped
    networks: [monitoring, database]  # Access PostgreSQL + provide metrics
    secrets: [postgres_password]
    volumes:
      - ./scripts/postgres-exporter-wrapper.sh:/usr/local/bin/postgres-exporter-wrapper.sh:ro
    entrypoint: ["/bin/sh"]
    command: ["/usr/local/bin/postgres-exporter-wrapper.sh"]
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:9187/metrics || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    profiles: ["monitoring"]

  mysql-exporter:
    image: prom/mysqld-exporter:v0.15.1
    restart: unless-stopped
    networks: [monitoring, database]  # Access MySQL + provide metrics
    environment:
      DATA_SOURCE_NAME: "root:D1TUn+ryaLiVRBS752rEo4Q4ckt3TB7HroVZUVdQEmM=@tcp(mysql-db:3306)/"
    command: [
      "--collect.info_schema.innodb_metrics",
      "--collect.info_schema.innodb_tablespaces",
      "--collect.info_schema.processlist",
      "--collect.perf_schema.tablelocks",
      "--collect.perf_schema.file_events",
      "--collect.perf_schema.eventswaits",
      "--collect.auto_increment.columns",
      "--collect.binlog_size",
      "--collect.perf_schema.tableiowaits",
      "--collect.perf_schema.indexiowaits"
    ]
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:9104/metrics || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    profiles: ["monitoring"]

  # Certificate Management

  cert-manager:
    build:
      context: ./cert-manager
      dockerfile: Dockerfile
    image: securenexus/cert-manager:v1.0.0
    container_name: cert-manager
    restart: unless-stopped
    networks: [proxy, monitoring]  # Access to Traefik + monitoring metrics
    user: "0:0"  # Run as root to access ACME files
    volumes:
      - ./acme:/acme:ro  # Read Traefik ACME storage
      - ./cert-manager/config.json:/etc/cert-manager/config.json:ro
      - ./certs:/certs:ro  # Read database certificates for distribution
      - /var/run/docker.sock:/var/run/docker.sock:ro  # Required for service restart functionality
    environment:
      - CONFIG_PATH=/etc/cert-manager/config.json

      # Metrics endpoint (VPN-only access)
      # Health endpoint (internal)
      # API endpoint (VPN-only access)
    healthcheck:
      test: ["CMD", "curl", "-f", "-s", "--max-time", "5", "http://localhost:8080/health"]
      interval: 30s
      timeout: 15s
      retries: 5
      start_period: 90s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          memory: 128M
    profiles: ["monitoring"]

  # Controlled Auto-Updates with Testing Policy
  watchtower:
    image: containrrr/watchtower:1.7.0
    container_name: watchtower
    restart: unless-stopped
    networks: [monitoring]  # Monitoring network for metrics
    command:
      - --schedule=0 2 * * SUN        # Weekly updates on Sundays at 2 AM
      - --label-enable                # Only update labeled containers
      - --cleanup                     # Remove old images after update
      - --debug                       # Enable debug logging
    environment:
      # Metrics collection
      WATCHTOWER_HTTP_API_METRICS: true
      WATCHTOWER_HTTP_API_TOKEN: /run/secrets/watchtower_api_token
      # Conservative update policy - explicitly disable restart features
      WATCHTOWER_NO_RESTART: true
      WATCHTOWER_INCLUDE_STOPPED: false
      WATCHTOWER_INCLUDE_RESTARTING: false
      WATCHTOWER_REVIVE_STOPPED: false
    secrets:
      - watchtower_email_password
      - watchtower_api_token
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro

      # Watchtower monitors itself (with caution)
      - com.centurylinklabs.watchtower.enable=false
      # Metrics endpoint (VPN-only access)
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8080/v1/update"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '0.3'
          memory: 128M
        reservations:
          memory: 64M
    profiles: ["monitoring"]

  # Container Management

  # Portainer - Docker container management with SSO
  portainer:
    image: portainer/portainer-ee:2.21.4
    container_name: portainer
    restart: unless-stopped
    networks: [proxy, database]
    command: -H unix:///var/run/docker.sock
    volumes:
      - portainer-data:/data
      - /var/run/docker.sock:/var/run/docker.sock

      # HTTPS router
      # HTTP router (redirect to HTTPS)
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:9000/api/status || exit 1"]
      interval: 30s
      timeout: 15s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          memory: 256M
    profiles: ["portal"]

  # DNS: CoreDNS with etcd + MySQL backends + DNS Updater + ACME TXT Webhook

  # etcd - Key-value store for dynamic DNS records
  etcd:
    image: quay.io/coreos/etcd:v3.5.16
    restart: unless-stopped
    networks: [proxy, database]
    command:
      - /usr/local/bin/etcd
      - --name=etcd0
      - --data-dir=/etcd-data
      - --listen-client-urls=http://0.0.0.0:2379
      - --advertise-client-urls=http://etcd:2379
      - --listen-peer-urls=http://0.0.0.0:2380
      - --initial-advertise-peer-urls=http://etcd:2380
      - --initial-cluster=etcd0=http://etcd:2380
      - --initial-cluster-state=new
      - --initial-cluster-token=etcd-cluster
    volumes:
      - etcd-data:/etcd-data
    healthcheck:
      test: ["CMD", "etcdctl", "endpoint", "health"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles: ["dns"]

  # MySQL database for CoreDNS mysql plugin
  mysql-db:
    image: mysql:8.0
    restart: unless-stopped
    networks: [database]  # Secure database network
    environment:
      MYSQL_DATABASE: coredns
      MYSQL_USER: coredns
      MYSQL_ROOT_PASSWORD_FILE: /run/secrets/mysql_password
      MYSQL_PASSWORD_FILE: /run/secrets/mysql_password
    command: |
      mysqld
        --ssl-cert=/run/secrets/mysql_server_cert
        --ssl-key=/run/secrets/mysql_server_key
        --require-secure-transport=ON
        --ssl-cipher=ECDHE-RSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384
        --tls-version=TLSv1.2,TLSv1.3
    secrets:
      - mysql_password
      - mysql_server_cert
      - mysql_server_key
    volumes:
      - mysql-data:/var/lib/mysql
      - ./dns/mysql-init:/docker-entrypoint-initdb.d:ro
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    profiles: ["dns"]

  # CoreDNS - Modern DNS server with plugin architecture
  coredns:
    image: coredns/coredns:1.11.3
    restart: unless-stopped
    depends_on: [etcd, mysql-db]
    networks: [proxy, database]
    command: -conf /etc/coredns/Corefile
    ports:
      - "0.0.0.0:53:53/tcp"     # Standard DNS port
      - "0.0.0.0:53:53/udp"     # Standard DNS port
      - "853:853/tcp"           # DNS-over-TLS
      - "8181:8080"            # Health check endpoint
      - "9153:9153"            # Prometheus metrics
    environment:
      - MYSQL_PASSWORD_FILE=/run/secrets/mysql_password
    secrets: [mysql_password]
    volumes:
      - ./dns/Corefile:/etc/coredns/Corefile:ro
      - ./dns/certs:/etc/coredns/certs:ro
      - ./dns/dnssec_keys:/etc/coredns/dnssec_keys
      - ./dns/zones:/etc/coredns/zones:ro

      - "coredns.name=dns"
    healthcheck:
      test: ["CMD", "/coredns", "-version"]
      interval: 30s
      timeout: 5s
      retries: 3
    profiles: ["dns"]

  # DNS Updater - Automatically creates DNS records based on container labels
  dns-updater:
    image: alpine:3.20
    restart: unless-stopped
    depends_on: [etcd, docker-proxy]
    networks: [proxy, database]
    environment:
      - DOMAIN=${DOMAIN}
      - ETCD_HOST=etcd
      - ETCD_PORT=2379
      - DNS_TTL=300
      - DOCKER_HOST=tcp://docker-proxy:2375
    volumes:
      - ./scripts/dns-updater.sh:/usr/local/bin/dns-updater:ro
    entrypoint: ["/bin/sh", "-c"]
    command: >
      "apk add --no-cache curl jq bash docker-cli && \
       while true; do /usr/local/bin/dns-updater || true; sleep 30; done"
    healthcheck:
      test: ["CMD", "pgrep", "-f", "dns-updater"]
      interval: 30s
      timeout: 5s
      retries: 3
    profiles: ["dns"]

  # etcd Browser - Web UI for viewing etcd data (optional)
  # Commented out for now - can be enabled if needed
  # etcd-browser:
  #   image: evildecay/etcdkeeper:latest
  #   restart: unless-stopped
  #   networks: [proxy]
  #   environment:
  #     HOST: 0.0.0.0
  #   command: ["etcdkeeper", "-h", "0.0.0.0", "-p", "8080"]
  #   labels:
  #     - "coredns.name=etcd-ui"
  #   profiles: ["dns"]

  # ACME TXT webhook - Updates etcd for DNS-01 challenges
  acme_webhook:
    image: python:3.12-alpine
    restart: unless-stopped
    networks: [proxy, database]
    depends_on: [etcd]
    working_dir: /app
    volumes:
      - ./acme-webhook-etcd:/app:ro
    environment:
      ETCD_HOST: etcd
      ETCD_PORT: 2379
      DNS_TTL: "120"
    entrypoint: ["sh", "-c"]
    command: >
      "pip install --no-cache-dir flask etcd3 && \
       python3 /app/app.py"
    healthcheck:
      test: ["CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:5000/health')"]
      interval: 30s
      timeout: 5s
      retries: 3
    profiles: ["dns"]

  wellknown:
    image: caddy:2-alpine
    restart: unless-stopped
    networks: [proxy, database]
    volumes:
      - ./wellknown/Caddyfile:/etc/caddy/Caddyfile:ro


  # Mail - mailcow
  # NOTE: mailcow requires its own docker-compose.yml in a subdirectory
  # See mail/mailcow/ for mailcow installation
  # Ports exposed: 25, 143, 465, 587, 993 (handled by mailcow's compose)

  brand-static:
    image: nginx:alpine
    container_name: brand-static
    restart: unless-stopped
    volumes:
      - ./branding:/usr/share/nginx/html:ro

    networks:
      - proxy

  # Byrne Accounting Services
  # Marketing website
  byrne-website:
    build:
      context: ./byrne-website
      dockerfile: Dockerfile
    container_name: byrne-website
    restart: unless-stopped
    networks: [proxy, database]


      # Main website - HTTPS router

      # Main website - HTTP router (redirect to HTTPS)

      # Client Portal - HTTPS router

      # Client Portal - HTTP router (redirect to HTTPS)

      # Service definition
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://127.0.0.1/health || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
    profiles: ["byrne"]

  # ERPNext database
  erpnext-db:
    image: mariadb:10.6
    container_name: erpnext-db
    restart: unless-stopped
    networks: [proxy, database]
    command: --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci --skip-character-set-client-handshake --skip-innodb-read-only-compressed
    environment:
      MYSQL_ROOT_PASSWORD_FILE: /run/secrets/erpnext_db_password
      MYSQL_DATABASE: erpnext
      MYSQL_USER: erpnext
      MYSQL_PASSWORD_FILE: /run/secrets/erpnext_db_password
    secrets: [erpnext_db_password]
    volumes:
      - erpnext-db-data:/var/lib/mysql
      - ./frappe_docker/mariadb.cnf:/etc/mysql/conf.d/custom.cnf:ro
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    healthcheck:
      test: ["CMD", "healthcheck.sh", "--connect", "--innodb_initialized"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 60s
    profiles: ["byrne"]

  # ERPNext Redis cache
  erpnext-redis-cache:
    image: redis:7-alpine
    container_name: erpnext-redis-cache
    restart: unless-stopped
    networks: [proxy, database]
    command: ["sh", "-c", "exec redis-server --requirepass \"$$(cat /run/secrets/erpnext_redis_cache_password)\" --maxmemory 512mb --maxmemory-policy allkeys-lru"]
    secrets: [erpnext_redis_cache_password]
    volumes:
      - erpnext-redis-cache-data:/data
    healthcheck:
      test: ["CMD-SHELL", "redis-cli -a \"$$(cat /run/secrets/erpnext_redis_cache_password)\" ping | grep PONG"]
      interval: 10s
      timeout: 5s
      retries: 5
    profiles: ["byrne"]

  # ERPNext Redis queue
  erpnext-redis-queue:
    image: redis:7-alpine
    container_name: erpnext-redis-queue
    restart: unless-stopped
    networks: [proxy, database]
    command: ["sh", "-c", "exec redis-server --requirepass \"$$(cat /run/secrets/erpnext_redis_queue_password)\" --maxmemory 256mb --maxmemory-policy noeviction --save 60 1000"]
    secrets: [erpnext_redis_queue_password]
    volumes:
      - erpnext-redis-queue-data:/data
    healthcheck:
      test: ["CMD-SHELL", "redis-cli -a \"$$(cat /run/secrets/erpnext_redis_queue_password)\" ping | grep PONG"]
      interval: 10s
      timeout: 5s
      retries: 5
    profiles: ["byrne"]

  # ERPNext configurator (creates site on first run)
  erpnext-configurator:
    image: erpnext-posawesome:v15.24.0
    container_name: erpnext-configurator
    restart: "no"
    depends_on:
      erpnext-db:
        condition: service_healthy
      erpnext-redis-cache:
        condition: service_healthy
      erpnext-redis-queue:
        condition: service_healthy
    networks: [proxy, database]
    secrets:
      - erpnext_db_password
      - erpnext_admin_password
    environment:
      DB_ROOT_PASSWORD_FILE: /run/secrets/erpnext_db_password
      ADMIN_PASSWORD_FILE: /run/secrets/erpnext_admin_password
    command:
      - bash
      - -c
      - |
        if [ ! -f '/home/frappe/frappe-bench/sites/erp.byrne-accounts.org/site_config.json' ]; then
          echo 'Initializing new ERPNext site...';
          DB_ROOT_PASSWORD=$(cat /run/secrets/erpnext_db_password)
          ADMIN_PASSWORD=$(cat /run/secrets/erpnext_admin_password)
          bench new-site erp.byrne-accounts.org \
            --mariadb-root-password "$$DB_ROOT_PASSWORD" \
            --admin-password "$$ADMIN_PASSWORD" \
            --db-host erpnext-db \
            --db-type mariadb \
            --install-app erpnext \
            --no-mariadb-socket;
          bench --site erp.byrne-accounts.org set-config developer_mode 0;
          bench --site erp.byrne-accounts.org set-config maintenance_mode 0;
          echo 'Site initialization complete';
        else
          echo 'Site already exists, skipping initialization';
        fi
    volumes:
      - erpnext-sites-data:/home/frappe/frappe-bench/sites
      - erpnext-assets-data:/home/frappe/frappe-bench/sites/assets
      - ./branding:/branding:ro
      - ./erp/branding:/custom-branding:ro
    profiles: ["byrne"]

  # ERPNext backend (Frappe/ERPNext application server)
  erpnext-backend:
    image: erpnext-posawesome:v15.24.0
    container_name: erpnext-backend
    restart: unless-stopped
    depends_on:
      erpnext-db:
        condition: service_healthy
      erpnext-redis-cache:
        condition: service_healthy
      erpnext-redis-queue:
        condition: service_healthy
      erpnext-configurator:
        condition: service_completed_successfully
    networks: [proxy, database]
    environment:
      SITE_NAME: erp.byrne-accounts.org
    command: ["bash", "-c", "cd /home/frappe/frappe-bench && bench serve --port 8000"]
    volumes:
      - erpnext-sites-data:/home/frappe/frappe-bench/sites
      - erpnext-assets-data:/home/frappe/frappe-bench/sites/assets
      - ./branding:/branding:ro
      - ./erp/branding:/custom-branding:ro

      # ERP access - temporarily public for initial setup (add SSO after Authentik outpost configured)
      # HTTP redirect to HTTPS
      # POS access subdomain - temporarily public for initial setup
    deploy:
      resources:
        limits:
          cpus: '1.5'
          memory: 1G
        reservations:
          cpus: '0.3'
          memory: 512M
    healthcheck:
      test: ["CMD-SHELL", "curl -f -H 'Host: erp.byrne-accounts.org' http://localhost:8000/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    profiles: ["byrne"]

  # ERPNext SocketIO (real-time updates)
  erpnext-socketio:
    image: erpnext-posawesome:v15.24.0
    container_name: erpnext-socketio
    restart: unless-stopped
    depends_on:
      - erpnext-backend
    networks: [proxy, database]
    environment:
      SITE_NAME: erp.byrne-accounts.org
      REDIS_CACHE: redis://erpnext-redis-cache:6379
      REDIS_QUEUE: redis://erpnext-redis-queue:6379
      REDIS_SOCKETIO: redis://erpnext-redis-cache:6379
    command: ["bash", "-c", "cd /home/frappe/frappe-bench && node apps/frappe/socketio.js"]
    volumes:
      - erpnext-sites-data:/home/frappe/frappe-bench/sites
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 128M
    healthcheck:
      test: ["CMD-SHELL", "test -f /proc/1/comm && exit 0 || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s
    profiles: ["byrne"]

  # ERPNext worker (background jobs)
  erpnext-worker:
    image: erpnext-posawesome:v15.24.0
    container_name: erpnext-worker
    restart: unless-stopped
    depends_on:
      - erpnext-backend
    networks: [proxy, database]
    environment:
      SITE_NAME: erp.byrne-accounts.org
      REDIS_CACHE: redis://erpnext-redis-cache:6379
      REDIS_QUEUE: redis://erpnext-redis-queue:6379
    command: ["bash", "-c", "cd /home/frappe/frappe-bench && bench --site erp.byrne-accounts.org worker --queue default,short,long"]
    volumes:
      - erpnext-sites-data:/home/frappe/frappe-bench/sites
      - erpnext-assets-data:/home/frappe/frappe-bench/sites/assets
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.2'
          memory: 256M
    healthcheck:
      test: ["CMD-SHELL", "test -f /proc/1/comm && exit 0 || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    profiles: ["byrne"]

  # ERPNext scheduler (scheduled tasks)
  erpnext-scheduler:
    image: erpnext-posawesome:v15.24.0
    container_name: erpnext-scheduler
    restart: unless-stopped
    depends_on:
      - erpnext-backend
    networks: [proxy, database]
    environment:
      SITE_NAME: erp.byrne-accounts.org
    command: ["bash", "-c", "cd /home/frappe/frappe-bench && bench --site erp.byrne-accounts.org schedule"]
    volumes:
      - erpnext-sites-data:/home/frappe/frappe-bench/sites
      - erpnext-assets-data:/home/frappe/frappe-bench/sites/assets
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 128M
    healthcheck:
      test: ["CMD-SHELL", "test -f /proc/1/comm && exit 0 || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    profiles: ["byrne"]


  # ====================================================================================
  # DICKSON SUPPLIES - CLIENT ERP INSTANCE
  # ====================================================================================
  
  # Dickson MariaDB database
  dickson-db:
    image: mariadb:10.6
    container_name: dickson-db
    restart: unless-stopped
    networks: [proxy, database]
    environment:
      MARIADB_ROOT_PASSWORD_FILE: /run/secrets/dickson_db_password
      MARIADB_CHARACTER_SET_SERVER: utf8mb4
      MARIADB_COLLATION_SERVER: utf8mb4_unicode_ci
    secrets: [dickson_db_password]
    volumes:
      - dickson-db-data:/var/lib/mysql
    healthcheck:
      test: ["CMD", "healthcheck.sh", "--connect", "--innodb_initialized"]
      interval: 10s
      timeout: 5s
      retries: 10
    profiles: ["dickson"]

  # Dickson Redis cache
  dickson-redis-cache:
    image: redis:7-alpine
    container_name: dickson-redis-cache
    restart: unless-stopped
    networks: [proxy, database]
    command: ["sh", "-c", "exec redis-server --requirepass \"$$(cat /run/secrets/dickson_redis_cache_password)\" --maxmemory 512mb --maxmemory-policy allkeys-lru"]
    secrets: [dickson_redis_cache_password]
    volumes:
      - dickson-redis-cache-data:/data
    healthcheck:
      test: ["CMD-SHELL", "redis-cli -a \"$$(cat /run/secrets/dickson_redis_cache_password)\" ping | grep PONG"]
      interval: 10s
      timeout: 5s
      retries: 5
    profiles: ["dickson"]

  # Dickson Redis queue
  dickson-redis-queue:
    image: redis:7-alpine
    container_name: dickson-redis-queue
    restart: unless-stopped
    networks: [proxy, database]
    command: ["sh", "-c", "exec redis-server --requirepass \"$$(cat /run/secrets/dickson_redis_queue_password)\" --maxmemory 256mb --maxmemory-policy noeviction --save 60 1000"]
    secrets: [dickson_redis_queue_password]
    volumes:
      - dickson-redis-queue-data:/data
    healthcheck:
      test: ["CMD-SHELL", "redis-cli -a \"$$(cat /run/secrets/dickson_redis_queue_password)\" ping | grep PONG"]
      interval: 10s
      timeout: 5s
      retries: 5
    profiles: ["dickson"]

  # Dickson configurator (creates site on first run)
  dickson-configurator:
    image: erpnext-posawesome:v15.24.0
    container_name: dickson-configurator
    restart: "no"
    depends_on:
      dickson-db:
        condition: service_healthy
      dickson-redis-cache:
        condition: service_healthy
      dickson-redis-queue:
        condition: service_healthy
    networks: [proxy, database]
    secrets: [dickson_db_password, dickson_admin_password]
    command:
      - bash
      - -c
      - |
        DB_ROOT_PASSWORD=$$(cat /run/secrets/dickson_db_password)
        ADMIN_PASSWORD=$$(cat /run/secrets/dickson_admin_password)
        if [ ! -f '/home/frappe/frappe-bench/sites/erp.dickson-supplies.com/site_config.json' ]; then
          echo 'Initializing new Dickson ERPNext site...';
          bench new-site erp.dickson-supplies.com \
            --mariadb-root-password "$$DB_ROOT_PASSWORD" \
            --admin-password "$$ADMIN_PASSWORD" \
            --db-host dickson-db \
            --db-type mariadb \
            --install-app erpnext \
            --no-mariadb-socket;
          bench --site erp.dickson-supplies.com set-config developer_mode 0;
          bench --site erp.dickson-supplies.com set-config maintenance_mode 0;
          echo 'Dickson site initialization complete';
        else
          echo 'Dickson site already exists, skipping initialization';
        fi
    volumes:
      - dickson-sites-data:/home/frappe/frappe-bench/sites
      - dickson-assets-data:/home/frappe/frappe-bench/sites/assets
      - ./erp/dickson-branding:/custom-branding:ro
    profiles: ["dickson"]

  # Dickson backend (Frappe/ERPNext application server)
  dickson-backend:
    image: erpnext-posawesome:v15.24.0
    container_name: dickson-backend
    restart: unless-stopped
    depends_on:
      dickson-db:
        condition: service_healthy
      dickson-redis-cache:
        condition: service_healthy
      dickson-redis-queue:
        condition: service_healthy
      dickson-configurator:
        condition: service_completed_successfully
    networks: [proxy, database]
    environment:
      SITE_NAME: erp.dickson-supplies.com
    secrets: [dickson_redis_cache_password, dickson_redis_queue_password]
    command:
      - bash
      - -c
      - |
        REDIS_CACHE_PASSWORD=$$(cat /run/secrets/dickson_redis_cache_password)
        REDIS_QUEUE_PASSWORD=$$(cat /run/secrets/dickson_redis_queue_password)
        export REDIS_CACHE="redis://:$$REDIS_CACHE_PASSWORD@dickson-redis-cache:6379"
        export REDIS_QUEUE="redis://:$$REDIS_QUEUE_PASSWORD@dickson-redis-queue:6379"
        cd /home/frappe/frappe-bench && bench serve --port 8000
    volumes:
      - dickson-sites-data:/home/frappe/frappe-bench/sites
      - dickson-assets-data:/home/frappe/frappe-bench/sites/assets
      - ./erp/dickson-branding:/custom-branding:ro

      # ERP domain
      # POS domain
    healthcheck:
      test: ["CMD-SHELL", "curl -f -H 'Host: erp.dickson-supplies.com' http://localhost:8000/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    profiles: ["dickson"]

  # Dickson SocketIO (real-time updates)
  dickson-socketio:
    image: erpnext-posawesome:v15.24.0
    container_name: dickson-socketio
    restart: unless-stopped
    depends_on:
      - dickson-backend
    networks: [proxy, database]
    environment:
      SITE_NAME: erp.dickson-supplies.com
      REDIS_CACHE: redis://dickson-redis-cache:6379
      REDIS_QUEUE: redis://dickson-redis-queue:6379
      REDIS_SOCKETIO: redis://dickson-redis-cache:6379
    command: ["bash", "-c", "cd /home/frappe/frappe-bench && node apps/frappe/socketio.js"]
    volumes:
      - dickson-sites-data:/home/frappe/frappe-bench/sites

    healthcheck:
      test: ["CMD-SHELL", "test -f /proc/1/comm && exit 0 || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s
    profiles: ["dickson"]

  # Dickson worker (background jobs)
  dickson-worker:
    image: erpnext-posawesome:v15.24.0
    container_name: dickson-worker
    restart: unless-stopped
    depends_on:
      - dickson-backend
    networks: [proxy, database]
    environment:
      SITE_NAME: erp.dickson-supplies.com
      REDIS_CACHE: redis://dickson-redis-cache:6379
      REDIS_QUEUE: redis://dickson-redis-queue:6379
    command: ["bash", "-c", "cd /home/frappe/frappe-bench && bench --site erp.dickson-supplies.com worker --queue default,short,long"]
    volumes:
      - dickson-sites-data:/home/frappe/frappe-bench/sites
      - dickson-assets-data:/home/frappe/frappe-bench/sites/assets
    healthcheck:
      test: ["CMD-SHELL", "test -f /proc/1/comm && exit 0 || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    profiles: ["dickson"]

  # Dickson scheduler (scheduled tasks)
  dickson-scheduler:
    image: erpnext-posawesome:v15.24.0
    container_name: dickson-scheduler
    restart: unless-stopped
    depends_on:
      - dickson-backend
    networks: [proxy, database]
    environment:
      SITE_NAME: erp.dickson-supplies.com
    command: ["bash", "-c", "cd /home/frappe/frappe-bench && bench --site erp.dickson-supplies.com schedule"]
    volumes:
      - dickson-sites-data:/home/frappe/frappe-bench/sites
      - dickson-assets-data:/home/frappe/frappe-bench/sites/assets
    healthcheck:
      test: ["CMD-SHELL", "test -f /proc/1/comm && exit 0 || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    profiles: ["dickson"]


  # ===== CLOUD STORAGE & SYNC SERVICES =====

  # Nextcloud Database
  nextcloud-db:
    image: postgres:15-alpine
    restart: unless-stopped
    networks: [proxy, database]
    environment:
      POSTGRES_USER: nextcloud
      POSTGRES_DB: nextcloud
      POSTGRES_PASSWORD_FILE: /run/secrets/nextcloud_db_password
      PGDATA: /var/lib/postgresql/data
    volumes:
      - nextcloud-db-data:/var/lib/postgresql/data
    secrets: [nextcloud_db_password]
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.2'
          memory: 256M
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U nextcloud -d nextcloud"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles: ["cloud"]

  # Nextcloud Application
  nextcloud:
    image: nextcloud:29-apache
    restart: unless-stopped
    depends_on:
      - nextcloud-db
    networks: [proxy, database]
    environment:
      POSTGRES_HOST: nextcloud-db
      POSTGRES_DB: nextcloud
      POSTGRES_USER: nextcloud
      POSTGRES_PASSWORD_FILE: /run/secrets/nextcloud_db_password
      NEXTCLOUD_ADMIN_USER: admin
      NEXTCLOUD_ADMIN_PASSWORD_FILE: /run/secrets/nextcloud_admin_password
      NEXTCLOUD_TRUSTED_DOMAINS: "nextcloud.${DOMAIN}"
      OVERWRITEPROTOCOL: https
      OVERWRITEHOST: "nextcloud.${DOMAIN}"
      # REDIS_HOST: redis_cache  # Disabled - causes session issues
      # REDIS_HOST_PASSWORD_FILE: /run/secrets/redis_password
    volumes:
      - nextcloud-data:/var/www/html
    secrets: [nextcloud_db_password, nextcloud_admin_password, redis_password]
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M

      # Main Nextcloud interface
      # Nextcloud headers middleware for WebDAV and CardDAV
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:80/status.php || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    profiles: ["cloud"]

  # ===== NOTESNOOK SYNC SERVER INFRASTRUCTURE =====

  # ===== NOTESNOOK SELF-HOSTED SYNC SERVER =====

  # Environment validation container
  notesnook-validate:
    image: alpine:3.20
    entrypoint: /bin/sh
    secrets:
      - notesnook_instance_name
      - notesnook_api_secret
      - notesnook_disable_signups
    command:
      - -c
      - |
        echo "Validating Notesnook configuration..."
        if [ ! -s /run/secrets/notesnook_instance_name ]; then
          echo "Error: Instance name not configured"
          exit 1
        fi
        if [ ! -s /run/secrets/notesnook_api_secret ]; then
          echo "Error: API secret not configured"
          exit 1
        fi
        echo "All required environment variables are set."
    restart: "no"
    profiles: ["cloud"]

  # MongoDB database with replica set
  notesnook-db:
    image: mongo:7.0.12
    hostname: notesnook-db
    restart: unless-stopped
    depends_on:
      notesnook-validate:
        condition: service_completed_successfully
    networks: [database]
    environment: {}
    volumes:
      - notesnook-db-data:/data/db
      - notesnook-db-data:/data/configdb
    command: --replSet rs0 --bind_ip_all --noauth
    healthcheck:
      test: echo 'db.runCommand("ping").ok' | mongosh mongodb://localhost:27017 --quiet
      interval: 40s
      timeout: 30s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 256M
    profiles: ["cloud"]

  # MongoDB replica set initialization
  notesnook-initiate-rs:
    image: mongo:7.0.12
    depends_on:
      - notesnook-db
    networks: [database]
    entrypoint: /bin/sh
    command:
      - -c
      - |
        mongosh mongodb://notesnook-db:27017 <<EOF
          rs.initiate();
          rs.status();
        EOF
    restart: "no"
    profiles: ["cloud"]

  # MinIO S3 storage for attachments
  notesnook-s3:
    image: minio/minio:RELEASE.2024-07-29T22-14-52Z
    restart: unless-stopped
    depends_on:
      notesnook-validate:
        condition: service_completed_successfully
    networks: [backend]
    environment:
      MINIO_ROOT_USER_FILE: /run/secrets/notesnook_s3_access_key
      MINIO_ROOT_PASSWORD_FILE: /run/secrets/notesnook_s3_password
      MINIO_BROWSER: "on"
    secrets: [notesnook_s3_access_key, notesnook_s3_password]
    volumes:
      - notesnook-s3-data:/data/s3
    command: server /data/s3 --console-address :9090
    healthcheck:
      test: timeout 5s bash -c ':> /dev/tcp/127.0.0.1/9000' || exit 1
      interval: 40s
      timeout: 30s
      retries: 3
      start_period: 60s
    profiles: ["cloud"]

  # S3 bucket setup
  notesnook-setup-s3:
    image: minio/mc:RELEASE.2024-07-26T13-08-44Z
    depends_on:
      - notesnook-s3
    networks: [backend]
    secrets: [notesnook_s3_access_key, notesnook_s3_password]
    entrypoint: /bin/bash
    command:
      - -c
      - |
        ACCESS_KEY=$$(cat /run/secrets/notesnook_s3_access_key)
        SECRET_KEY=$$(cat /run/secrets/notesnook_s3_password)
        until mc alias set minio http://notesnook-s3:9000/ $$ACCESS_KEY $$SECRET_KEY; do
          sleep 1;
        done;
        mc mb minio/attachments -p
    restart: "no"
    profiles: ["cloud"]

  # Identity/Auth server
  notesnook-identity:
    image: notesnook-identity:source
    restart: unless-stopped
    depends_on:
      - notesnook-db
      - notesnook-initiate-rs
    networks: [proxy, backend, database]
    environment:
      # Official Repository Format
      AUTH_SERVER_PUBLIC_URL: https://identity.${DOMAIN}
      NOTESNOOK_APP_PUBLIC_URL: https://notes.${DOMAIN}
      MONOGRAPH_PUBLIC_URL: https://mono.${DOMAIN}
      ATTACHMENTS_SERVER_PUBLIC_URL: https://files.${DOMAIN}
      # Server Discovery (Internal)
      NOTESNOOK_SERVER_PORT: 5264
      NOTESNOOK_SERVER_HOST: notesnook-server
      IDENTITY_SERVER_PORT: 8264
      IDENTITY_SERVER_HOST: notesnook-identity
      IDENTITY_SERVER_URL: https://identity.${DOMAIN}
      SSE_SERVER_PORT: 7264
      SSE_SERVER_HOST: notesnook-sse
      # Configuration
      SELF_HOSTED: 1
      # Database connection
      MONGODB_CONNECTION_STRING: mongodb://notesnook-db:27017/identity?replSet=rs0
      MONGODB_DATABASE_NAME: identity
      # MongoDB Configuration for IdentityServer4.MongoDB
      MongoDBConfiguration__ConnectionString: mongodb://notesnook-db:27017/identity?replSet=rs0
      MongoDBConfiguration__Database: identity
      # Instance configuration
      INSTANCE_NAME: My SecureNexus Notesnook
      NOTESNOOK_API_SECRET_FILE: /run/secrets/notesnook_api_secret
      # Direct API secret (application expects this)
      NOTESNOOK_API_SECRET: "xuz6B/GyIFEjjJPmIn7087AjQY266WFuHi5S+9Ci/og="
      DISABLE_SIGNUPS: false
      # SSO Configuration with Authentik
      OIDC_AUTHORITY: https://auth.${DOMAIN}/application/o/notesnook/
      OIDC_CLIENT_ID: notesnook-sso
      OIDC_CLIENT_SECRET_FILE: /run/secrets/notesnook_oauth_secret
      OIDC_RESPONSE_TYPE: code
      OIDC_SCOPE: "openid profile email"
      ENABLE_SSO: true
    secrets: [notesnook_api_secret, notesnook_oauth_secret]
    volumes:
      - ./config/notesnook/appsettings.json:/app/appsettings.json:ro

    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8264/health || exit 1"]
      interval: 30s
      timeout: 15s
      retries: 5
      start_period: 90s
    profiles: ["cloud"]

  # Main Notesnook sync server
  notesnook-server:
    image: notesnook-server:source
    restart: unless-stopped
    depends_on:
      - notesnook-s3
      - notesnook-setup-s3
      - notesnook-identity
    networks: [proxy, backend, database]
    secrets:
      - notesnook_s3_access_key
      - notesnook_s3_password
      - notesnook_connection_string
      - notesnook_api_secret
    environment:
      # Official Repository Format
      AUTH_SERVER_PUBLIC_URL: https://identity.${DOMAIN}
      NOTESNOOK_APP_PUBLIC_URL: https://notes.${DOMAIN}
      MONOGRAPH_PUBLIC_URL: https://mono.${DOMAIN}
      ATTACHMENTS_SERVER_PUBLIC_URL: https://files.${DOMAIN}
      # ASP.NET Core binding configuration
      ASPNETCORE_URLS: http://0.0.0.0:5264
      ASPNETCORE_HTTP_PORTS: 5264
      # Server Discovery (Internal)
      NOTESNOOK_SERVER_PORT: 5264
      NOTESNOOK_SERVER_HOST: notesnook-server
      IDENTITY_SERVER_PORT: 8264
      IDENTITY_SERVER_HOST: notesnook-identity
      IDENTITY_SERVER_URL: https://identity.${DOMAIN}
      SSE_SERVER_PORT: 7264
      SSE_SERVER_HOST: notesnook-sse
      SELF_HOSTED: 1
      # API secret configuration
      NOTESNOOK_API_SECRET_FILE: /run/secrets/notesnook_api_secret
      # Direct API secret (application expects this)
      NOTESNOOK_API_SECRET: "xuz6B/GyIFEjjJPmIn7087AjQY266WFuHi5S+9Ci/og="
      # Database connection
      MONGODB_CONNECTION_STRING: mongodb://notesnook-db:27017/notesnook?replSet=rs0
      MONGODB_CONNECTION_STRING_FILE: /run/secrets/notesnook_connection_string
      MONGODB_DATABASE_NAME: notesnook
      # S3 configuration
      S3_INTERNAL_SERVICE_URL: "http://notesnook-s3:9000/"
      S3_INTERNAL_BUCKET_NAME: "attachments"
      S3_ACCESS_KEY_ID_FILE: /run/secrets/notesnook_s3_access_key
      S3_SECRET_ACCESS_KEY_FILE: /run/secrets/notesnook_s3_password
      # Direct S3 credentials (application expects these)
      S3_ACCESS_KEY_ID: "OjFkamE3pXGWBxuWugbV6bt7PaQ+QGUpMznX5bLncHk="
      S3_SECRET_ACCESS_KEY: "Wq1PU5MKFa1Ql9dZYKkh2VMXREYjIf9LGXkVdGG6KZo="
      S3_ACCESS_KEY: "OjFkamE3pXGWBxuWugbV6bt7PaQ+QGUpMznX5bLncHk="
      S3_SECRET_KEY: "Wq1PU5MKFa1Ql9dZYKkh2VMXREYjIf9LGXkVdGG6KZo="
      S3_SERVICE_URL: "https://files.${DOMAIN}/"
      S3_REGION: ""
      S3_BUCKET_NAME: "attachments"
      S3_FORCE_PATH_STYLE: "true"
      # AWS SDK specific configuration for MinIO
      AWS_ACCESS_KEY_ID: "OjFkamE3pXGWBxuWugbV6bt7PaQ+QGUpMznX5bLncHk="
      AWS_SECRET_ACCESS_KEY: "Wq1PU5MKFa1Ql9dZYKkh2VMXREYjIf9LGXkVdGG6KZo="
      AWS_REGION: ""
      AWS_S3_FORCE_PATH_STYLE: "true"
      AWS_S3_USE_PATH_STYLE_ACCESS: "true"

      # Main API endpoint with WebSocket support
    # healthcheck:
    #   test: ["CMD-SHELL", "curl -f http://localhost:5264/health || exit 1"]
    #   interval: 30s
    #   timeout: 15s
    #   retries: 5
    #   start_period: 90s
    profiles: ["cloud"]

  # Server-Sent Events server
  notesnook-sse:
    image: streetwriters/sse:latest
    restart: unless-stopped
    depends_on:
      - notesnook-identity
      - notesnook-server
    networks: [proxy, backend, database]
    secrets: [notesnook_connection_string]
    environment:
      # Official Repository Format
      AUTH_SERVER_PUBLIC_URL: https://identity.${DOMAIN}
      NOTESNOOK_APP_PUBLIC_URL: https://notes.${DOMAIN}
      MONOGRAPH_PUBLIC_URL: https://mono.${DOMAIN}
      ATTACHMENTS_SERVER_PUBLIC_URL: https://files.${DOMAIN}
      # Server Discovery (Internal)
      NOTESNOOK_SERVER_PORT: 5264
      NOTESNOOK_SERVER_HOST: notesnook-server
      IDENTITY_SERVER_PORT: 8264
      IDENTITY_SERVER_HOST: notesnook-identity
      IDENTITY_SERVER_URL: https://identity.${DOMAIN}
      SSE_SERVER_PORT: 7264
      SSE_SERVER_HOST: notesnook-sse
      SELF_HOSTED: 1
      # Database connection
      MONGODB_CONNECTION_STRING_FILE: /run/secrets/notesnook_connection_string

      # SSE endpoint with WebSocket support
    healthcheck:
      test: ["CMD", "wget", "--tries=1", "-nv", "-q", "http://localhost:7264/health", "-O-"]
      interval: 40s
      timeout: 30s
      retries: 3
      start_period: 60s
    profiles: ["cloud"]

  # MonoGraph server for public notes
  notesnook-monograph:
    image: streetwriters/monograph:v1.0.0
    restart: unless-stopped
    depends_on:
      - notesnook-server
    networks: [proxy, backend]
    environment:
      # Official Repository Format
      AUTH_SERVER_PUBLIC_URL: https://identity.${DOMAIN}
      NOTESNOOK_APP_PUBLIC_URL: https://notes.${DOMAIN}
      MONOGRAPH_PUBLIC_URL: https://mono.${DOMAIN}
      ATTACHMENTS_SERVER_PUBLIC_URL: https://files.${DOMAIN}
      # Server Discovery (Internal)
      NOTESNOOK_SERVER_PORT: 5264
      NOTESNOOK_SERVER_HOST: notesnook-server
      IDENTITY_SERVER_PORT: 8264
      IDENTITY_SERVER_HOST: notesnook-identity
      IDENTITY_SERVER_URL: https://identity.${DOMAIN}
      SSE_SERVER_PORT: 7264
      SSE_SERVER_HOST: notesnook-sse
      SELF_HOSTED: 1
      # MonoGraph configuration
      API_HOST: http://notesnook-server:5264/
      PUBLIC_URL: https://mono.${DOMAIN}/

      # MonoGraph endpoint with caching
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/health || exit 1"]
      interval: 30s
      timeout: 15s
      retries: 3
      start_period: 60s
    profiles: ["cloud"]

  # ===== KANIDM IDENTITY MANAGEMENT (TEST EVALUATION) =====

  # Kanidm - Modern Rust-based Identity Management Server
  # Test deployment alongside Authentik for evaluation
  kanidm:
    image: kanidm/server:1.1.0-rc.15
    restart: unless-stopped
    command: ["/sbin/kanidmd", "server", "-c", "/data/server.toml"]
    networks: [proxy, database]
    volumes:
      - kanidm-data:/data
      - ./kanidm/config/server.toml:/data/server.toml:ro
      - ./kanidm/certs/fullchain.pem:/data/fullchain.pem:ro
      - ./kanidm/certs/privkey.pem:/data/privkey.pem:ro
    environment:
      RUST_LOG: info

      # Web UI
    healthcheck:
      test: ["CMD-SHELL", "curl -k -f https://localhost:8443/status || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    profiles: ["test"]

  # Caddy reverse proxy (replaces Traefik)
  caddy:
    build:
      context: ./config/caddy
      dockerfile: Dockerfile.enhanced
    image: caddy-enhanced:v2.9.1
    container_name: caddy-main
    restart: unless-stopped
    networks: [proxy, database]
    ports:
      - "80:80"
      - "443:443"
    environment:
      DOMAIN: ${DOMAIN}
      EMAIL: ${EMAIL}
      CROWDSEC_BOUNCER_API_KEY: "" # Will be loaded from Docker secret by entrypoint
    volumes:
      - ./config/caddy/Caddyfile:/etc/caddy/Caddyfile:ro
      - ./config/caddy/snippets:/etc/caddy/snippets:ro
      - caddy-data:/data
      - caddy-config:/config
    secrets:
      - crowdsec_bouncer_api_key
    healthcheck:
      test: ["CMD", "caddy", "validate", "--config", "/etc/caddy/Caddyfile"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    profiles: ["core"]

  # ===== ADDITIONAL CLOUD SERVICES =====

  # Vikunja - Modern task management (reusing existing PostgreSQL + SSO)
  vikunja:
    image: vikunja/vikunja:latest
    restart: unless-stopped
    user: "1000:1000"
    depends_on:
      authentik_db:
        condition: service_healthy
    networks: [proxy, database]
    environment:
      # Database configuration
      VIKUNJA_DATABASE_TYPE: postgres
      VIKUNJA_DATABASE_HOST: authentik_db
      VIKUNJA_DATABASE_DATABASE: vikunja
      VIKUNJA_DATABASE_USER: authentik
      VIKUNJA_DATABASE_PASSWORD: "6efgFqOeIg+Vx5/awU7uKnF3wL/s1/mg0rZpNH3BdiQ="
      # Service configuration
      VIKUNJA_SERVICE_JWTSECRET_FILE: /run/secrets/vikunja_jwt_secret
      VIKUNJA_SERVICE_FRONTENDURL: https://vikunja.securenexus.net
      VIKUNJA_SERVICE_PUBLICURL: https://vikunja.securenexus.net
      # Disable CORS completely with multiple formats
      VIKUNJA_CORS_ENABLE: "false"
      VIKUNJA_SERVICE_CORS_ENABLE: "false"
      VIKUNJA_CORS_ORIGINS: ""
      # Additional service configuration
      VIKUNJA_SERVICE_ENABLEREGISTRATION: "false"
      VIKUNJA_SERVICE_ENABLETASKATTACHMENTS: "true"
      # Cache configuration to avoid permission issues
      VIKUNJA_CACHE_ENABLED: "false"
    volumes:
      - vikunja-data:/app/vikunja/files
    secrets:
      - postgres_password
      - vikunja_jwt_secret
    labels:
      caddy: vikunja.securenexus.net
      caddy.reverse_proxy: "{{upstreams 3456}}"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3456/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles: ["cloud"]

  # Calibre - E-book management
  calibre:
    image: lscr.io/linuxserver/calibre:latest
    restart: unless-stopped
    networks: [proxy, database]
    environment:
      PUID: 1000
      PGID: 1000
      TZ: Etc/UTC
      PASSWORD_FILE: /run/secrets/calibre_password
      CLI_ARGS: --auth-mode=server
    volumes:
      - calibre-data:/config
    secrets: [calibre_password]
    labels:
      caddy: calibre.securenexus.net
      caddy.reverse_proxy: "{{upstreams 8080}}"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles: ["cloud"]

  # HedgeDoc - Collaborative markdown editor (reusing existing PostgreSQL)
  hedgedoc:
    image: quay.io/hedgedoc/hedgedoc:latest
    restart: unless-stopped
    depends_on:
      authentik_db:
        condition: service_healthy
    networks: [proxy, database]
    environment:
      CMD_DB_HOST: authentik_db
      CMD_DB_PORT: 5432
      CMD_DB_DIALECT: postgres
      CMD_DB_USERNAME: authentik
      CMD_DB_PASSWORD: "6efgFqOeIg+Vx5/awU7uKnF3wL/s1/mg0rZpNH3BdiQ="
      CMD_DB_DATABASE: hedgedoc
      CMD_DOMAIN: hedgedoc.securenexus.net
      CMD_URL_ADDPORT: "false"
      CMD_PROTOCOL_USESSL: "true"
      CMD_SESSION_SECRET: "RGZ1LE9ilzU3dDf0fXc38GIMMEXtABdTj+cbbxqn05U="
    volumes:
      - hedgedoc-data:/hedgedoc/public/uploads
    secrets:
      - postgres_password
    labels:
      caddy: hedgedoc.securenexus.net
      caddy.reverse_proxy: "{{upstreams 3000}}"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles: ["cloud"]

  # Forgejo - Git forge (reusing existing PostgreSQL + SSO)
  forgejo:
    image: codeberg.org/forgejo/forgejo:9
    restart: unless-stopped
    depends_on:
      authentik_db:
        condition: service_healthy
    networks: [proxy, database]
    environment:
      USER_UID: 1000
      USER_GID: 1000
      FORGEJO__database__DB_TYPE: postgres
      FORGEJO__database__HOST: authentik_db:5432
      FORGEJO__database__NAME: forgejo
      FORGEJO__database__USER: authentik
      FORGEJO__database__PASSWD__FILE: /run/secrets/postgres_password
      FORGEJO__security__SECRET_KEY__FILE: /run/secrets/forgejo_secret_key
      FORGEJO__server__DOMAIN: git.securenexus.net
      FORGEJO__server__ROOT_URL: https://git.securenexus.net/
      # OAuth2/OIDC Configuration for Authentik
      FORGEJO__oauth2__ENABLED: "true"
    volumes:
      - forgejo-data:/data
      - /etc/timezone:/etc/timezone:ro
      - /etc/localtime:/etc/localtime:ro
    secrets:
      - postgres_password
      - forgejo_secret_key
    labels:
      caddy: git.securenexus.net
      caddy.reverse_proxy: "{{upstreams 3000}}"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles: ["cloud"]

  # Bitwarden/Vaultwarden - Password manager
  bitwarden:
    image: vaultwarden/server:latest
    restart: unless-stopped
    networks: [proxy, database]
    environment:
      WEBSOCKET_ENABLED: "true"
      ADMIN_TOKEN_FILE: /run/secrets/bitwarden_admin_token
      DOMAIN: https://passwords.securenexus.net
    volumes:
      - bitwarden-data:/data
    secrets:
      - bitwarden_admin_token
    labels:
      caddy: passwords.securenexus.net
      caddy.reverse_proxy: "{{upstreams 80}}"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/alive"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles: ["cloud"]

  # Firefly III - Personal finance manager (reusing existing PostgreSQL)
  firefly:
    image: fireflyiii/core:latest
    restart: unless-stopped
    depends_on:
      authentik_db:
        condition: service_healthy
    networks: [proxy, database]
    environment:
      APP_KEY_FILE: /run/secrets/firefly_app_key
      DB_HOST: authentik_db
      DB_PORT: 5432
      DB_CONNECTION: pgsql
      DB_DATABASE: firefly
      DB_USERNAME: authentik
      DB_PASSWORD_FILE: /run/secrets/postgres_password
      APP_URL: https://finance.securenexus.net
      TRUSTED_PROXIES: "**"
    volumes:
      - firefly-data:/var/www/html/storage/upload
    secrets:
      - postgres_password
      - firefly_app_key
    labels:
      caddy: finance.securenexus.net
      caddy.reverse_proxy: "{{upstreams 8080}}"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles: ["cloud"]

  # Evil Rabbit Art - Gothic Artist Ecommerce Platform
  evilrabbit-db:
    image: mysql:8.0
    container_name: evilrabbit-db
    restart: unless-stopped
    networks: [proxy, database]
    environment:
      MYSQL_ROOT_PASSWORD_FILE: /run/secrets/evilrabbit_db_root_password
      MYSQL_DATABASE: evilrabbit_wp
      MYSQL_USER: wordpress
      MYSQL_PASSWORD_FILE: /run/secrets/evilrabbit_db_password
    volumes:
      - evilrabbit-mysql-data:/var/lib/mysql
    secrets:
      - evilrabbit_db_root_password
      - evilrabbit_db_password
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    profiles: ["evilrabbit"]

  evilrabbit-wordpress:
    image: wordpress:6-php8.2-apache
    container_name: evilrabbit-wordpress
    restart: unless-stopped
    depends_on:
      evilrabbit-db:
        condition: service_healthy
    networks: [proxy, database]
    environment:
      WORDPRESS_DB_HOST: evilrabbit-db
      WORDPRESS_DB_NAME: evilrabbit_wp
      WORDPRESS_DB_USER: wordpress
      WORDPRESS_DB_PASSWORD_FILE: /run/secrets/evilrabbit_db_password
      WORDPRESS_CONFIG_EXTRA: |
        // Multi-tenant client configuration
        define('AUTOMATIC_UPDATER_DISABLED', true);
        define('DISALLOW_FILE_EDIT', true);
        define('DISALLOW_FILE_MODS', true);
        // Evil Rabbit Art branding
        define('WP_DEFAULT_THEME', 'evilrabbit-gothic');
    volumes:
      - evilrabbit-wp-data:/var/www/html
      - ./client-sites/evilrabbitart/themes/evilrabbit-gothic:/var/www/html/wp-content/themes/evilrabbit-gothic
    secrets:
      - evilrabbit_db_password
      - evilrabbit_admin_password
    labels:
      caddy: evilrabbitart.com
      caddy.reverse_proxy: "{{upstreams 80}}"
      client.name: "Evil Rabbit Art"
      client.type: "artist-ecommerce"
      client.admin: "cloud-architects"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:80/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    profiles: ["evilrabbit"]



# Database TLS certificates added to main secrets section above

