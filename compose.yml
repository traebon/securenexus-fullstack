# docker compose v2 (no 'version:' key)
networks:
  proxy:
    external: false

volumes:
  grafana-data:
  prometheus-data:
  alertmanager-data:
  loki-data:
  redis-data:
  pg-data:
  keycloak-db-data:
  etcd-data:
  mysql-data:
  tailscale-data:
  uptime-kuma-data:
  homarr-data:  # Homarr v1.0 data (previously homarr-v1-data)
  portainer-data:  # Portainer container management data
  # stalwart-data: # Removed - switched to mailcow
  # Byrne Accounting volumes
  erpnext-db-data:
  erpnext-redis-cache-data:
  erpnext-redis-queue-data:
  erpnext-sites-data:
  erpnext-assets-data:
  # Dickson Supplies volumes
  dickson-db-data:
  dickson-redis-cache-data:
  dickson-redis-queue-data:
  dickson-sites-data:
  dickson-assets-data:

secrets:
  authentik_secret_key:
    file: ./secrets/authentik_secret_key.txt
  postgres_password:
    file: ./secrets/postgres_password.txt
  redis_password:
    file: ./secrets/redis_password.txt
  mysql_password:
    file: ./secrets/mysql_password.txt
  coredns_api_key:
    file: ./secrets/coredns_api_key.txt
  smtp_username:
    file: ./secrets/smtp_username.txt
  smtp_password:
    file: ./secrets/smtp_password.txt
  grafana_oauth_secret:
    file: ./secrets/grafana_oauth_secret.txt
  keycloak_db_password:
    file: ./secrets/keycloak_db_password.txt
  keycloak_admin_password:
    file: ./secrets/keycloak_admin_password.txt
  tailscale_authkey:
    file: ./secrets/tailscale_authkey.txt
  crowdsec_bouncer_api_key:
    file: ./secrets/crowdsec_bouncer_api_key.txt
  souin_redis_password:
    file: ./secrets/souin_redis_password.txt
  homarr_encryption_key:
    file: ./secrets/homarr_encryption_key.txt
  # Byrne Accounting secrets
  erpnext_db_password:
    file: ./secrets/erpnext_db_password.txt
  erpnext_admin_password:
    file: ./secrets/erpnext_admin_password.txt
  erpnext_redis_cache_password:
    file: ./secrets/erpnext_redis_cache_password.txt
  erpnext_redis_queue_password:
    file: ./secrets/erpnext_redis_queue_password.txt
  # Dickson Supplies secrets
  dickson_db_password:
    file: ./secrets/dickson_db_password.txt
  dickson_admin_password:
    file: ./secrets/dickson_admin_password.txt
  dickson_redis_cache_password:
    file: ./secrets/dickson_redis_cache_password.txt
  dickson_redis_queue_password:
    file: ./secrets/dickson_redis_queue_password.txt

services:
  docker-proxy:
    image: ghcr.io/tecnativa/docker-socket-proxy:latest
    restart: unless-stopped
    networks: [proxy]
    environment:
      INFO: "1"
      CONTAINERS: "1"
      SERVICES: "1"
      TASKS: "1"
      NETWORKS: "1"
      PLUGINS: "0"
      POST: "0"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f haproxy > /dev/null"]
      interval: 30s
      timeout: 5s
      retries: 3
    profiles: ["core"]

  traefik:
    image: traefik:v3.1
    restart: unless-stopped
    command:
      - "--configFile=/etc/traefik/traefik.yml"
      - "--certificatesresolvers.le.acme.email=${EMAIL}"
    networks: [proxy]
    dns:
      - 8.8.8.8
      - 1.1.1.1
    ports:
      - "80:80"
      - "443:443"
      # - "587:587/tcp"  # Removed - mailcow handles mail ports directly
    environment:
      # DNS-01 via CoreDNS webhook
      ACME_WEBHOOK_URL: http://acme_webhook:5000/update-txt-record
    volumes:
      - ./config/traefik.yml:/etc/traefik/traefik.yml:ro        # static
      - ./config/dynamic:/etc/traefik/dynamic:ro                # dynamic
      - ./acme:/acme
    labels:
      - traefik.enable=true
      # Dashboard (secured: admin-vpn + internal service)
      - traefik.http.routers.api.rule=Host(`traefik.${DOMAIN}`)
      - traefik.http.routers.api.entrypoints=websecure
      - traefik.http.routers.api.tls.certresolver=le
      - traefik.http.routers.api.middlewares=admin-vpn@file,secure-headers@file
      - traefik.http.routers.api.service=api@internal
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:8080/ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    profiles: ["core"]

  souin_redis:
    image: redis:7-alpine
    restart: unless-stopped
    networks: [proxy]
    command: ["sh","-c","exec redis-server --requirepass \"$$(cat /run/secrets/souin_redis_password)\""]
    secrets: [souin_redis_password]
    healthcheck:
      test: ["CMD-SHELL","redis-cli -a \"$$(cat /run/secrets/souin_redis_password)\" ping | grep PONG"]
      interval: 10s
      timeout: 5s
      retries: 5
    profiles: ["core"]

  tailscale:
    image: tailscale/tailscale:latest
    container_name: tailscale
    restart: unless-stopped
    network_mode: host
    volumes:
      - tailscale-data:/var/lib/tailscale
      - /dev/net/tun:/dev/net/tun
    cap_add:
      - NET_ADMIN
      - NET_RAW
    environment:
      - TS_AUTHKEY_FILE=/run/secrets/tailscale_authkey
      - TS_STATE_DIR=/var/lib/tailscale
      - TS_USERSPACE=false
      - TS_ACCEPT_DNS=true
    secrets: [tailscale_authkey]
    command: ["tailscaled"]
    profiles: ["core"]

  # CrowdSec + Traefik bouncer
  crowdsec:
    image: crowdsecurity/crowdsec:latest
    restart: unless-stopped
    networks: [proxy]
    environment:
      COLLECTIONS: "crowdsecurity/traefik crowdsecurity/http-cve crowdsecurity/linux"
      GID: "0"
      DISABLE_AGENT: "true"
    volumes:
      - ./crowdsec/data:/var/lib/crowdsec/data
      - ./crowdsec/config:/etc/crowdsec
      - /var/log:/var/log:ro
    healthcheck:
      test: ["CMD-SHELL", "cscli lapi status || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 5
    profiles: ["core"]

  crowdsec_bouncer:
    image: fbonalair/traefik-crowdsec-bouncer:latest
    restart: unless-stopped
    depends_on: [crowdsec]
    networks: [proxy]
    env_file:
      - ./crowdsec_bouncer.env
    healthcheck:
      disable: true
    profiles: ["core"]

  # Identity
  authentik_db:
    image: postgres:16-alpine
    restart: unless-stopped
    networks: [proxy]
    environment:
      POSTGRES_DB: authentik
      POSTGRES_USER: authentik
      POSTGRES_PASSWORD_FILE: /run/secrets/postgres_password
    secrets: [postgres_password]
    volumes:
      - pg-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL","pg_isready -U authentik -d authentik"]
      interval: 10s
      timeout: 5s
      retries: 5
    profiles: ["identity"]

  redis_cache:
    image: redis:7-alpine
    restart: unless-stopped
    command: ["sh","-c","exec redis-server --requirepass \"$$(cat /run/secrets/redis_password)\" --appendonly yes --appendfsync everysec"]
    secrets: [redis_password]
    volumes:
      - redis-data:/data
    networks: [proxy]
    healthcheck:
      test: ["CMD-SHELL","redis-cli -a \"$$(cat /run/secrets/redis_password)\" ping | grep PONG"]
      interval: 10s
      timeout: 5s
      retries: 5
    profiles: ["identity"]

  authentik_server:
    image: ghcr.io/goauthentik/server:2025.10.1
    restart: unless-stopped
    depends_on: [authentik_db]
    networks: [proxy]
    command: server
    environment:
      AUTHENTIK_POSTGRESQL__HOST: authentik_db
      AUTHENTIK_POSTGRESQL__USER: authentik
      AUTHENTIK_POSTGRESQL__NAME: authentik
      AUTHENTIK_POSTGRESQL__PASSWORD: file:///run/secrets/postgres_password
      AUTHENTIK_SECRET_KEY: file:///run/secrets/authentik_secret_key
    secrets: [postgres_password, authentik_secret_key]
    healthcheck:
      test: ["CMD-SHELL", "python -c \"import urllib.request; urllib.request.urlopen('http://localhost:9000/-/health/ready/')\""]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    labels:
      - traefik.enable=true
      - traefik.http.routers.authentik.rule=Host(`authentik.${DOMAIN}`) || Host(`sso.${DOMAIN}`) || Host(`auth.${DOMAIN}`) || Host(`auth.byrne-accounts.org`)
      - traefik.http.routers.authentik.entrypoints=websecure
      - traefik.http.routers.authentik.tls.certresolver=le
      - traefik.http.routers.authentik.middlewares=secure-headers@file
      - traefik.http.services.authentik.loadbalancer.server.port=9000
    profiles: ["identity"]

  authentik_worker:
    image: ghcr.io/goauthentik/server:2025.10.1
    restart: unless-stopped
    depends_on: [authentik_db]
    networks: [proxy]
    environment:
      AUTHENTIK_POSTGRESQL__HOST: authentik_db
      AUTHENTIK_POSTGRESQL__USER: authentik
      AUTHENTIK_POSTGRESQL__NAME: authentik
      AUTHENTIK_POSTGRESQL__PASSWORD: file:///run/secrets/postgres_password
      AUTHENTIK_SECRET_KEY: file:///run/secrets/authentik_secret_key
    secrets: [postgres_password, authentik_secret_key]
    command: worker
    healthcheck:
      test: ["CMD", "python", "-c", "import os; exit(0 if os.path.exists('/proc/1/comm') else 1)"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    profiles: ["identity"]

  # Keycloak SSO (alternative to Authentik)
  keycloak_db:
    image: postgres:16-alpine
    restart: unless-stopped
    networks: [proxy]
    environment:
      POSTGRES_DB: keycloak
      POSTGRES_USER: keycloak
      POSTGRES_PASSWORD_FILE: /run/secrets/keycloak_db_password
    secrets: [keycloak_db_password]
    volumes:
      - keycloak-db-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL","pg_isready -U keycloak -d keycloak"]
      interval: 10s
      timeout: 5s
      retries: 5
    profiles: ["identity"]

  keycloak:
    image: quay.io/keycloak/keycloak:26.0.7
    restart: unless-stopped
    depends_on: [keycloak_db]
    networks: [proxy]
    command: start
    environment:
      KC_DB: postgres
      KC_DB_URL: jdbc:postgresql://keycloak_db:5432/keycloak
      KC_DB_USERNAME: keycloak
      KC_DB_PASSWORD: "${KC_DB_PASSWORD}"
      KC_HOSTNAME: keycloak.${DOMAIN}
      KC_PROXY_HEADERS: xforwarded
      KC_HTTP_RELATIVE_PATH: /
      KC_HTTP_ENABLED: "true"
      KC_HEALTH_ENABLED: "true"
      KC_METRICS_ENABLED: "true"
      KEYCLOAK_ADMIN: admin
      KEYCLOAK_ADMIN_PASSWORD: "${KEYCLOAK_ADMIN_PASSWORD}"
    healthcheck:
      test: ["CMD-SHELL", "exec 3<>/dev/tcp/127.0.0.1/9000 && echo -e 'GET /health/ready HTTP/1.1\\r\\nHost: localhost\\r\\nConnection: close\\r\\n\\r\\n' >&3 && timeout 1 cat <&3 | grep -q 'UP'"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    labels:
      - traefik.enable=true
      - traefik.http.routers.keycloak.rule=Host(`keycloak.${DOMAIN}`)
      - traefik.http.routers.keycloak.entrypoints=websecure
      - traefik.http.routers.keycloak.tls.certresolver=le
      # No middleware - let Keycloak handle its own security headers
      - traefik.http.services.keycloak.loadbalancer.server.port=8080
    profiles: ["identity"]

  # Landing & Portal
  landing:
    image: caddy:2-alpine
    restart: unless-stopped
    networks: [proxy]
    volumes:
      - ./landing:/usr/share/caddy:ro
    labels:
      - traefik.enable=true
      # HTTPS router
      - traefik.http.routers.landing.rule=Host(`${DOMAIN}`)
      - traefik.http.routers.landing.entrypoints=websecure
      - traefik.http.routers.landing.tls.certresolver=le
      - traefik.http.routers.landing.middlewares=secure-headers@file
      # HTTP router (redirect to HTTPS)
      - traefik.http.routers.landing-http.rule=Host(`${DOMAIN}`)
      - traefik.http.routers.landing-http.entrypoints=web
      - traefik.http.routers.landing-http.middlewares=redirect-to-https@file
      - traefik.http.services.landing.loadbalancer.server.port=80
    profiles: ["portal"]

  # ERPNext Setup Wizard Portal
  erp-setup-portal:
    build: ./erp-setup-portal
    restart: unless-stopped
    networks: [proxy]
    labels:
      - traefik.enable=true
      # HTTPS router
      - traefik.http.routers.erp-setup.rule=Host(`setup.byrne-accounts.org`)
      - traefik.http.routers.erp-setup.entrypoints=websecure
      - traefik.http.routers.erp-setup.tls.certresolver=le
      - traefik.http.routers.erp-setup.middlewares=secure-headers@file
      # HTTP router (redirect to HTTPS)
      - traefik.http.routers.erp-setup-http.rule=Host(`setup.byrne-accounts.org`)
      - traefik.http.routers.erp-setup-http.entrypoints=web
      - traefik.http.routers.erp-setup-http.middlewares=redirect-to-https@file
      - traefik.http.services.erp-setup.loadbalancer.server.port=80
    profiles: ["byrne"]

  # Homarr v1.0 (Primary Portal)
  homarr:
    image: ghcr.io/homarr-labs/homarr:latest
    container_name: homarr
    restart: unless-stopped
    networks: [proxy]
    ports:
      - "7575:7575"
    environment:
      - BASE_URL=https://portal.${DOMAIN}
      - SECRET_ENCRYPTION_KEY=868dbce3483128d67f1da74cde540b5205786d32815b4ed38d217b73d1495c0c
      - AUTH_PROVIDERS=oidc,credentials
      - AUTH_OIDC_ISSUER=https://sso.${DOMAIN}/application/o/homarr/
      - AUTH_OIDC_CLIENT_ID=homarr
      - AUTH_OIDC_CLIENT_SECRET=fdfb1d840aa9a1f2cafcce8c8de7c38403ca885efa22105955f87677adb5fe7e
      - AUTH_OIDC_CLIENT_NAME=Authentik
    volumes:
      - homarr-data:/appdata
      - /var/run/docker.sock:/var/run/docker.sock:ro
    labels:
      - traefik.enable=true
      - traefik.docker.network=securenexus-fullstack_proxy
      # HTTPS router
      - traefik.http.routers.portal.rule=Host(`portal.${DOMAIN}`)
      - traefik.http.routers.portal.entrypoints=websecure
      - traefik.http.routers.portal.tls.certresolver=le
      - traefik.http.routers.portal.middlewares=secure-headers@file
      - traefik.http.routers.portal.service=portal-svc
      # HTTP router (redirect to HTTPS)
      - traefik.http.routers.portal-http.rule=Host(`portal.${DOMAIN}`)
      - traefik.http.routers.portal-http.entrypoints=web
      - traefik.http.routers.portal-http.middlewares=redirect-to-https@file
      - traefik.http.routers.portal-http.service=portal-svc
      - traefik.http.services.portal-svc.loadbalancer.server.port=7575
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://127.0.0.1:7575 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    profiles: ["portal"]

  # App Catalog - Custom application deployment system
  app-catalog:
    build:
      context: ./apps-catalog
      dockerfile: Dockerfile
    container_name: app-catalog
    restart: unless-stopped
    networks: [proxy]
    environment:
      - DOMAIN=${DOMAIN}
      - EMAIL=${EMAIL}
      - COMPOSE_PROJECT_NAME=securenexus-fullstack
    volumes:
      - ./apps-catalog/catalog:/catalog:ro
      - ./apps-catalog/deployed:/deployed
      - ./compose.yml:/compose.yml:ro
      - /var/run/docker.sock:/var/run/docker.sock
    labels:
      - traefik.enable=true
      - traefik.docker.network=securenexus-fullstack_proxy
      # HTTPS router
      - traefik.http.routers.apps.rule=Host(`apps.${DOMAIN}`)
      - traefik.http.routers.apps.entrypoints=websecure
      - traefik.http.routers.apps.tls.certresolver=le
      - traefik.http.routers.apps.middlewares=sso@file,secure-headers@file
      # HTTP router (redirect to HTTPS)
      - traefik.http.routers.apps-http.rule=Host(`apps.${DOMAIN}`)
      - traefik.http.routers.apps-http.entrypoints=web
      - traefik.http.routers.apps-http.middlewares=redirect-to-https@file
      - traefik.http.services.apps.loadbalancer.server.port=5000
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://127.0.0.1:5000/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    profiles: ["portal"]

  # Documentation Wiki (MkDocs Material)
  wiki:
    build:
      context: ./wiki
      dockerfile: Dockerfile
    container_name: wiki
    restart: unless-stopped
    networks: [proxy]
    environment:
      - TZ=${TZ:-America/New_York}
    labels:
      - traefik.enable=true
      - traefik.docker.network=securenexus-fullstack_proxy
      # HTTPS router
      - traefik.http.routers.wiki.rule=Host(`docs.${DOMAIN}`)
      - traefik.http.routers.wiki.entrypoints=websecure
      - traefik.http.routers.wiki.tls.certresolver=le
      - traefik.http.routers.wiki.middlewares=secure-headers@file
      - traefik.http.routers.wiki.service=wiki-svc
      # HTTP router (redirect to HTTPS)
      - traefik.http.routers.wiki-http.rule=Host(`docs.${DOMAIN}`)
      - traefik.http.routers.wiki-http.entrypoints=web
      - traefik.http.routers.wiki-http.middlewares=redirect-to-https@file
      - traefik.http.routers.wiki-http.service=wiki-svc
      - traefik.http.services.wiki-svc.loadbalancer.server.port=8000
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://127.0.0.1:8000 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    profiles: ["portal"]

  # Monitoring
  prometheus:
    image: prom/prometheus:v2.53.0
    restart: unless-stopped
    networks: [proxy]
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    volumes:
      - prometheus-data:/prometheus
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/blackbox.yml:/etc/prometheus/blackbox.yml:ro
      - ./monitoring/alert_rules.yml:/etc/prometheus/alert_rules.yml:ro
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          memory: 1G
    labels:
      - traefik.enable=true
      - traefik.http.routers.prom.rule=Host(`prometheus.${DOMAIN}`)
      - traefik.http.routers.prom.entrypoints=websecure
      - traefik.http.routers.prom.tls.certresolver=le
      - traefik.http.routers.prom.middlewares=admin-vpn@file,secure-headers@file
      - traefik.http.services.prom.loadbalancer.server.port=9090
    profiles: ["monitoring"]

  alertmanager:
    image: prom/alertmanager:v0.27.0
    restart: unless-stopped
    networks: [proxy]
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
    volumes:
      - alertmanager-data:/alertmanager
      - ./monitoring/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
    labels:
      - traefik.enable=true
      - traefik.http.routers.alertmanager.rule=Host(`alerts.${DOMAIN}`)
      - traefik.http.routers.alertmanager.entrypoints=websecure
      - traefik.http.routers.alertmanager.tls.certresolver=le
      - traefik.http.routers.alertmanager.middlewares=admin-vpn@file,secure-headers@file
      - traefik.http.services.alertmanager.loadbalancer.server.port=9093
    profiles: ["monitoring"]

  blackbox:
    image: prom/blackbox-exporter:v0.25.0
    restart: unless-stopped
    networks: [proxy]
    command: ["--config.file=/etc/blackbox_exporter/config.yml"]
    volumes:
      - ./monitoring/blackbox.yml:/etc/blackbox_exporter/config.yml:ro
    profiles: ["monitoring"]

  loki:
    image: grafana/loki:2.9.6
    restart: unless-stopped
    networks: [proxy]
    command: -config.file=/etc/loki/loki.yml
    volumes:
      - loki-data:/loki
      - ./monitoring/loki.yml:/etc/loki/loki.yml:ro
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3100/ready || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    profiles: ["monitoring"]

  promtail:
    image: grafana/promtail:2.9.6
    restart: unless-stopped
    networks: [proxy]
    command: -config.file=/etc/promtail/config.yml
    volumes:
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - ./monitoring/promtail.yml:/etc/promtail/config.yml:ro
    profiles: ["monitoring"]

  grafana:
    image: grafana/grafana:11.1.0
    restart: unless-stopped
    networks: [proxy]
    user: "472"
    environment:
      GF_SERVER_ROOT_URL: https://grafana.${DOMAIN}
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: admin
      GF_AUTH_DISABLE_LOGIN_FORM: "false"
      GF_AUTH_ANONYMOUS_ENABLED: "false"
      # OAuth / OIDC (Authentik)
      GF_AUTH_GENERIC_OAUTH_ENABLED: "true"
      GF_AUTH_GENERIC_OAUTH_NAME: "Authentik"
      GF_AUTH_GENERIC_OAUTH_CLIENT_ID: "grafana-oauth"
      GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET__FILE: /run/secrets/grafana_oauth_secret
      GF_AUTH_GENERIC_OAUTH_SCOPES: "openid profile email"
      GF_AUTH_GENERIC_OAUTH_AUTH_URL: https://sso.${DOMAIN}/application/o/authorize/
      GF_AUTH_GENERIC_OAUTH_TOKEN_URL: https://sso.${DOMAIN}/application/o/token/
      GF_AUTH_GENERIC_OAUTH_API_URL: https://sso.${DOMAIN}/application/o/userinfo/
      GF_AUTH_GENERIC_OAUTH_ALLOW_SIGN_UP: "true"
      GF_AUTH_GENERIC_OAUTH_AUTO_LOGIN: "false"
      GF_AUTH_GENERIC_OAUTH_ROLE_ATTRIBUTE_PATH: "contains(groups[*], 'Grafana Admins') && 'Admin' || contains(groups[*], 'authentik Admins') && 'Admin' || 'Viewer'"
      GF_AUTH_GENERIC_OAUTH_ROLE_ATTRIBUTE_STRICT: "false"
      GF_AUTH_SIGNOUT_REDIRECT_URL: https://sso.${DOMAIN}/application/o/grafana-oauth/end-session/
    secrets:
      - grafana_oauth_secret
    volumes:
      - grafana-data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./monitoring/dashboards:/var/lib/grafana/dashboards:ro
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    labels:
      - traefik.enable=true
      - traefik.http.routers.grafana.rule=Host(`grafana.${DOMAIN}`)
      - traefik.http.routers.grafana.entrypoints=websecure
      - traefik.http.routers.grafana.tls.certresolver=le
      - traefik.http.routers.grafana.middlewares=admin-vpn@file,secure-headers@file
      - traefik.http.services.grafana.loadbalancer.server.port=3000
    profiles: ["monitoring"]

  uptime-kuma:
    image: louislam/uptime-kuma:1
    restart: unless-stopped
    networks: [proxy]
    volumes:
      - uptime-kuma-data:/app/data
      - /var/run/docker.sock:/var/run/docker.sock:ro
    labels:
      - traefik.enable=true
      - traefik.http.routers.uptime.rule=Host(`status.${DOMAIN}`)
      - traefik.http.routers.uptime.entrypoints=websecure
      - traefik.http.routers.uptime.tls.certresolver=le
      - traefik.http.routers.uptime.middlewares=secure-headers@file
      - traefik.http.services.uptime.loadbalancer.server.port=3001
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3001 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    profiles: ["monitoring"]

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.49.2
    restart: unless-stopped
    networks: [proxy]
    privileged: true
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          memory: 512M
    healthcheck:
      test: ["CMD", "wget", "-q", "--tries=1", "--spider", "http://localhost:8080/metrics"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles: ["monitoring"]

  node-exporter:
    image: prom/node-exporter:v1.8.1
    restart: unless-stopped
    networks: [proxy]
    pid: host
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
    profiles: ["monitoring"]

  redis-exporter:
    image: redis-exporter-with-shell:local
    restart: unless-stopped
    networks: [proxy]
    secrets: [redis_password]
    profiles: ["monitoring"]

  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:v0.15.0
    restart: unless-stopped
    networks: [proxy]
    secrets: [postgres_password]
    volumes:
      - ./scripts/postgres-exporter-wrapper.sh:/usr/local/bin/postgres-exporter-wrapper.sh:ro
    entrypoint: ["/bin/sh"]
    command: ["/usr/local/bin/postgres-exporter-wrapper.sh"]
    profiles: ["monitoring"]

  # Container Management

  # Portainer - Docker container management with SSO
  portainer:
    image: portainer/portainer-ee:latest
    container_name: portainer
    restart: unless-stopped
    networks: [proxy]
    command: -H unix:///var/run/docker.sock
    volumes:
      - portainer-data:/data
      - /var/run/docker.sock:/var/run/docker.sock
    labels:
      - traefik.enable=true
      - traefik.docker.network=securenexus-fullstack_proxy
      # HTTPS router
      - traefik.http.routers.frontend.rule=Host(`portainer.${DOMAIN}`)
      - traefik.http.routers.frontend.entrypoints=websecure
      - traefik.http.services.frontend.loadbalancer.server.port=9000
      - traefik.http.routers.frontend.service=frontend
      - traefik.http.routers.frontend.tls.certresolver=le
      - traefik.http.routers.frontend.middlewares=secure-headers@file
      # HTTP router (redirect to HTTPS)
      - traefik.http.routers.frontend-http.rule=Host(`portainer.${DOMAIN}`)
      - traefik.http.routers.frontend-http.entrypoints=web
      - traefik.http.routers.frontend-http.middlewares=redirect-to-https@file
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          memory: 256M
    profiles: ["portal"]

  # DNS: CoreDNS with etcd + MySQL backends + DNS Updater + ACME TXT Webhook

  # etcd - Key-value store for dynamic DNS records
  etcd:
    image: quay.io/coreos/etcd:v3.5.16
    restart: unless-stopped
    networks: [proxy]
    command:
      - /usr/local/bin/etcd
      - --name=etcd0
      - --data-dir=/etcd-data
      - --listen-client-urls=http://0.0.0.0:2379
      - --advertise-client-urls=http://etcd:2379
      - --listen-peer-urls=http://0.0.0.0:2380
      - --initial-advertise-peer-urls=http://etcd:2380
      - --initial-cluster=etcd0=http://etcd:2380
      - --initial-cluster-state=new
      - --initial-cluster-token=etcd-cluster
    volumes:
      - etcd-data:/etcd-data
    healthcheck:
      test: ["CMD", "etcdctl", "endpoint", "health"]
      interval: 30s
      timeout: 10s
      retries: 3
    profiles: ["dns"]

  # MySQL database for CoreDNS mysql plugin
  mysql-db:
    image: mysql:8.0
    restart: unless-stopped
    networks: [proxy]
    environment:
      MYSQL_DATABASE: coredns
      MYSQL_USER: coredns
      MYSQL_ROOT_PASSWORD_FILE: /run/secrets/mysql_password
      MYSQL_PASSWORD_FILE: /run/secrets/mysql_password
    secrets: [mysql_password]
    volumes:
      - mysql-data:/var/lib/mysql
      - ./dns/mysql-init:/docker-entrypoint-initdb.d:ro
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    profiles: ["dns"]

  # CoreDNS - Modern DNS server with plugin architecture
  coredns:
    image: coredns/coredns:latest
    restart: unless-stopped
    depends_on: [etcd, mysql-db]
    networks: [proxy]
    command: -conf /etc/coredns/Corefile
    ports:
      - "0.0.0.0:53:53/tcp"     # Standard DNS port
      - "0.0.0.0:53:53/udp"     # Standard DNS port
      - "853:853/tcp"           # DNS-over-TLS
      - "8181:8080"            # Health check endpoint
      - "9153:9153"            # Prometheus metrics
    environment:
      - MYSQL_PASSWORD_FILE=/run/secrets/mysql_password
    secrets: [mysql_password]
    volumes:
      - ./dns/Corefile:/etc/coredns/Corefile:ro
      - ./dns/certs:/etc/coredns/certs:ro
      - ./dns/dnssec_keys:/etc/coredns/dnssec_keys
      - ./dns/zones:/etc/coredns/zones:ro
    labels:
      - traefik.enable=true
      - traefik.http.routers.coredns.rule=Host(`dns.${DOMAIN}`)
      - traefik.http.routers.coredns.entrypoints=websecure
      - traefik.http.routers.coredns.tls.certresolver=le
      - traefik.http.routers.coredns.middlewares=admin-vpn@file,secure-headers@file
      - traefik.http.services.coredns.loadbalancer.server.port=8080
      - "coredns.name=dns"
    healthcheck:
      test: ["CMD", "/coredns", "-version"]
      interval: 30s
      timeout: 5s
      retries: 3
    profiles: ["dns"]

  # DNS Updater - Automatically creates DNS records based on container labels
  dns-updater:
    image: alpine:3.20
    restart: unless-stopped
    depends_on: [etcd]
    networks: [proxy]
    environment:
      - DOMAIN=${DOMAIN}
      - ETCD_HOST=etcd
      - ETCD_PORT=2379
      - DNS_TTL=300
    volumes:
      - ./scripts/dns-updater.sh:/usr/local/bin/dns-updater:ro
    entrypoint: ["/bin/sh", "-c"]
    command: >
      "apk add --no-cache curl jq bash docker-cli && \
       while true; do /usr/local/bin/dns-updater || true; sleep 30; done"
    healthcheck:
      test: ["CMD", "pgrep", "-f", "dns-updater"]
      interval: 30s
      timeout: 5s
      retries: 3
    profiles: ["dns"]

  # etcd Browser - Web UI for viewing etcd data (optional)
  # Commented out for now - can be enabled if needed
  # etcd-browser:
  #   image: evildecay/etcdkeeper:latest
  #   restart: unless-stopped
  #   networks: [proxy]
  #   environment:
  #     HOST: 0.0.0.0
  #   command: ["etcdkeeper", "-h", "0.0.0.0", "-p", "8080"]
  #   labels:
  #     - traefik.enable=true
  #     - traefik.http.routers.etcd-ui.rule=Host(`etcd-ui.${DOMAIN}`)
  #     - traefik.http.routers.etcd-ui.entrypoints=websecure
  #     - traefik.http.routers.etcd-ui.tls.certresolver=le
  #     - traefik.http.routers.etcd-ui.middlewares=admin-vpn@file,secure-headers@file
  #     - traefik.http.services.etcd-ui.loadbalancer.server.port=8080
  #     - "coredns.name=etcd-ui"
  #   profiles: ["dns"]

  # ACME TXT webhook - Updates etcd for DNS-01 challenges
  acme_webhook:
    image: python:3.12-alpine
    restart: unless-stopped
    networks: [proxy]
    depends_on: [etcd]
    working_dir: /app
    volumes:
      - ./acme-webhook-etcd:/app:ro
    environment:
      ETCD_HOST: etcd
      ETCD_PORT: 2379
      DNS_TTL: "120"
    entrypoint: ["sh", "-c"]
    command: >
      "pip install --no-cache-dir flask etcd3 && \
       python3 /app/app.py"
    healthcheck:
      test: ["CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:5000/health')"]
      interval: 30s
      timeout: 5s
      retries: 3
    profiles: ["dns"]

  wellknown:
    image: caddy:2-alpine
    restart: unless-stopped
    networks: [proxy]
    volumes:
      - ./wellknown/Caddyfile:/etc/caddy/Caddyfile:ro
    labels:
      - traefik.enable=true
      - traefik.http.routers.wk.rule=PathPrefix(`/.well-known/`) && !PathPrefix(`/.well-known/acme-challenge/`)
      - traefik.http.routers.wk.entrypoints=websecure
      - traefik.http.routers.wk.tls=true
      - traefik.http.routers.wk.middlewares=secure-headers@file
      - traefik.http.services.wk.loadbalancer.server.port=80

  # Mail - mailcow
  # NOTE: mailcow requires its own docker-compose.yml in a subdirectory
  # See mail/mailcow/ for mailcow installation
  # Ports exposed: 25, 143, 465, 587, 993 (handled by mailcow's compose)

  brand-static:
    image: nginx:alpine
    container_name: brand-static
    restart: unless-stopped
    volumes:
      - ./branding:/usr/share/nginx/html:ro
    labels:
      - traefik.enable=true
      - traefik.http.routers.brand-static.rule=Host(`brand.${DOMAIN}`)
      - traefik.http.routers.brand-static.entrypoints=websecure
      - traefik.http.routers.brand-static.tls=true
      - traefik.http.middlewares.brand-headers.headers.customResponseHeaders.Cache-Control=max-age=86400,public
      - traefik.http.routers.brand-static.middlewares=brand-headers@docker
    networks:
      - proxy

  # Byrne Accounting Services
  # Marketing website
  byrne-website:
    build:
      context: ./byrne-website
      dockerfile: Dockerfile
    container_name: byrne-website
    restart: unless-stopped
    networks: [proxy]
    labels:
      - traefik.enable=true
      - traefik.docker.network=securenexus-fullstack_proxy

      # Main website - HTTPS router
      - traefik.http.routers.byrne.rule=Host(`byrne-accounts.org`) || Host(`www.byrne-accounts.org`)
      - traefik.http.routers.byrne.entrypoints=websecure
      - traefik.http.routers.byrne.tls.certresolver=le
      - traefik.http.routers.byrne.middlewares=secure-headers@file
      - traefik.http.routers.byrne.service=byrne

      # Main website - HTTP router (redirect to HTTPS)
      - traefik.http.routers.byrne-http.rule=Host(`byrne-accounts.org`) || Host(`www.byrne-accounts.org`)
      - traefik.http.routers.byrne-http.entrypoints=web
      - traefik.http.routers.byrne-http.middlewares=redirect-to-https@file

      # Client Portal - HTTPS router
      - traefik.http.routers.byrne-portal.rule=Host(`portal.byrne-accounts.org`)
      - traefik.http.routers.byrne-portal.entrypoints=websecure
      - traefik.http.routers.byrne-portal.tls.certresolver=le
      - traefik.http.routers.byrne-portal.middlewares=secure-headers@file
      - traefik.http.routers.byrne-portal.service=byrne

      # Client Portal - HTTP router (redirect to HTTPS)
      - traefik.http.routers.byrne-portal-http.rule=Host(`portal.byrne-accounts.org`)
      - traefik.http.routers.byrne-portal-http.entrypoints=web
      - traefik.http.routers.byrne-portal-http.middlewares=redirect-to-https@file

      # Service definition
      - traefik.http.services.byrne.loadbalancer.server.port=80
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://127.0.0.1/health || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
    profiles: ["byrne"]

  # ERPNext database
  erpnext-db:
    image: mariadb:10.6
    container_name: erpnext-db
    restart: unless-stopped
    networks: [proxy]
    command: --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci --skip-character-set-client-handshake --skip-innodb-read-only-compressed
    environment:
      MYSQL_ROOT_PASSWORD_FILE: /run/secrets/erpnext_db_password
      MYSQL_DATABASE: erpnext
      MYSQL_USER: erpnext
      MYSQL_PASSWORD_FILE: /run/secrets/erpnext_db_password
    secrets: [erpnext_db_password]
    volumes:
      - erpnext-db-data:/var/lib/mysql
      - ./frappe_docker/mariadb.cnf:/etc/mysql/conf.d/custom.cnf:ro
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    healthcheck:
      test: ["CMD", "healthcheck.sh", "--connect", "--innodb_initialized"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 60s
    profiles: ["byrne"]

  # ERPNext Redis cache
  erpnext-redis-cache:
    image: redis:7-alpine
    container_name: erpnext-redis-cache
    restart: unless-stopped
    networks: [proxy]
    command: ["sh", "-c", "exec redis-server --requirepass \"$$(cat /run/secrets/erpnext_redis_cache_password)\" --maxmemory 512mb --maxmemory-policy allkeys-lru"]
    secrets: [erpnext_redis_cache_password]
    volumes:
      - erpnext-redis-cache-data:/data
    healthcheck:
      test: ["CMD-SHELL", "redis-cli -a \"$$(cat /run/secrets/erpnext_redis_cache_password)\" ping | grep PONG"]
      interval: 10s
      timeout: 5s
      retries: 5
    profiles: ["byrne"]

  # ERPNext Redis queue
  erpnext-redis-queue:
    image: redis:7-alpine
    container_name: erpnext-redis-queue
    restart: unless-stopped
    networks: [proxy]
    command: ["sh", "-c", "exec redis-server --requirepass \"$$(cat /run/secrets/erpnext_redis_queue_password)\" --maxmemory 256mb --maxmemory-policy noeviction --save 60 1000"]
    secrets: [erpnext_redis_queue_password]
    volumes:
      - erpnext-redis-queue-data:/data
    healthcheck:
      test: ["CMD-SHELL", "redis-cli -a \"$$(cat /run/secrets/erpnext_redis_queue_password)\" ping | grep PONG"]
      interval: 10s
      timeout: 5s
      retries: 5
    profiles: ["byrne"]

  # ERPNext configurator (creates site on first run)
  erpnext-configurator:
    image: erpnext-posawesome:latest
    container_name: erpnext-configurator
    restart: "no"
    depends_on:
      erpnext-db:
        condition: service_healthy
      erpnext-redis-cache:
        condition: service_healthy
      erpnext-redis-queue:
        condition: service_healthy
    networks: [proxy]
    environment:
      DB_ROOT_PASSWORD: "2m9b6KAUgt59SgDNIelq6vDL/gMbWN0jmuALxLm3Jug="
      ADMIN_PASSWORD: "GAJN4jze46OixmPB76+rsO+vFu8/Adoq"
    command:
      - bash
      - -c
      - |
        if [ ! -f '/home/frappe/frappe-bench/sites/erp.byrne-accounts.org/site_config.json' ]; then
          echo 'Initializing new ERPNext site...';
          bench new-site erp.byrne-accounts.org \
            --mariadb-root-password "$$DB_ROOT_PASSWORD" \
            --admin-password "$$ADMIN_PASSWORD" \
            --db-host erpnext-db \
            --db-type mariadb \
            --install-app erpnext \
            --no-mariadb-socket;
          bench --site erp.byrne-accounts.org set-config developer_mode 0;
          bench --site erp.byrne-accounts.org set-config maintenance_mode 0;
          echo 'Site initialization complete';
        else
          echo 'Site already exists, skipping initialization';
        fi
    volumes:
      - erpnext-sites-data:/home/frappe/frappe-bench/sites
      - erpnext-assets-data:/home/frappe/frappe-bench/sites/assets
      - ./branding:/branding:ro
      - ./erp/branding:/custom-branding:ro
    profiles: ["byrne"]

  # ERPNext backend (Frappe/ERPNext application server)
  erpnext-backend:
    image: erpnext-posawesome:latest
    container_name: erpnext-backend
    restart: unless-stopped
    depends_on:
      erpnext-db:
        condition: service_healthy
      erpnext-redis-cache:
        condition: service_healthy
      erpnext-redis-queue:
        condition: service_healthy
      erpnext-configurator:
        condition: service_completed_successfully
    networks: [proxy]
    environment:
      SITE_NAME: erp.byrne-accounts.org
    command: ["bash", "-c", "cd /home/frappe/frappe-bench && bench serve --port 8000"]
    volumes:
      - erpnext-sites-data:/home/frappe/frappe-bench/sites
      - erpnext-assets-data:/home/frappe/frappe-bench/sites/assets
      - ./branding:/branding:ro
      - ./erp/branding:/custom-branding:ro
    labels:
      - traefik.enable=true
      - traefik.docker.network=securenexus-fullstack_proxy
      # ERP access - temporarily public for initial setup (add SSO after Authentik outpost configured)
      - traefik.http.routers.erp.rule=Host(`erp.byrne-accounts.org`)
      - traefik.http.routers.erp.entrypoints=websecure
      - traefik.http.routers.erp.tls.certresolver=le
      - traefik.http.routers.erp.middlewares=secure-headers@file
      - traefik.http.routers.erp.service=erp
      - traefik.http.services.erp.loadbalancer.server.port=8000
      # HTTP redirect to HTTPS
      - traefik.http.routers.erp-http.rule=Host(`erp.byrne-accounts.org`)
      - traefik.http.routers.erp-http.entrypoints=web
      - traefik.http.routers.erp-http.middlewares=redirect-to-https@file
      # POS access subdomain - temporarily public for initial setup
      - traefik.http.routers.pos.rule=Host(`pos.byrne-accounts.org`)
      - traefik.http.routers.pos.entrypoints=websecure
      - traefik.http.routers.pos.tls.certresolver=le
      - traefik.http.routers.pos.middlewares=secure-headers@file
      - traefik.http.routers.pos.service=erp
      - traefik.http.routers.pos-http.rule=Host(`pos.byrne-accounts.org`)
      - traefik.http.routers.pos-http.entrypoints=web
      - traefik.http.routers.pos-http.middlewares=redirect-to-https@file
    healthcheck:
      test: ["CMD-SHELL", "curl -f -H 'Host: erp.byrne-accounts.org' http://localhost:8000/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    profiles: ["byrne"]

  # ERPNext SocketIO (real-time updates)
  erpnext-socketio:
    image: erpnext-posawesome:latest
    container_name: erpnext-socketio
    restart: unless-stopped
    depends_on:
      - erpnext-backend
    networks: [proxy]
    environment:
      SITE_NAME: erp.byrne-accounts.org
      REDIS_CACHE: redis://erpnext-redis-cache:6379
      REDIS_QUEUE: redis://erpnext-redis-queue:6379
      REDIS_SOCKETIO: redis://erpnext-redis-cache:6379
    command: ["bash", "-c", "cd /home/frappe/frappe-bench && node apps/frappe/socketio.js"]
    volumes:
      - erpnext-sites-data:/home/frappe/frappe-bench/sites
    labels:
      - traefik.enable=true
      - traefik.docker.network=securenexus-fullstack_proxy
      - traefik.http.routers.erpnext-socketio.rule=Host(`erp.byrne-accounts.org`) && PathPrefix(`/socket.io`)
      - traefik.http.routers.erpnext-socketio.entrypoints=websecure
      - traefik.http.routers.erpnext-socketio.tls.certresolver=le
      - traefik.http.services.erpnext-socketio.loadbalancer.server.port=9000
    healthcheck:
      test: ["CMD-SHELL", "test -f /proc/1/comm && exit 0 || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s
    profiles: ["byrne"]

  # ERPNext worker (background jobs)
  erpnext-worker:
    image: erpnext-posawesome:latest
    container_name: erpnext-worker
    restart: unless-stopped
    depends_on:
      - erpnext-backend
    networks: [proxy]
    environment:
      SITE_NAME: erp.byrne-accounts.org
      REDIS_CACHE: redis://erpnext-redis-cache:6379
      REDIS_QUEUE: redis://erpnext-redis-queue:6379
    command: ["bash", "-c", "cd /home/frappe/frappe-bench && bench --site erp.byrne-accounts.org worker --queue default,short,long"]
    volumes:
      - erpnext-sites-data:/home/frappe/frappe-bench/sites
      - erpnext-assets-data:/home/frappe/frappe-bench/sites/assets
    healthcheck:
      test: ["CMD-SHELL", "test -f /proc/1/comm && exit 0 || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    profiles: ["byrne"]

  # ERPNext scheduler (scheduled tasks)
  erpnext-scheduler:
    image: erpnext-posawesome:latest
    container_name: erpnext-scheduler
    restart: unless-stopped
    depends_on:
      - erpnext-backend
    networks: [proxy]
    environment:
      SITE_NAME: erp.byrne-accounts.org
    command: ["bash", "-c", "cd /home/frappe/frappe-bench && bench --site erp.byrne-accounts.org schedule"]
    volumes:
      - erpnext-sites-data:/home/frappe/frappe-bench/sites
      - erpnext-assets-data:/home/frappe/frappe-bench/sites/assets
    healthcheck:
      test: ["CMD-SHELL", "test -f /proc/1/comm && exit 0 || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    profiles: ["byrne"]

  # Watchtower - Automatic Docker container updates
  watchtower:
    image: containrrr/watchtower:latest
    container_name: watchtower
    restart: unless-stopped
    networks: [proxy]
    environment:
      - TZ=UTC
      - WATCHTOWER_CLEANUP=true
      - WATCHTOWER_INCLUDE_RESTARTING=true
      - WATCHTOWER_INCLUDE_STOPPED=false
      - WATCHTOWER_POLL_INTERVAL=86400  # Check daily (24 hours)
      - WATCHTOWER_ROLLING_RESTART=true
      - WATCHTOWER_NOTIFICATIONS=shoutrrr
      - WATCHTOWER_NOTIFICATION_URL=logger://
      - WATCHTOWER_LABEL_ENABLE=false  # Update all containers by default
      - WATCHTOWER_REVIVE_STOPPED=false
      - WATCHTOWER_NO_STARTUP_MESSAGE=false
      - WATCHTOWER_SCHEDULE=0 0 3 * * *  # Run at 3 AM daily
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    command: --debug --cleanup
    deploy:
      resources:
        limits:
          cpus: '0.2'
          memory: 128M
        reservations:
          memory: 64M
    profiles: ["monitoring"]

  # ====================================================================================
  # DICKSON SUPPLIES - CLIENT ERP INSTANCE
  # ====================================================================================
  
  # Dickson MariaDB database
  dickson-db:
    image: mariadb:10.6
    container_name: dickson-db
    restart: unless-stopped
    networks: [proxy]
    environment:
      MARIADB_ROOT_PASSWORD_FILE: /run/secrets/dickson_db_password
      MARIADB_CHARACTER_SET_SERVER: utf8mb4
      MARIADB_COLLATION_SERVER: utf8mb4_unicode_ci
    secrets: [dickson_db_password]
    volumes:
      - dickson-db-data:/var/lib/mysql
    healthcheck:
      test: ["CMD", "healthcheck.sh", "--connect", "--innodb_initialized"]
      interval: 10s
      timeout: 5s
      retries: 10
    profiles: ["dickson"]

  # Dickson Redis cache
  dickson-redis-cache:
    image: redis:7-alpine
    container_name: dickson-redis-cache
    restart: unless-stopped
    networks: [proxy]
    command: ["sh", "-c", "exec redis-server --requirepass \"$$(cat /run/secrets/dickson_redis_cache_password)\" --maxmemory 512mb --maxmemory-policy allkeys-lru"]
    secrets: [dickson_redis_cache_password]
    volumes:
      - dickson-redis-cache-data:/data
    healthcheck:
      test: ["CMD-SHELL", "redis-cli -a \"$$(cat /run/secrets/dickson_redis_cache_password)\" ping | grep PONG"]
      interval: 10s
      timeout: 5s
      retries: 5
    profiles: ["dickson"]

  # Dickson Redis queue
  dickson-redis-queue:
    image: redis:7-alpine
    container_name: dickson-redis-queue
    restart: unless-stopped
    networks: [proxy]
    command: ["sh", "-c", "exec redis-server --requirepass \"$$(cat /run/secrets/dickson_redis_queue_password)\" --maxmemory 256mb --maxmemory-policy noeviction --save 60 1000"]
    secrets: [dickson_redis_queue_password]
    volumes:
      - dickson-redis-queue-data:/data
    healthcheck:
      test: ["CMD-SHELL", "redis-cli -a \"$$(cat /run/secrets/dickson_redis_queue_password)\" ping | grep PONG"]
      interval: 10s
      timeout: 5s
      retries: 5
    profiles: ["dickson"]

  # Dickson configurator (creates site on first run)
  dickson-configurator:
    image: erpnext-posawesome:latest
    container_name: dickson-configurator
    restart: "no"
    depends_on:
      dickson-db:
        condition: service_healthy
      dickson-redis-cache:
        condition: service_healthy
      dickson-redis-queue:
        condition: service_healthy
    networks: [proxy]
    secrets: [dickson_db_password, dickson_admin_password]
    command:
      - bash
      - -c
      - |
        DB_ROOT_PASSWORD=$$(cat /run/secrets/dickson_db_password)
        ADMIN_PASSWORD=$$(cat /run/secrets/dickson_admin_password)
        if [ ! -f '/home/frappe/frappe-bench/sites/erp.dickson-supplies.com/site_config.json' ]; then
          echo 'Initializing new Dickson ERPNext site...';
          bench new-site erp.dickson-supplies.com \
            --mariadb-root-password "$$DB_ROOT_PASSWORD" \
            --admin-password "$$ADMIN_PASSWORD" \
            --db-host dickson-db \
            --db-type mariadb \
            --install-app erpnext \
            --no-mariadb-socket;
          bench --site erp.dickson-supplies.com set-config developer_mode 0;
          bench --site erp.dickson-supplies.com set-config maintenance_mode 0;
          echo 'Dickson site initialization complete';
        else
          echo 'Dickson site already exists, skipping initialization';
        fi
    volumes:
      - dickson-sites-data:/home/frappe/frappe-bench/sites
      - dickson-assets-data:/home/frappe/frappe-bench/sites/assets
      - ./erp/dickson-branding:/custom-branding:ro
    profiles: ["dickson"]

  # Dickson backend (Frappe/ERPNext application server)
  dickson-backend:
    image: erpnext-posawesome:latest
    container_name: dickson-backend
    restart: unless-stopped
    depends_on:
      dickson-db:
        condition: service_healthy
      dickson-redis-cache:
        condition: service_healthy
      dickson-redis-queue:
        condition: service_healthy
      dickson-configurator:
        condition: service_completed_successfully
    networks: [proxy]
    environment:
      SITE_NAME: erp.dickson-supplies.com
    command: ["bash", "-c", "cd /home/frappe/frappe-bench && bench serve --port 8000"]
    volumes:
      - dickson-sites-data:/home/frappe/frappe-bench/sites
      - dickson-assets-data:/home/frappe/frappe-bench/sites/assets
      - ./erp/dickson-branding:/custom-branding:ro
    labels:
      - traefik.enable=true
      - traefik.docker.network=securenexus-fullstack_proxy
      - traefik.http.services.dickson.loadbalancer.server.port=8000
      # ERP domain
      - traefik.http.routers.dickson-erp.rule=Host(`erp.dickson-supplies.com`)
      - traefik.http.routers.dickson-erp.entrypoints=websecure
      - traefik.http.routers.dickson-erp.tls.certresolver=le
      - traefik.http.routers.dickson-erp.middlewares=secure-headers@file
      - traefik.http.routers.dickson-erp.service=dickson
      - traefik.http.routers.dickson-erp-http.rule=Host(`erp.dickson-supplies.com`)
      - traefik.http.routers.dickson-erp-http.entrypoints=web
      - traefik.http.routers.dickson-erp-http.middlewares=redirect-to-https@file
      # POS domain
      - traefik.http.routers.dickson-pos.rule=Host(`pos.dickson-supplies.com`)
      - traefik.http.routers.dickson-pos.entrypoints=websecure
      - traefik.http.routers.dickson-pos.tls.certresolver=le
      - traefik.http.routers.dickson-pos.middlewares=secure-headers@file
      - traefik.http.routers.dickson-pos.service=dickson
      - traefik.http.routers.dickson-pos-http.rule=Host(`pos.dickson-supplies.com`)
      - traefik.http.routers.dickson-pos-http.entrypoints=web
      - traefik.http.routers.dickson-pos-http.middlewares=redirect-to-https@file
    healthcheck:
      test: ["CMD-SHELL", "curl -f -H 'Host: erp.dickson-supplies.com' http://localhost:8000/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    profiles: ["dickson"]

  # Dickson SocketIO (real-time updates)
  dickson-socketio:
    image: erpnext-posawesome:latest
    container_name: dickson-socketio
    restart: unless-stopped
    depends_on:
      - dickson-backend
    networks: [proxy]
    environment:
      SITE_NAME: erp.dickson-supplies.com
      REDIS_CACHE: redis://dickson-redis-cache:6379
      REDIS_QUEUE: redis://dickson-redis-queue:6379
      REDIS_SOCKETIO: redis://dickson-redis-cache:6379
    command: ["bash", "-c", "cd /home/frappe/frappe-bench && node apps/frappe/socketio.js"]
    volumes:
      - dickson-sites-data:/home/frappe/frappe-bench/sites
    labels:
      - traefik.enable=true
      - traefik.docker.network=securenexus-fullstack_proxy
      - traefik.http.routers.dickson-socketio.rule=Host(`erp.dickson-supplies.com`) && PathPrefix(`/socket.io`)
      - traefik.http.routers.dickson-socketio.entrypoints=websecure
      - traefik.http.routers.dickson-socketio.tls.certresolver=le
      - traefik.http.services.dickson-socketio.loadbalancer.server.port=9000
    healthcheck:
      test: ["CMD-SHELL", "test -f /proc/1/comm && exit 0 || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 30s
    profiles: ["dickson"]

  # Dickson worker (background jobs)
  dickson-worker:
    image: erpnext-posawesome:latest
    container_name: dickson-worker
    restart: unless-stopped
    depends_on:
      - dickson-backend
    networks: [proxy]
    environment:
      SITE_NAME: erp.dickson-supplies.com
      REDIS_CACHE: redis://dickson-redis-cache:6379
      REDIS_QUEUE: redis://dickson-redis-queue:6379
    command: ["bash", "-c", "cd /home/frappe/frappe-bench && bench --site erp.dickson-supplies.com worker --queue default,short,long"]
    volumes:
      - dickson-sites-data:/home/frappe/frappe-bench/sites
      - dickson-assets-data:/home/frappe/frappe-bench/sites/assets
    healthcheck:
      test: ["CMD-SHELL", "test -f /proc/1/comm && exit 0 || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    profiles: ["dickson"]

  # Dickson scheduler (scheduled tasks)
  dickson-scheduler:
    image: erpnext-posawesome:latest
    container_name: dickson-scheduler
    restart: unless-stopped
    depends_on:
      - dickson-backend
    networks: [proxy]
    environment:
      SITE_NAME: erp.dickson-supplies.com
    command: ["bash", "-c", "cd /home/frappe/frappe-bench && bench --site erp.dickson-supplies.com schedule"]
    volumes:
      - dickson-sites-data:/home/frappe/frappe-bench/sites
      - dickson-assets-data:/home/frappe/frappe-bench/sites/assets
    healthcheck:
      test: ["CMD-SHELL", "test -f /proc/1/comm && exit 0 || exit 1"]
      interval: 60s
      timeout: 10s
      retries: 3
      start_period: 30s
    profiles: ["dickson"]

